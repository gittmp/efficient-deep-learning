% ------------------ DOCUMENT SETUP / PACKAGES ------------------ 
\documentclass[a4paper, 11pt]{report}
\usepackage[a4 paper, top=25mm, bottom=25mm, left=25mm, right=20mm]{geometry}
% \usepackage{times}
\usepackage{setspace}
\onehalfspacing

\usepackage{float}
\usepackage{color}
\usepackage{emptypage}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}
\graphicspath{{./resources/}} 

\usepackage[table, xcdraw]{xcolor}
\renewcommand{\arraystretch}{1.3}

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin}

% Manages hyperlinks 
\usepackage{xurl}
\usepackage[colorlinks=true, linkcolor=black, urlcolor=red]{hyperref}

% Bibliography package
\usepackage[round]{natbib}
\bibliographystyle{plainnat}
	

% Line Break Properties
% \tolerance=1
\emergencystretch=\maxdimen
% \hyphenpenalty=10000
\hbadness=10000


% Title page information
\title{Addressing the Energy and Data Efficiency of Deep Learning for Finance}
\author{Tom Maxwell Potter}
\date{\today}


% ---------------------  DOCUMENT ----------------------
\begin{document}

    % ------------------  TITLE PAGE -------------------
    \begin{titlepage}
        \begin{center}
            % UCL Image
            \vspace*{1cm}
            \makebox[\textwidth]{\includegraphics[width=.5\paperwidth]{resources/UCL_LOGO.png}}
            
            \vfill
            
            % Title
            \makeatletter
            {\Huge\textbf{\@title}}

            \vspace{0.8cm}
            by
            \vspace{0.8cm}

            % Author
            {\Large\textbf{\@author}}

            % Date
            \vspace{1.5cm}
            {\textbf{\\\@date}}

            \vfill

            {A dissertation submitted in part fulfilment\\
            of the requirements for the degree of\\}
            {\setstretch{2.0}
            \textbf{Master of Science}\\
            of\\
            \textbf{University College London\\}}
            \vspace{1cm}
            {Scientific and Data Intensive Computing\\
            Department of Physics and Astronomy}

            \vspace{2cm}
        \end{center}
    \end{titlepage}


    % -----------------  DECLARATION  -------------------
    \pagenumbering{roman}
    \chapter*{Declaration}
    \addcontentsline{toc}{chapter}{Declaration}
    
    I, Tom Maxwell Potter, confirm that the work presented in this thesis is my own. Where information has been derived from other sources, I confirm that this has been indicated in the dissertation.


    % ----------------------  ABSTRACT -----------------------
    \newpage
    \addcontentsline{toc}{chapter}{Abstract}

    \begin{abstract}

        This thesis investigates the use of energy and data-efficient methods for deep learning-based financial volatility forecasting, aiming to reduce the computational cost of such models and demonstrate how the sustainability of deep learning for finance can be improved.

        \textbf{Context/background:} The financial sector has long been associated with largely negative environmental, social, and governance (ESG) impacts, including being a major contributor to global carbon emissions. Despite the attempts by some to prioritise \emph{sustainable finance}, the recent expansion of financial technology---incorporating new, expensive methods such as \emph{deep learning} (DL)---has only worsened the energy consumption attributed to this industry, accelerating its carbon emissions.
        
        \textbf{Aims:} In an attempt to address these negative impacts of financial technology, this project aims to develop an energy-efficient DL system for financial modelling. This research will explore \emph{Green AI} methods that attempt to reduce the energy expended training DL models and apply these for the first time to models used in finance. To exemplify the benefits of these methods, a performant financial volatility model will be developed that not only produces accurate results but prioritises generating this performance in an efficient manner, minimising the energy and data resources required during training. This system aims to demonstrate that the principles of Green AI are applicable within the financial sector, furthering the scope of sustainable finance by improving the sustainability of deep learning for finance and, hence, minimising the ESG impacts of the financial sector.
        
        \textbf{Method:} This research will commence with an analysis of the resource requirements of typical systems in the field of deep learning for finance. A particular focus will be given to the domain of financial volatility modelling, as this is a major application of deep learning in finance, and the \emph{long short-term memory} (LSTM) networks typically exploited for such tasks. Energy and data-efficient training methods will be explored, developing a deep model that consumes less energy and requires less data to train, but maintains accurate performance. Specifically, methods such as \emph{active learning}, \emph{progressive training}, and \emph{mixed-precision} will be explored that reduce the resource requirements of training, proving the feasibility of efficient models within this field.
        \\ \\ 
        \textbf{Contributions to science:} 
        \begin{enumerate}
            \item \emph{Expanding the applications of Green AI}. The first application of Green AI to the finance sector, further demonstrating the utility and importance of Green AI in lowering the environmental cost of deep learning. 

            \item \emph{Reducing the environmental impact of financial technology}. The improvement of sustainable finance to include the new research domain of \emph{sustainable deep learning for sustainable finance}. 

            \item \emph{Improving the inclusivity of finance}. Lowering the bar-to-entry to engage in deep learning for finance,  allowing more individuals to leverage financial technology and analytics.
        \end{enumerate}

        \textbf{\\Outline of research:} 
        \begin{itemize}
            \item \underline{Section \ref{section: baseline}: \emph{Baseline finanical volatility model}}. An initial deep model will be implemented, using a traditional training process and LSTM architecture, to act as an exemplar of the resource requirements of this domain.

            \item \underline{Section \ref{section: energy-extensions}: \emph{Energy-efficient training extensions}}. Several adaptations to the model training process will be made that prioritise reducing the energy consumed by the system.
            
            \item \underline{Section \ref{section: data-extensions}: \emph{Data-efficient training extensions}}. Additional adaptations will be made that reduce the necessary amount of training data, further lowering resource requirements.
            
            \item \underline{Chapter \ref{chapter: results-discussion}: \emph{Results \& discussion}}. An analysis will be made between the baseline and extended models, comparing the performance and efficiency of each, and discussing their success in reducing resource requirements.
        \end{itemize}
        
        \textbf{\\ \\Keywords:} Green AI, Green Deep Learning, Energy Efficiency, Data Efficiency, Sustainable Finance, Financial Technology, Financial Volatility Modelling, Long Short-Term Memory

    \end{abstract}


    % -----------------  ACKNOWLEDGEMENTS  -------------------
    \newpage
    \chapter*{Acknowledgements}
    \addcontentsline{toc}{chapter}{Acknowledgements}


    % -----------------  TABLE OF CONTENTS -------------------
    \newpage
    \tableofcontents


    % -------------------  LIST OF FIGURES --------------------
    \newpage 
    \listoffigures
    \addcontentsline{toc}{chapter}{List of Figures}


    % -------------------  LIST OF TABLES ---------------------
    \newpage
    \listoftables 
    \addcontentsline{toc}{chapter}{List of Tables}




    % --------------------  INTRODUCTION ----------------------
    \newpage
    \pagenumbering{arabic}
    \chapter{Introduction}
    \label{chapter: intro}

    \section{Topic \& Background}
    \label{section: topic}

    Many industries have recently been under increased pressure to monitor and rectify their environmental impact. This pressure is typically directed towards the perceived high carbon industries that constitute the major pollutant sectors of the economy, such as transport, energy supply, and agriculture. For example, recent estimates suggest that of the $33.5$ billion tons of carbon dioxide emissions generated globally in 2018, $8$ billion tons could be attributed to the transport sector \citep{iea-2022}, and $6$ billion tons to farming and livestock \citep{ahmad-2022}. These concerning figures have rightly sparked increased international discussion surrounding global carbon emissions and sustainability, such as the 2021 \emph{United Nations Climate Change Conference} (COP26).


    \subsection{Sustainable Finance}

    The finance sector has long been closely associated with sustainability concerns such as those discussed at COP26, being a major contributor to global carbon emissions both directly and indirectly. The most visible environmental impact of the financial industry is its direct emissions from business practices such as the distribution of cash through the economy (e.g. cash transport and ATM power consumption), card payment processing centres, and everyday operational costs such as heating office buildings \citep{hanegraaf-2018}. However, indirect emissions---attributable to services such as investing and lending---have been estimated to contribute over 700 times more to the carbon footprint of the financial industry than all direct emissions \citep{power-2020}. This form of carbon emissions, entitled \emph{financed emissions} by \citet{power-2020}, includes practices such as financing fossil fuel companies---who have received \$3.8 trillion in funding from global banks since the \emph{Paris Agreement} was signed in 2016 \citep{rainforest-2021}. In their survey of $700$ global financial institutions, \citet{power-2020} estimated that the production of over $1.04$ billion tons of carbon dioxide was attributable to financed emissions in 2020 (approximately $3.1\%$ of global emissions). However, they note this figure is likely to significantly understate the total global financed emissions, as of the $700$ contacted institutions only $332$ responded, and only $25\%$ of those reported financed emissions (typically on less than $50\%$ of their portfolios). Furthermore, a recent report by \emph{Greenpeace} and the \emph{WWF} concluded that the combined carbon emissions of the largest banks and investors in the UK in 2019 totalled $805$ million tons, which (if consolidated into its own country) would rank 9th in the global list of total emissions per country (Greenpeace, 2021). This figure is $1.8$ times higher than the total emissions of the UK ($455$ million tons), and almost $90\%$ of the global emissions from commercial aviation ($918$ million tons) in the same year \citep{graver-2020}.

    The increasing awareness of the negative \emph{environmental, social, and governance} (ESG) impacts of the finance industry, highlighted by studies such as those of \citet{power-2020} and \citet{greenpeace-2021}, has led researchers to investigate methods that prioritise the \emph{sustainable development goals} (SDGs) within the financial sector. Towards this objective, the field of \emph{sustainable finance} has emerged, which aims to consider ESG impacts and SDGs in financial decisions (such as investment and lending activities) to improve the sustainability of finance. Namely, despite the lack of a rigorous consensus on what constitutes sustainable finance, recent reviews---such as those of \citet{cunha-2021} and \citet{kumar-2022}---typically use the term to refer to research into financial activities, resources, and investments that prioritise long-term sustainability. In particular, a focus is given to those practices that produce a measurable positive improvement to the social and environmental impact of the financial industry, global economy, and wider society. 

    This discussion around sustainable finance largely began with \citeauthor{ferris-1986}'s examination of the benefits of investing pension funds in a socially responsible way. Following this, early research mainly focussed on \emph{socially responsible investing}, where investments are made that not only prioritise profits but further current positive social movements and mitigate societal concerns \citep{cunha-2021}. During the 2000s, research began to exhibit a new focus on environmental sustainability, considering factors such as climate change and renewable energy \citep{laan-2004}. Later research further pushed the scope and impact of environmentally-focused practices with the development of new domains such as \emph{climate finance} \citep{hogarth-2012}, where the mitigation of climate change is prioritised through investment and financing, and \emph{carbon finance} \citep{aglietta-2015}, which focusses on investments that seek to lower or offset carbon emissions. Recent research within sustainable finance has pushed this environmental focus further, aiming to put into practice the sustainability goals set out by the Paris Agreement, ESG factors, and SDGs. Specifically, a new interest has been taken in sustainable investment fields such as \emph{impact investing} \citep{agrawal-2021} and \emph{ESG investing} \citep{alessandrini-2020}, where investments are made that produce measurable improvements to environmental issues (according to criteria such as their ESG impacts). This recent focus has significantly increased the prominence and influence of sustainable finance. In their review of $936$ research papers, \citet{kumar-2022} found that almost $70\%$ of sustainable finance research had been published between 2015--2020, and an exponential trend was exhibited in the increase in papers being published each year; additionally, they found that the top three most cited papers all conducted research in the field of impact investing. Furthermore, \citeauthor{kumar-2022} assert that in 2020, \$400 billion of new sustainability funds were raised on capital markets. Hence, it is clear the scope and impact of sustainable finance is currently on the rise, predominantly driven by a renewed focus on the ESG impacts of financial practices, resources, and investments.


    \subsection{Financial Technology and the Issues with Sustainable Finance}

    Whilst the \$400 billion raised in sustainability funds seems impressive, in the same year the total US equity market value was over \$40 trillion \citep{siblis-2022}, meaning globally only $0.98\%$ of the value of the US equity market alone was raised. Furthermore, recent research has uncovered the prevalence of investment \emph{greenwashing} \citep{popescu-2021}, where institutions misleadingly classify their practices and investments as sustainable without credible data to back up their claims (and often excluding data that would suggest the opposite). In their review, \citet{cunha-2021} raised similar concerns, asserting that research into sustainable finance is currently ``excessively fragmented". These issues indicate that whilst attention is growing around the sustainability of financial practices, this domain is still not widely recognised, and further work and research are still necessary to increase the adoption of sustainable methods and tools within finance. 

    An additional concern is that the tools used to conduct financial practices are becoming increasingly resource-hungry at a pace exceeding the adoption rate of sustainable finance. A clear exemple of this is the increased adoption of technology throughout the finance industry. Recently, a surge of developments in financial technology (\emph{Fintech}) has revolutionalised the methods and practices used across the field of finance, from the large financial institutions and increasing number of Fintech startups, to groups of academic researchers. This Fintech revolution has transformed many aspects of finance, promising to enhance and automate existing financial services, and deliver new, innovative financial products. In their exploration of the evolution of Fintech, \citet{palmie-2020} assert that this adoption emerged in three waves. They suggest that it began with the utilisation of electronic payments and online banking, digitalising the world of finance; the second wave then came with the emergence of blockchain technology and cryptocurrencies, which further disrupted the way currency is stored and transacted. The third and most recent Fintech wave, \citet{palmie-2020} claim, is the current upwards trend in financial institutions' reliance upon \emph{artificial intelligence} (AI). Driven by the promise of increased automation and computing power, the utilisation of AI within the financial sector has been rapidly expanding over recent years, becoming a core component of many of the financial products and services used today: from accurate real-time financial fraud detection \citep{sadgali-2019} to automated analysis of financial statements \citep{amel-2020}.


    \subsection{Deep Learning for Finance}

    Because of their power and potential, AI methods have been used to produce state-of-the-art results over a plethora of scientific problems and research fields: from DeepMind's \emph{AlphaFold} \citep{jumper-2021}---the first programmatic solution to the age-old protein folding problem in Biology---to Google's \emph{PaLM} \citep{chowdhery-2022}---a cutting-edge human language model that delivers breakthrough results in multi-step arithmetic and common-sense reasoning (a major step towards \emph{artificial general intelligence}). This research typically revolves around the use of \emph{machine learning} (ML), where large collections of data are used to train computational models how to perform certain tasks independently \citep{samuel-1959}. Recent innovations in ML have increasingly taken advantage of \emph{deep learning} (DL) methods, which use large, complex models to produce state-of-the-art performance \citep{witten-2017}.

    The recent success in utilising DL---such as that of AlphaFold and PaLM---has driven an increased adoption of AI and DL further afield, such as within the finance industry. In fact, global spending on AI is predicted to double in value by 2024, from \$50 billion in 2020 to an estimated \$110 billion \citep{oecd-2021}. Specifically, the global AI Fintech market was estimated as being worth \$7.91 billion in 2020 and is forecast to grow to \$27 billion by 2026 \citep{mordor-2021}. Furthermore, in a survey of 206 executives from US financial service companies, \citet{gokhale-2019} found that $70\%$ used ML within their financial institutions for practices such as detecting irregular patterns in transitions, and building advanced credit models. In a similar survey, \citet{chartis-2019} found ML to be a core component in current financial technology, revolutionising data processing and modelling practices.

    In their survey of financial professionals, \citet{chartis-2019} discovered that $44\%$ of respondents cited ``greater accuracy of process and analysis" as a key motivation behind their adoption of AI methods. This superior accuracy provided by ML and DL methods---publicised through models like PaLM pushing the boundaries of computational accuracy---provides a compelling alternative to traditional statistical models. Hence, the promise of increased accuracy of performance is a major driving factor behind recent ML adoption within Fintech. For example, recent DL-based financial systems have been shown to predict borrower defaults with greater accuracy than achievable with traditional approaches \citep{albanesi-2019}.


    \subsection{The Issues with Deep Learning}

    Whilst cutting-edge DL models push the boundaries of computational accuracy, few of these systems prioritise the efficient use of energy and data. This has led to DL inflicting a great cost upon the environment, as the energy-intensive algorithms, long training phases, and power-hungry data centres they utilise inflict a high carbon footprint \citep{lacoste-2019}. \citet{schwartz-2019} label these accurate but energy-intensive DL models as \emph{Red AI}, which they define as ``research that seeks to improve accuracy through the use of massive computational power while disregarding the cost". \citeauthor{schwartz-2019} explain how these systems generate their performance gains majoritively through the use of extensive computational resources, such as complex models with vast parameter sets, large collections of data, and power-hungry computer hardware. \citet{bender-2021} illustrate this trend through the progression of recent language models: whilst the 2019's state-of-the-art model \emph{BERT} \citep{devlin-2018} used $340$ million parameters and a $16$GB dataset, the leading models of 2020 (\emph{GPT-3} by \citet{brown-2020}) and 2021 (\emph{Switch-C} by \citet{fedus-2021}) utilised $175$ billion and $1.57$ trillion parameters respectively, and data sets of size $570$GB and $745$GB. In fact, between 2012 and 2018 the computational resources used to train cutting edge models increased by a factor of $300,000$, outpacing \emph{Moore's Law} \citep{amodei-2018}.

    The intense computational load of these DL models does not come for free; the large parameter and data sets mean training, storing, and computing with these models draws a significant amount of energy---referred to by \citet{bietti-2019} as \emph{data waste}. Partly due to this inefficiency, the data centres at which DL models rely upon for storage and cloud computing become a significant hidden contributor to carbon emissions \citep{aljarrah-2015}. Studies such as \citet{masanet-2020} and \citet{malmodin-2018}, 2020 have estimated that processing at these data centres consumes around $200--250$TWh of electricity a year, with the cost of data transmission exceeding this at $260--340$TWh per year \citep{iea-2022}. These estimates suggest that global data centres use more electricity than the majority of countries in the world, ranking above both Australia and Spain \citep{eia-2019}, and account for around $1\%$ of global electricity consumption (rising to $2.4\%$ when including transmission costs). Furthermore, this energy is likely not entirely carbon-neutral; \citet{cook-2017} showed that of their total electricity demand, \emph{Amazon Web Services} only powered $12\%$ through renewable sources, and \emph{Google Cloud} $56\%$ (with the latter figure ranging between $4\%$ and $94\%$ depending on location). These figures are also somewhat unrepresentative of the true carbon emissions of such companies and facilities, as they report net emissions including carbon offsetting measures such as purchasing carbon credits, which \citet{schwartz-2019} argue have negligible impact on mitigating the environmental consequences of this work. This means that these large energy budgets generate considerable carbon emissions, resulting in the use of data centres coming alongside significant environmental detriment. Specifically, research suggests that in 2018, cloud computing at data centres generated the equivalent of $31$ million tons of carbon dioxide \citep{hockstad-2018} in the US alone, equaling the total emissions generated by electric power in the state of California \citep{iea-2022}. Furthermore, the digital technology sector as a whole is estimated to be responsible for $4\%$ of global carbon emissions, with this figure forecast to double by 2025 \citep{bietti-2019}.

    Beyond these general figures, \citet{strubell-2019} showcased the carbon emissions specifically produced by training DL models. They found that training the language model BERT, which utilised 110M parameters and trained for 96 hours over 16 TPUs, produced the equivalent of 1438 lbs of $CO_2$---the same as a trans-American flight. \citeauthor{strubell-2019} also found that whilst the \emph{Evolved Transformer} of \citet{so-2019} improves state-of-the-art accuracy in English-German translation by $0.1$ \emph{BLEU} (a common metric of translated text quality), if implemented on GPU hardware this model could cost \$3.2 million to train (or \$147,000 if using TPUs), and generate $626,155$lbs of $CO_2$ (almost five times the lifetime emissions of an average car).

    It is important to note that the large financial cost associated with these intensive DL models also inflicts a great social cost. Namely, as the systems used at the forefront of DL research get larger and more complex (such as $175$ billion parameters and $745$GB dataset of  Switch-C), the price of storing the model and its training data, as well as the cost of running its training process on the specialist hardware this would require, becomes prohibitively high \citep{schwartz-2019}. This financial barrier restricts who can engage in cutting-edge research to only those with the backing of a large institution. Thus, this lack of accessibility drives a \emph{``rich get richer"} cycle of research funding \citep{strubell-2019} where only research driections within the interest of these institutions receive enough funding. This not only stifles creativity, but leaves the allocation of who benefits from the development of these systems, and who bears the negative side effects, to a handful of large corporations. In particular, \citet{bender-2021} highlight this disconnect between the benefits of energy-intensive DL research and the environmental consequences it inflicts (using the example of language models (LMs)): ``is it fair or just, for example, that the residents of the Maldives [\ldots] pay the environmental price of training and deploying ever larger English LMs".

    Hence, whilst DL been shown to provide state-of-the-art computational accuracy across a range of fields, its environmental impact cannot be ignored. Therefore, the accelerating reliance on ML and DL within Fintech poses issues for its sustainability, as this the models and methods contribute further to the negative ESG impacts of the financial sector. This issue generates a clear conflict between the growing use of DL in Fintech, and the growing need for sustainable finance. Both of these fields provide great utility to the finance sector: Fintech (including DL) provides innovative services to consumers and accurate tools for institutions, and sustainable finance ensures the industry has minimal ESG problems. Moreover, DL has been shown to have utility for work in sustainability further afield, such as improving the efficiency of exploiting renewable energy sources \citep{daniel-2021}, and even within sustainable finance itself, for example using ML to analyse the ESG factors of potential investments \citep{mehra-2022}. For these reasons, a compromise between the use of DL Fintech and the prioritisation of sustainable finance must be reached. 


    \subsection{Green AI}

    This apparent gap between the utility of DL systems---provided by their superior accuracy over traditional methods---and their environmental consequences has recently started to gather attention from ML researchers. In what has become known as \emph{Green AI} \citep{schwartz-2019}, new research has begun to consider how to mitigate the negative ESG impacts of ML by improving the efficiency of DL models. These efficient models reduce the energy required for training and deployment, minimising the carbon emissions they contribute towards. 

    Alongside \emph{mobile computing}, which prioritises energy-efficient methods due to the hardware constraints of mobile devices, the research domains of \emph{Natural Language Processing} (NLP) and \emph{Computer Vision} (CV) are currently the predominat fields focussing on Green AI, as both these areas exploit highly complex models, and require efficient real-time processing when deployed. NLP focuses on the processing and understanding of language (e.g. using language models to make predictions about text sequences), typically using large, complex DL models in an attempt to match and exceed human accuracy in language modelling (e.g. \citet{chowdhery-2022}). CV typically uses complex models with expensive methods---such as the \emph{convolution operation} \citep{dumoulin-2018}---to analyse, classify, and map visual environments. Within these fields, Green AI developers have been working to promote the utilisation of efficient methods that reduce the energy, time, and data requirements of model training by using parameters, data, and operations more intelligently (without producing a significant drop in accuracy). However, the major limitation of Green AI is that these methods have yet to garner significant attention outside of the specific research domains of NLP, CV, and mobile computing. Therefore the utility of these methods to improving the sustainability of ML has largely not been demonstrated further afield, meaning their potential benefits to reduce the energy consumption and carbon emissions of ML have yet to be seen on a wide scale.


    \section{Research Motivations}
    \label{section: motivations}

    Due to the energy-intensive nature of high-performance DL systems, it is clear their usage comes alongside several environmental and social issues, including their significant carbon footprint and financial cost. For this reason, the accelerating adoption of ML and DL methods within Fintech raises concerns about increasing the negative ESG impacts of the financial industry. Namely, the side effects of these complex DL models produced by their large parameter and data sets and long training phases (and hence high energy budgets) are in direct conflict with the general global effort to reduce carbon emissions, and the specific goal of sustainable finance to reduce the negative ESG impacts of the financial sector. However, the benefits of DL to finance (and indeed sustainable finance) have been shown, such as improved fraud detection \citep{sadgali-2019} and ESG data analysis \citep{mehra-2022}, making their continued adoption inevitable. 

    Therefore, to minimise the negative ESG impacts of the financial sector in light of the recent Fintech revolution (and solidify the benefits of sustainable finance), the adoption of Green AI principles and methods is paramount. In the first study of its kind, this thesis investigates how the energy-efficient Green AI methods can be adapted for DL in finance. This research aims to demonstrate how the Fintech revolution (in particular the expanding use of DL) does not have to come with significant environmental and social costs, and how the models and methods used in this field can coincide with the SDGs of sustainable finance. To exemplify the promise of introducing Green AI to Fintech and sustainable finance, energy-efficient methods will be applied to one of the most popular applications of DL in finance: \emph{financial risk modelling}. Specifically, a popular and important area of financial risk analysis known as \emph{volatility forecasting} will be explored, developing a DL-based model that accurately predicts the future volatility (a statistical measure of dispersion) of the \emph{S\&P 500} market, but takes minimal energy and data to train. 

    Volatility forecasting is commonly used to give insight into the risk of a financial market or asset (French et al., 1987); it is also been extensively explored by researchers to showcase the prediction capabilities of DL: for example, \citet{xiong-2015} combine S\&P 500 and Google domestic trends data to model stock volatility through DL, and \citet{zhang-2022} use DL for forecasting intraday volatility. Hence, as financial risk modelling and analysis is often cited as a core application of DL in finance (such as in the reviews of \citet{sezer-2019}, \citet{ozbayoglu-2020}, and \citet{thakkar-2021}, volatility forecasting has been chosen as the specific application within Fintech to demonstrate the promise of applying Green AI methods to the finance industry.

    By showing the utility of Green AI within Fintech, this thesis aims to advance the field of sustainable finance as a whole, creating a new field of \emph{sustainable deep learning for sustainable finance}. This work is the first study to address the conflict of interest between sustainable finance and the growing reliance of Fintech on DL systems with high energy consumption and concerning ESG impacts. Hence, the following research aims to demonstrate how the energy-efficient methods proposed by Green AI research (in the fields of NLP, CV, and mobile computing) can be exploited within the DL systems used in finance, increasing the scope and impact of sustainable finance by allowing it to use helpful DL systems without compromising SDGs, and reducing the emissions of Fintech in general.


    \section{Contributions to Science}
    \label{section: contributions}

    This research contributes to scientific literature and the finance industry in a number of ways:

    \begin{enumerate}
        \item \emph{Expanding the applications of Green AI}. This thesis is the first application of Green AI principles and methodology to the field of finance. This will further demonstrate the utility and promise of Green AI by proving that such systems can provide compelling performance with a lower environmental cost in new domains outside of the existing research focus on NLP, CV, and mobile computing, expanding the scope of this field.
        \item \emph{Reducing the environmental impact of financial technology}. The following work combines the research fields of FinTech, deep learning for finance, sustainable finance, and Green AI, to create the new research domain of \emph{sustainable deep learning for sustainable finance}. This further improves the ESG impact of the financial sector beyond previous work in sustainable finance by mitigating the conflict between the utility of DL models for sustainability modelling and the intrinsic carbon footprint of these energy-intensive systems.
        \item \emph{Improving the inclusivity of finance}. The reduction in the financial, environmental, and social cost of DL for finance also increases the inclusivity of this field. Improving the energy efficiency of these models creates a lower bar-to-entry to using DL in finance, allowing more industry players, developers, and individual traders to utilise the advantages brought by Fintech and DL (for example, accessing improved analytics that allow more informed financial decisions).
    \end{enumerate}


    \section{Research Objectives \& Structure}
    \label{section: structure}

    The initial objective of this research is to analyse the efficiency of a baseline DL model used for financial volatility forecasting. In particular, the resource requirements in terms of training time and training dataset size are inspected, from which an estimate can be produced of the carbon emissions associated with training such a model. A particular focus is given to the use of \emph{long short-term memory} (LSTM) networks, as these are the typically used DL model for sequence forecasting tasks in general, and recent research into volatility forecasting with DL (for example by \citet{xiong-2015}). This network and training process is implemented in \emph{Python} with the popular DL framework \emph{TensorFlow} \citep{abadi-2016}.

    The baseline model is then extended with energy-efficient methods proposed by the field of Green AI. These methods adapt the way in which the LSTM-based model is trained, minimising its resource requirements. Specifically, energy-efficient approaches such as \emph{mixed-precision training} and \emph{progressive training} are explored, and their benefits shown. Beyond these initial adaptations, data-efficient extensions to the training process are additionally be explored, such as the use of active learning. These methods, also adapted from Green AI research, further reduce the resource requirements of a DL model, reducing the amount of training data necessary and thus also reducing the memory and energy required by the DL training process.

    Given these energy and data-efficient model extensions, an extensive analysis of the benefit of Green AI methods to this domain is made, quantifying the reduction in time and energy (and hence carbon emissions) made by these efficient methods. This compares both the accuracy of the final model and each energy and data-efficient extension to the original baseline model. The resource requirements (in terms of time, data, and energy) required for training will then be compared, to evaluate any accuracy-efficiency compromises made. A general conclusion is then settled upon as to the utility and viability of Green AI methods to Fintech, sustainable finance, and the finance industry in general.
    \\ \\
    The aforementioned research will be organised as follows: 

    \begin{itemize}
        \item \underline{Chapter \ref{chapter: literature}: \emph{Literature review}}. The relevant literature to this research is first explored to give an overview of the key concepts and methodologies utilised in the following experimentation. This begins with a general exploration of machine learning, which then leads to a more refined discussion of the use of DL in finance, and the specific DL methods and concepts focussed upon within this thesis, giving a clear outline of the domain of financial volatility forecasting. Following this, the review explores the research domain of Green AI, with a focus on the specific methods and practices used in Green AI research which will go on to be a core aspect of this thesis.
        
        \item \underline{Section \ref{section: baseline}: \emph{Baseline finanical volatility model}}. An explanation and analysis of the baseline volatility model implemented, exemplifying the typical models, methods, and resource requirements of this domain.

        \item \underline{Section \ref{section: energy-extensions}: \emph{Energy-efficient training extensions}}. Once the baseline model has been implemented, several energy-efficient adaptations to its training process shall be explored. This section gives a complete overview of these methods, explaining how they work, the motivations behind their use, and their utility in decreasing the energy consumption of training.
        
        \item \underline{Section \ref{section: data-extensions}: \emph{Data-efficient training extensions}}. The utilised methods to reduce the data requirements of training are then be explored, discussing their origins within Green AI and explaining how they aim to reduce the data requirements and computational cost of DNNs.
        
        \item \underline{Chapter \ref{chapter: results-discussion}: \emph{Results \& discussion}}. Once the detail of the utilised model and extensions has been thoroughly explained, the results of the experimentation are discussed, evaluating the accuracy and efficiency of each method to deduce the success of the application of Green AI to finance.
    \end{itemize}


    % --------------------  LITERATURE REVIEW ----------------------
    \newpage
    \chapter{Background \& Literature Review}
    \label{chapter: literature}

    To understand how Fintech can be aligned with the goals of sustainable finance, first, a detailed understanding of ML and DL must be conveyed, and then the avenues used in Green AI to improve the efficiency of these methods can be effectively conveyed. This chapter achieves this by initially introducing the relevant background literature within the field of ML, centring the research of this thesis within the context of existing scientific literature. Specifically, this begins with a general summary of the components and characteristics of ML and DL methods, before focussing on the specific models and structures used within the chosen application of financial volatility modelling. The second part of the chapter outlines the field of Green AI, its motivations, applications, and the relevant tools and practices used in this domain to improve the energy efficiency of DL. These overviews of DL and Green AI act as the basis for the experimentation and analysis that follow within this thesis. Hence, the chapter concludes by drawing together these concepts to evaluate the current state of this research domain and identify the research gaps focussed upon by this thesis.


    \section{Machine Learning}

    The field of machine learning, first discussed by Arthur Samuel in his 1953 exploration of ``mechanical brains" \citep{samuel-1959}, is a subset of artificial intelligence concerned with using data to allow computer programs to independently learn how to complete a given task without explicit information about the rules of such task \citep{samuel-1959}. This learning typically involves a \emph{training} procedure, where the program is supplied with instances of experience from the training dataset describing the problem to be solved. The program learns from dataset $D$ how to perform the desired task $T$ by taking input of a data instance and outputting the result it believes is correct in this scenario. This result is then evaluatied through some performance metric $P$ (typically computed through a \emph{loss function}). For example, in a \emph{prediction} task, the program takes input of a sequence of values relating to some variable and outputs what it predicts to be the next value in the sequence: e.g. \citet{xiong-2015} use a dataset of S\&P 500 values and Google domestic trends to predict future stock market volatility. Alternatively, in a \emph{classification} task, a data point is input into the program to be assigned to a distinct class: e.g. \citet{sadgali-2019} use financial transaction data to classify whether a partiuclar transacition is fradulent or not. Such tasks are typically trained through \emph{supervised learning}, where the correct result (known as the \emph{label}) is provided to the program after it has produced its output, and the correctness of its solution is evaluated through the given metric $P$ (producing an \emph{accuracy} value), and its bahaviour adjusted to maximise performance according to $P$.


    \subsection{Neural Networks}

    Since the conception of machine learning, a core goal has been the development of algorithms that can accurately mimic the processing mechanisms of the human brain. These algorithms, known as \emph{artificial neural networks} (ANN), typically centre around neural models that recreate a version of the complex system of communications between neurons in the brain. The first implementation of such a model was Frank Rosenblatt's \emph{Mark I Perceptron} \citep{rosenblatt-1958}, which attempted to perform binary classification of images. Each pixel in the input image was represented as a single value in a 2-dimensional matrix of input neurons known as the \emph{input layer}. These values were then passed to the single internal neuron through \emph{weighted channels} to compute the weighted sum of all input values,  the result of which was passed through an \emph{activation function} to normalise the result to zero or one—representing an output classification of 'class zero' or 'class one'. 


    \subsection{Deep Learning}
    \label{section: deep-learning}

    Whilst Rosenblatt's model produced underwhelming results due to its simplicity, its design became a fundamental building block of the larger, more complex ANNs used today. Namely, modern networks build on this approach by having wider and deeper \emph{model architectures} consisting of more internal neurons arranged in multiple layers. This approach of using \emph{multi-layer perceptrons} was popularised by \citet{rumelhart-1986}; however, recent work has pushed the boundaries of neural network performance by designing deeper and deeper architectures---known as \emph{deep neural networks} (DNNs)---to establish the field of deep learning. These deep networks perform computations in a similar vein to Rosenblatt's perceptron. Information is passed between the input layer $l^{(0)}$ and output layer $l^{(L-1)}$ along weighted channels between neurons, with the \emph{activation value} $a^{(i)}_n$ assigned each neuron $n$ in layer $i$ being determined through the dot product between the weights $W^{(i-1, i)}$ of channels entering $n$ and the values $x^{(i-1)}$ of neurons in the preceding layer $i-1$ connected through those channels \citep{witten-2017}. This product is then summed with a \emph{bias} term $b^{(i)}$, and passed through a chosen activation function $\alpha$ (Equation \ref{eq: activation-value}).

    \begin{equation}
        \label{eq: activation-value}
        a^{(i)}_n = \alpha( W^{(i-1, i)} \cdot x^{(i-1)} + b^{(i)} )
    \end{equation}

    This computation is performed over all layers of the network, starting with the input layer---whose neurons take the values of the input vector---and propagating through to the output layer—the values of which are the final network output. During training, the values of the network's parameters (i.e. the elements of weight matrix $W$ and bias vector $b$) are adjusted as the model learns, manipulating its outputs to more closely align with the true labels. The full training process is typically implemented over a predetermined number of iterations. Within each iteration (known as an \emph{epoch}) subsets of data of a given size (referred to as the \emph{batch size}) are sampled from the training dataset and fed into the network, over which the aforementioned computations are made and the network's parameters tuned. \emph{Backpropagation} can be used to implement this tuning process, where the gradient of loss function with respect to the parameters is determined by propagating the error of the network on the current input backwards through all its layers \citep{zaras-2022}. These parameters can then be adjusted in the direction of the negative gradient through \emph{gradient descent} (towards the minimum of the loss function).


    \subsection{Sequence Modelling Problems}

    In machine learning, a \emph{sequence} is an ordered set of data elements, each of uniform type and dimension: for example, a stream of text can be seen as a sequence of strings (i.e. words), or a video can be modelled as a sequence of pixel matrices (i.e. frames of the video). Mathematically, temporal sequences---typically referred to as \emph{time series}---can be denoted by their data elements and time indices: 

    \begin{equation}
        \label{eq: timeseries-full}
        seq = \langle x_1, t_1 \rangle, \ldots, \langle x_T, t_T \rangle 
        \text{.}
    \end{equation}

    These sequences are commonly abbreviated to simply denote their data elements x, indexed over the time t that they were observed:

    \begin{equation}
        \label{eq: timeseries}
        seq = x_1, x_2, \ldots, x_t, \ldots, x_T
        \text{.}
    \end{equation}

    \emph{Sequence modelling} (also known as \emph{sequence learning}) involves using a machine learning model, known as a sequence model, to analyse and generate sequences. The problem is common in fields such as NLP, due to data being naturally ordered in sequences (e.g. sentences of text): for example, Google's \emph{Universal Sentence Encoder} \citep{cer-2018} is a state-of-the-art DNN for text embedding (the task of encoding sentences as vectors for performing further NLP tasks such as text classification).

    Sequence modelling typically takes three main avenues. Firstly, the \emph{sequence-to-vector} problem takes input of a sequence of data elements $x_1, x_2, \ldots, x_T$ up to timestep $T$ (one element at a time) and outputs a single value, or vector of values (either at each timestep or delayed to after the end of the sequence). For example, in \emph{part-of-speech tagging}---a ML application in NLP extensively reviewed by \citet{chiche-2022}---a model reads elements of a text sequence, and uses the current instance (a particular word in the sequence) and its context (the previously seen words) to classify elements of the sequence grammatically as adjectives, nouns, verbs, etc. In this case, whee each grammatical tag would be allocated a numerical label, and the output would be a vector of probabilities over all tags from which we could select the most probable predicted tag.

    Secondly, the \emph{vector-to-sequence} problem completes the opposite task, taking input of a vector of values and generating a corresponding output sequence. Artificial text generators (e.g. OpenAI's GPT-3 \citep{brown-2020} and Google's PaLM \citep{chowdhery-2022}) are a representative example of this problem, where an output sequence of text is artificially created based upon a vector of input parameters specifying the desired properties of the text.

    The final, and most common, form of sequence modelling is the \emph{sequence-to-sequence} problem, where both the inputs and outputs of the model are a sequence of elements. This task typically involves either \emph{translation}, where the $M$ elements in an original sequence are converted into a new sequence of length $N$, or \emph{prediction}, where the original sequence of elements $x_1, \ldots, x_M$ is used to predict a subsequent sequence $x_{M+1}, \ldots, x_{M+N}$. \emph{Machine translation} is one such application, in which a model reads a sequence of text written in one language, then translates this text into a different language, output as a new sequence: for example, Meta AI's \emph{M2M-100} \citep{fan-2020}, the first language model that can directly translate between any pair of 100 different languages. Prediction tasks are typically used for \emph{time series forecasting}, where the value of data elements is predicted over $N$ future timesteps, in such applications as \emph{stock price prediction} (extensively surveyed by \citet{sezer-2019}). In this case, the output time series $y = x_{T+1}, \ldots, x_{T+N}$ is computed based upon the context of the past time series $x_0, \ldots, x_T$. Specifically, a model $M_{\theta}$ (parameterised by $\theta$) is build that approximates the mapping $y = f( x_0, \ldots, x_T \vert \theta )$ between past and future time series.

    \section{Recurrent Neural Networks}

    \subsection{Neural Networks for Sequence Modelling}

    The task of forecasting future values of a sequence typically poses a challenge for traditional ANN and DNN architectures. A major reason for this is the fixed input structure of a typical ANN, meaning all elements of an input sequences would have to be fed into the network at the same time, with one element per neuron in the input layer. Hence, the input would contain no intrinsic sense of order, severely limiting the amount of analysis possible by the network, as the ordering of sequences such as time series is crucial to understanding and forecasting them \citep{tsantekidis-2022}. The chosen size of the input layer would also pose a challenge for a network with a fixed input structure, as the number of input neurons would have to be made adequately large to accommodate the possible variations in sequence lengths (e.g. the length of a time series of previous values grows as the current timestep being considered moves forward in time). Furthermore, to learn a sequence’s characteristic behaviour requires persistence of information independent from its location within the sequence. Namely, the network must contextually use previous understandings to inform the current instance, wheverver they appeared in the preceding sequence. This is not possible with a traditional ANN as the parameters of channels connected to different input neurons are not shared. Therefore, when the parameters of a specific neuron and its channels are adjusted after seeing an important pattern in part of a sequence, this information will only prove useful if that patten appears again in the exact same location on the input neurons, as if the pattern occurs at a different position it will be input through different neurons with different parameters that have not been previously learned to deal with this subsequence.


    \subsection{Recurrent Networks}

    To overcome these issues with traditional ANNs, \emph{recurrent neural networks} (RNNs) were conceived that utilise recurrent loops to allow varying input lengths and the persistence of learned sequential information \citep{sharma-2022}. These networks take input of a single sequence element $x_t$ at each timestep, using this and the context of previously observed sequence elements to inform the internal state $h_t$ of the network which acts as a form of short-term memory. This state is updated sequentially at each timestep $t$ through the function $g$, parameterised by $\theta^{(g)} = \{ W^{(g)}, U^{(g)}, b^{(g)} \}$, where $W^{(g)}$ and $U^{(g)}$ are weight matrices, and $b^{(g)}$ a bias vector. Namely, the function updates the previous state $h_{t-1}$ with new useful information learned from the input $x_t$ \citep{sharma-2022}:

    \begin{align}
        \label{eq: rnn-state}
        h_t &= g( h_{t-1}, x_t \vert \theta^{(g)} ) \\
        &= W^{(g)} \cdot h_{t-1} + U^{(g)} \cdot x_t + b^{(g)}
        \text{.}
    \end{align}

    The state $h_t$ is then output by the network along a recurrent loop that connects the output of the network at time $t-1$ to the input of the network at the next timestep $t$. This allows information to be passed between successive versions of the same neural network at different points in time; namely, the network itself stays the same at each computational step, it is only the internal state that changes. The concept can be demonstrated by unrolling the network in time, shown in Figure \ref{fig: rnn-diagram}. \citet{tsantekidis-2022} note that to avoid confusion is important to remember that the sequence of networks depicted in illustrations such as Figure BELOW are in fact just the same network only at different points in time, with the connecting arrows between each signifying that the output at time $t-1$ is being passed on to the same network at the subsequent timestep $t$.

    \begin{figure}[ht]
        \label{fig: rnn-diagram}
        \centering
        \includegraphics[width=0.7\textwidth]{rnn.png}
        \caption{\centering Diagram of a RNN at a single timestep $t$ (left) and unrolled in time between timesteps $0$ and $t$ (right)}
    \end{figure}

    To capture more information from the input sequences, RNNs can consist of multiple internal layers (shown in Figure BELOW), allowing them to learn higher-dimensional internal representations \citep{bengio-2009}. In this case, each layer $i$ retains its own internal state $h^{(i)}_t$, which is passed on to both the next layer of the network to compute its state $h^{(i+1)}_t$ (at the current input timestep) and reccurred back into itself at the subsequent timestep to compute the state $h^{(i)}_{t+1}$ \citep{zhang-2021}. 

    \begin{figure}[ht]
        \label{fig: deep-rnn}
        \centering
        \includegraphics[width=0.4\textwidth]{deep-rnn.png}
        \caption{\centering Diagram of a multi-layer RNN (showing layers $1$ to $L$) with each layer unrolled in time between timesteps $1$ and $T$}
    \end{figure}


    \subsection{Training}

    Training of RNNs can be conducted through a similar backpropagation process to that used on traditional DNNs. This is done through computing the final output of the network on a given sequence (i.e. the output state at the final timestep) and evaluating the loss function between this and the true result, finding the total error of the RNN. The error is then propagated backwards through both the internal layers of the network and through time to the network state at each timestep of the input sequence---and hence is referred to as \emph{backpropagation through time} \citep{zhang-2021}. This allows the training procedure to adjust both the network parameters (i.e. weights $W$ and biases $b$) and the parameters $\theta^{(g)}$ of the state function $g$ determining how exactly the internal state $h^{(i)}_t$ is updated at each timestep.


    \subsection{Applications}

    Much recent research into sequence learning has exploited recurrent networks to conduct accurate modelling. In their review of the applications of RNNs within this domain, \citet{lipton-2015} explain how NLP tasks and time-series forecasting are the most common avenues explored by researchers utilising RNNs. These two areas have been extensively surveyed in literature; in their review of DL for NLP, described the rapidly increasing popularity of RNNs in recent years, highlighting language modelling, machine translation, speech recognition, and image captioning as major areas of recent process. They highlight the RNN model of \citet{karpathy-2015} for generating image descriptions---which significantly outperformed existing baselines---as a illustrative example of the benefits provided by this architecture. Additionally, \citet{hewamalage-2021} survey the current and future applications of RNNs for time series forecasting, highlighting the recent success of this model architecture at forecasting competitions, such as an RNN-based model by \citet{smyl-2020} winning The M4 Competition in 2019 with cutting-edge accuracy nearly $10\%$ greater than the utilised baseline \citep{makridakis-2020}.


    \section{Long Short-Term Memory}

    \subsection{The Issue with Recurrent Networks}

    The traditional RNN described above is incredibly effective at learning short-term dependencies within an input sequence, and exploiting these to make accurate predictions of subsequent values. In many cases, however, to understand the full context of a sequence longer-term dependencies need to be taken into account. Unfortunately, research into the effectiveness of recurrent networks by \citet{hochreiter-1991} and \citet{bengio-1994} has shown that RNNs cannot accurately capture or exploit long-term dependencies within sequences, as previous elements that are contextually relevant but were observed many timesteps ago can be forgotten prematurely. This issue is known as the short-term memory problem of RNNs, and is caused by the \emph{vanishing gradient problem} \citep{hochreiter-1991}. This issue is inherent to the backpropagation through time algorithm and the nature of recurrent variables. As demonstrated in Equation \ref{eq: rnn-state}, to calculate the value of a single recurrence involves multiplying by the weight matrix $W^{(g)}$: ignoring the bias and input element terms (as these aren't recurrent), we have $h_t = W^{(g)} \cdot h_{t-1}$. This poses a challenge when a recurrence is made over many steps, which is the case when evaluating the recurrent state function over many timesteps in a long sequence, as the same weight matrix is used over all recurrent state computations (shown in Equation \ref{eq: reccurent-weights} for a length $T$ sequence).

    \begin{align}
        \label{eq: reccurent-weights}
        h_T &= W^{(g)} \cdot W^{(g)} \cdot \ldots \cdot W^{(g)} \cdot h_0 \\
        &= (W^{(g)})^T \cdot h_0
    \end{align}

    Hence, the calculation is dependent on the computation of the weight matrix product $(W^{(g)})^T$; this means that if the parameters in the weight matrix are below one (i.e. we have $W^{(g)} < 1$) then the value of $(W^{(g)})^T$ will tend to zero with increasing sequence length $T$. This causes the gradient of the loss function with respect to the parameters $W^{(g)}$ to also tend to zero during backpropagation, meaning no significant updates are made to the values of $W^{(g)}$ when training on long sequences. Therefore, the gradient is said to \emph{vanish}, causing an inability of the network to learn dependencies over long sequences \citep{bengio-1994}.


    \subsection{Long Short-Term Memory Networks}

    Whilst several solutions to the vanishing gradient problem have been proposed---such as \emph{skip connections} and \emph{leaky recurrent units} \citep{pascanu-2012}---by far the most widespread approach is the use of \emph{Long Short-Term Memory} (LSTM), presented by \citet{hochreiter-1997}. An LSTM network is a RNN that uses multiple internal states and parameter matrices to mitigate the vanishing gradient problem and model both short-term and long-term dependencies within sequences. Similarly to an RNN, this architecture contains a core network consistent of one or more layers---where each layer is a distinct LSTM cell---and information is passed both between these cells (from the input to the output layer of the network) and between cell and and itself at the next timestep (reccuring the output at time $t-1$ to be input at time $t$). However, instead of the single recurrent state $h_t$ of the simple RNN, the LSTM contains two: a short-term memory state $h_t$ (analagous to that of a RNN) and long-term memory state $c_t$. This long-term state $c_t$ is used to retain information from far back in the input sequence, adding and forgetting relevant knowledge as the context of the sequence changes. 

    \begin{figure}[ht]
        \label{fig: lstm}
        \centering
        \includegraphics[width=0.7\textwidth]{lstm.png}
        \caption{\centering Diagram of a the arrangement of logical gates within a LSTM cell}
    \end{figure}

    At each timestep, the preceding long-term state $c_{t-1}$ is input into the LSTM cell (alongside the current sequence element $x_t$) to compute the new state $c_t$. These updates are achieved through several logical \emph{gates} within the network (between each cell and timestep). Firstly, the \emph{forget gate} is used to decide what information to discard from the previous state $c_{t-1}$. The previous short-term state $h_{t-1}$ and input element $x_t$ are multiplied with the forget gate's weight matrices $W^{(F)}$ and $U^{(F)}$ and added to its bias $b^{(F)}$, the result of which is passed through the \emph{sigmoid} activation function to produce the intermediate state value $\tilde{c}^{(F)}_t$, shown in Equation \ref{eq: forget-gate} \citep{zhang-2021}.

    \begin{equation}
        \label{eq: forget-gate}
        \tilde{c}^{(F)}_t = \sigma( W^{(F)} \cdot h_{t-1} + U^{(F)} \cdot x_t + b^{(F)} )
    \end{equation}

    Secondly, when new relevant information is seen in the input sequence, the \emph{input gate} is used to add it to the long-term memory state $c_t$. This process similarly uses the short-term state $h_{t-1}$ and current input $x_t$, but evaluates both a sigmoid and \emph{hyperbolic tangent} activation function in parallel on these inputs to decide both which values of $c_{t-1}$ must be updated (through the sigmoid evaluation) and by how much (the tangent evaluation). The result of these two operations are then multiplied together to give the second intermediate state value $\tilde{c}^{(I)}_t$, shown in Equation \ref{eq: input-gate} \citep{zhang-2021}.

    \begin{align}
        \label{eq: input-gate}
        (\tilde{c}^{(I)}_t)_{\sigma} &= \sigma( W^{(I,0)} \cdot h_{t-1} + U^{(I,0)} \cdot x_t + b^{(I,0)} ) \\
        (\tilde{c}^{(I)}_t)_{\tanh} &= \tanh{( W^{(I,1)} \cdot h_{t-1} + U^{(I,1)} \cdot x_t + b^{(I,1)} )} \\
        \tilde{c}^{(I)}_t &= (\tilde{c}^{(I)}_t)_{\sigma} \cdot (\tilde{c}^{(I)}_t)_{\tanh}
    \end{align}

    Both intermediate state values $\tilde{c}^{(F)}_t$ and $\tilde{c}^{(I)}_t$ are combined to give the new updated long-term state $c_t$ (Equation \ref{eq: long-state}).

    \begin{equation}
        \label{eq: long-state}
        c_t = \tilde{c}^{(F)}_t \cdot c_{t-1} + \tilde{c}^{(I)}_t
    \end{equation}

    The third and final gate used by LSTMs is the \emph{output gate}, which decides what each cell outputs. This is used as both the short-term state $h_t$ and the final network output $y$. The output gate uses the newly computed long-term state $c_t$, input $x_t$, and previous short-term state $h{t-1}$; a biased weighted sum of $h^{t-1}$ and $x_t$ is computed and fed into the sigmoid function, which is then multiplied by the hyperbolic tangent function evaluated on $c_t$ (Equation \ref{eq: output-gate}). Similarly to within the input gate, the sigmoid evaluation determines the values of $h_{t-1}$ to be updated, and the tangent evaluation determines by how much \citep{zhang-2021}.

    \begin{align}
        \label{eq: output-gate}
        (h_t)_{\sigma} &= \sigma( W^{(O)} \cdot h_{t-1} + U^{(O)} \cdot x_t + b^{(O)} ) \\
        (h_t)_{\tanh} &= \tanh{( c_t )} \\
        h_t &= (h_t)_{\sigma} \cdot (h_t)_{\tanh}
    \end{align}

    This system of gates occurs in every LSTM cell, passing information from the input to output layer of the network at every timestep in the form of the short and long-term states. When the network reaches the final timestep, the short-term state is output as the final network output $y$; note, if the sequence modelling problem involves outputting a generated sequence of length $N$ (e.g. forecasting multiple future values of a time series), this is produced one element at a time over $N$ additional timesteps that take no input and output the value $y_i = h_{T+i}$.


    \subsection{Applications}

    Whilst reviewing modern RNN architectures, \citet{lipton-2015} note that most state-of-the-art applications within the field of sequence learning use a LSTM-based model. Similarly,  in their review of the applications of this architecture and its variants \citet{yu-2019} assert that ``almost all" important recent advancements in this domain have been facilitated by LSTMs. This is primarily due to the architecture's ability to accurately model both long and short-term dependencies in sequences; hence, such networks are largely used for the same sequence modelling problems as traditional RNNs. 

    Due to the superior accuracy provided by this architecture, one of the most common use cases of LSTMs for sequence learning is NLP. For example, in their exhaustive study of language modelling, \citet{jozefowicz-2016} found an LSTM-based network to provide the most competitive results in this field. Their work evaluated several ML models such as \emph{convolutional neural networks} and RNNs, finding that a large scale LSTM language model produced the best results, significantly improving state-of-the-art \emph{perplexity} (a commonly used NLP metric of prediction error) from $51.3$ to $30.0$ on the commonly used \emph{One Billion Word Benchmark} dataset \citep{chelba-2013}. Recently, LSTM models have been applied to even more complex tasks within the NLP space due to the ability to capture high-fidelity dependencies. This includes the work of \citet{saleh-2021} who used an LSTM-based DNN to detect hate speech in online content, producing an impressive \emph{F1-score} (the weighted average of a model's \emph{precision} and \emph{recall}) of $93\%$ on a composite dataset from multiple online hate speech repositories.

    Like other RNNs, LSTMs have also been widely used for time series forecasting. Due to the advantages provided by these networks' ability to model long-term dependencies, the benefits of LSTMs have been explored across an extensive spectrum of different time series applications. For example, \citet{shi-2022} recently compared the performance of a number of networks (including both traditional RNNs and LSTMs) for predicting Bejing air quality. They concluded that LSTMs generated more accurate predictions than simpler RNNs, and demonstrated their improved long-term memory by showing LSTMs outperform other networks even when the context window available to the model is small. Another popular area of time series forecasting in which LSTM networks are increasingly being used is in the prediction of financial markets, as it has been shown by studies such as \citet{li-2017} that LSTMs can accurately capture the complex dependencies within highly variable financial variables. The success of LSTMs within this domain has been widely shown across a variety of financial modelling tasks, such as forecasting commodity prices \citep{ly-2021}, stock market indices like the S\&P 500 \citep{fjellstrom-2022}, currency pairs on the Foreign Exchange \citep{qi-2021}, and the fluctuations of financial market risk \citep{du-2019}.


    \section{The Efficiency of Recurrent Networks}

    The use of RNNs and LSTMs has revolutionalised sequence learning, permitting models with cutting-edge accuracy across a spectrum of applications, including financial risk forecasting \citep{du-2019}. However, further research into recurrent architectures has shown that the sequential processing methods vital to their performance also make these models highly inefficient.


    \subsection{Memory and Energy Efficiency}

    The tradeoff between accuracy and efficiency within RNNs has been explored by several recent research papers aiming to reduce the complexity and energy consumption of these architectures. In their study of compression techniques for LSTM models, \citet{wang-2018} identify this conflict as a core limiting factor of RNNs restricting their use in resource-constrained domains. This is similarly identified by \citet{zarzycki-2021} in their exploration of LSTM-based predictors for chemical reactors; they concluded that whilst the number of parameters utilised by an LSTM was directly proportional to its modelling performance, higher complexity models inflicted a significant computational cost. 

    Numerous sources, such as \citet{cao-2017}, \citet{feliz-2021}, and \citet{zhang-2021}, have deduced that this inefficiency is directly caused by the sequential nature of RNNs. They assert that the many dependencies within LSTM cells---both between cells and to the input sequence---restrict the way in which these models can be trained and used. Specifically, cells in an LSTM network take input from both the input sequence and preceding cells; each cell in every network layer and on every sequence timestep receives both an input vector and two memory states. \citet{cao-2017} highlight that this introduces both temporal dependencies (between cell states at each timestep) and layerwise dependencies (between the output and input of each network layer). Both \citet{cao-2017} and \citet{feliz-2021} identify that these dependencies mean that LSTM training and inference cannot effectively exploit parallelisation, as the sequential dependencies mean many operations have to be computed serially. Thus, these networks cannot fully take advantage of the computational speedups provided by specialised hardware such as \emph{Graphics Processing Units} (GPUs) and \emph{Tensor Processing Units} (TPU) that have been effectively used to parallelise other DNNs, making training long and energy-intensive \citep{zhang-2021b}.

    These explorations have also quantified the large memory requirements of complex recurrent networks. \citet{feliz-2021} concluded that accessing memory to fetch the parameters of an RNN is the main source of energy consumption during training. Similarly, \citet{cao-2017} found that larger models with more layers take more time (and energy) to load their parameters on each computational pass through the network. Because of this, the memory bandwidth (the speed at which data is sent to the processor) required for evaluating operations becomes a bottleneck to compute speed. Furthermore, \citet{zhang-2021} identify that the large intermediate values used between computations inside each LSTM cell (typically stored in a high-bit representation to maintain accuracy) require large amounts of memory to store, and expend significant energy to compute with.


    \subsection{Efficient Adaptations to Recurrent Networks}

    Several researchers have proposed methods for improving the efficiency of RNNs, typically through reducing memory requirements by either minimising the number of model parameters, compressing the network after training, or reducing the bit-size of variables. A notable exception to this trend is the language model of \citet{li-2016}, who suggested that training speed can be increased by reducing the number of possible network inputs. \citet{li-2016} showed that reducing the vocabulary of their RNN-based model decreased the required network size by a factor of $40--100$, decreasing training time by a factor of $2$. 

    Most commonly, training time, memory, and energy requirements can be reduced by shrinking an RNN's parameter set. The LSTM-based implementations of \citet{wang-2018} and \citet{chen-2022} achieve this through compressing the network; \citet{wang-2018} propose a structured compression technique that shrinks the necessary size of weight matrices, reducing the computational complexity of evaluating passes through the LSTM and decreasing the number of memory accesses required. This approach improved the model's energy efficiency by a factor of $33.5$ (compared to a state-of-the-art LSTM) whilst only inflicting a small accuracy drop. Similarly, \citet{chen-2022} implement a compact LSTM model, designed for the low resource requirements of medical sensors; they reduced the number of bits required to store variables, minimising memory constraints and simplifying expensive multiplications. Their network reduced power consumption by $56\%$ and the required circuit area of the specialised chip used by $54\%$ (compared to a typical 16-bit implementation).

    \citet{feliz-2021} extend the work of \citet{chen-2022} in reducing the complexity of calculations by lowering the precision of bit-representations. This approach utilised an additional policy network to identify instances within each network pass where the output of individual neurons doesn't significantly change between consecutive evaluations. If the policy network predicted only a minute change, the current output is cached and reused as the output of the next calculation, saving the need to evaluate the next computation. \citet{feliz-2021} found that this technique avoided over $24\%$ of computations, produced an average reduction in energy consumption of $18.5\%$, and reduced training time by a factor of $1.35$. Additionally, the policy network was used to dynamically select the bit-size of variables to maintain a compromise between accuracy and efficiency. This adaptation resulted in $57\%$ of computations being evaluated through a smaller representation, providing a further $19\%$ average energy saving and $1.46$ times training speedup.

    These results demonstrate that whilst RNNs are expensive in terms of memory and energy, several research directions have been proposed to mitigate this. The conclusions drawn show promise for improving the efficiency of RNNs and LSTMs, however, this research largely focuses on improving efficiency for specialised applications and hardware. Hence, further work in this field is required to demonstrate the general applicability of these adaptations to improving the efficiency of RNNs, especially within high-performance applications such as those in finance.


    \section{Financial Risk Modelling}

    Predicting stock price movements is a common example of sequence-to-sequence modelling that has received great attention within the field of deep learning for finance, with numerous reviews extensively exploring the area (such as \citet{sezer-2019} and \citet{jiang-2021}) and innovative approaches continually pushing the boundaries of prediction accuracy (e.g. the recent combination of deep learning and sentiment analysis by \citet{darapaneni-2022} for predicting stock price movements). Despite this plethora of research, it is still a challenging task to accurately model price movements in real financial markets due to the complexity of financial systems, non-linear relationships between financial variables, and high-frequency variations in values \citep{timmermann-2004}.


    \subsection{Forecasting Financial Risk}

    The challenges associated with market price prediction mean that within the finance industry a more helpful use case of sequence-to-sequence modelling is forecasting financial risk. In their white paper explaining the applications and benefits of ML for financial modelling, \citet{laplante-2019} exemplify this sentiment, asserting that ANNs provide the most utility for highlighting potential risks, not explicitly modelling variable movements. In fact, a recent survey of financial professionals by \citet{chartis-2019} found that $70\%$ of respondents use ML tools in the financial risk sector, majoritively for analysing ``market risk for the trading book" ($51\%$ of respondents) and ``market risk for the banking book" ($44\%$). This trend is echoed by \citet{peng-2021} in their review of Fintech, DL, and financial risk; they assert that modelling risk is currently in high demand due to the large fluctuations, uncertainty, and high volatility exhibited in financial markets, making risk an important consideration when making financial decisions. Furthermore, \citet{mashrur-2020} highlight that financial market risk typically exhibits identifiable trends as risk patterns reoccur in cycles, facilitating accurate modelling based upon historical data.

    For these reasons, modelling financial market risk is a popular application of DL. This typically involves the development of statistical models to forecast trends within different risk measures such as \emph{value-at-risk} (VaR) and \emph{volatility} \citep{peng-2021}. VaR estimates the value an asset is likely to lose over a given time period, to a specified confidence level, returning a value that indicates the maximum expected loss over that period at that confidence level. This metric has long been considered a valuable method of measuring market risk, with a vast spectrum of research covering its computation and use \citep{khindanova-2000}. Furthermore, studies such as that of \citet{sun-2009} have shown that ANNs provide superior forecasting of VaR than other models; this has established VaR popular choice for recent DL models aiming to forecast market risk using DNNs. For example, \citet{du-2019} proposed an RNN-based model for estimating VaR, showing that this approach provides a flexible architecture and improved prediction accuracy.

    Financial market volatility is also an important risk measure that is often forecast within academic research and the finance industry. Volatility can be defined as the scale of fluctuations in the pricing of an asset within a financial market over time and is intrinsically related to the risk associated with that asset \citep{cavalcante-2016}. \citet{cavalcante-2016} assert that this is a vital measure to financial analysis, as it is a key indicator of the state of many economic and external factors that affect financial markets (such as investor sentiment and political conditions). This is because changes to these factors directly affect the volatility of a market; for instance, in times of economic crisis or political instability, volatility tends to rise, meaning large price variations of assets are likely, and the market is considered high risk \citep{sezer-2019}. Because of this relationship between volatility and risk, \citet{tino-2001} highlight that volatility changes are typically used as buy and sell signals to investors, and \citet{ge-2022} assert market volatility dictates many decisions of market players; hence, forecasting market volatility is a popular modelling domain. In fact, whilst analysing models for the financial derivatives market, \citet{ozbayoglu-2020} found that most research into ML and DL-based modelling in this field either focuses on pricing or volatility estimation. This popularity is further demonstrated through the plethora of academic reviews specifically surveying this field, such as those by \citet{poon-2003} and \citet{ge-2022}, and the spectrum of recently proposed DNN-based forecasting models.

    \subsection{Measures of Volatility}

    The review of \citet{ge-2022} and recent exploration of volatility modelling by \citet{tino-2001} both provide comprehensive overviews of the most commonly used measures to quantify volatility measures: \emph{historical volatility}, \emph{realised volatility}, and \emph{implied volatility}. Due to their relative simplicity, these three metrics have been extensively explored in literature and their ability to be forecast demonstrated. Each of these measures calculates a slightly different version of volatility. Implied volatility (IV) is computed given a specific option price, computing the expected volatility associated with that price; whereas, realised volatility (RV) is a measure computed directly from the price movements of a market and hence is the volatility actually realised in the underlying financial market. Historical volatility (HV) is a type of realised volatility, which computes the volatility over a preceding time period based upon market closing prices; it is the opposite of \emph{future realised volatility}, which is the forecast value realised volatility is expected to take in the future \citep{busch-2011}.

    Due to the differences in what these measures represent, each is computed in a slightly different way. Since IV represents an expectation of volatility, it cannot be computed directly from financial market data; instead, IV is estimated through an option pricing model such as the \emph{Black-Scholes model} \citep{black-1973}. Options models are typically used to estimate the price of an option (a financial contract between buyers and sellers) under the current market conditions \citep{wu-2015}. When inverted, these models can additionally be used to compute IV by inputting a given option price \citep{tino-2001}. On the contrary, RV is computed directly from market price movements over a time window $\tau_1 \to \tau_2$ \citep{ge-2022}. This measure is typically calculated as variation in the logarithmic returns $r_t$ of an asset (Equation \ref{eq: return}) over the specified time period. Most commonly, this time window covers returns observed in the past, which is the HV of that period. When surveying various volatility measures, \citet{ge-2022} found HV to be the most popular, primarily because it provides an intuitive metric that is simple to compute. The most common method of computing HV is to approximate the volatility $v_t$ over the $N$ values over the interval $t \colon \tau_1 \to \tau_2$ as the standard deviation of logarithmic returns between these times; this is shown in Equation \ref{eq: hv1}, where $\mu(\tau_1, \tau_2)$ is the mean return over the time interval \citep{ge-2022}. This method of computing HV is used across a wide range of applications within volatility modelling, such as the ANN-based model of \citet{lahmiri-2017} that forecasts the volatilities of the exchange rates between currency pairs.

    \begin{equation}
        \label{eq: return}
        r_t = \log\Big(\frac{c_t}{c_{t-1}}\Big)
    \end{equation}

    \begin{equation}
        \label{eq: hv1}
        v_t = \sqrt{ \frac{1}{N} \sum_{t' = \tau_1}^{\tau_2} \big( r_{t'} - \mu(\tau_1, \tau_2) \big)^2 }
    \end{equation}

    Whilst this is the most commonly referenced method of computing HV, a spectrum of different formulae have been proposed. For example, \citet{tino-2001} computed HV as an exponentially weighted average (by weighting factor $\alpha \in [0, 1]$) of the square of logarithmic returns over the time window (Equation \ref{eq: hv2}). 

    \begin{equation}
        \label{eq: hv2}
        v_t = (1 - \alpha) \sum_{t' = \tau_1}^{\tau_2} \alpha^{\tau_2 - t'} r_{t'}^2
    \end{equation}

    In their review of 35 volatility forecasting ANNs, \citet{ge-2022} found HV was the most popular metric, constituting $71\%$ of research papers; this was followed by other RV measures ($17\%$), and finally IV ($9\%$). Additionally, \citet{ge-2022} explored the most common domains for forecasting volatility, finding the S\&P 500 market to be the most popular (accounting for 12 out of the 35 research papers), primarily due to its accessibility; the price of oil (7 papers) and metal (6 papers) were also found to be common modelling applications. However, a vast spectrum of models have been developed outside of these domains; for instance, \citet{ge-2022} also explored the use of IV, RV, and HV within the context of stocks, bonds, and other financial indices. Hence, the selection of a volatility measure is typically a domain-specific choice, based upon the different benefits and drawbacks associated with each metric. For example, IV has been seeing increased adoption from institutional investors, hedge funds, and banks \citep{neftci-2008}, and is used to define major volatility indices such as the \emph{Chicago Board Options Exchange's Volatility Index} (VIX). However, due to its reliance upon an options pricing model, IV requires options data, and hence cannot be used in other domains; furthermore, it exhibits the \emph{volatility smile}---where near-identical option prices can result in highly different volatility levels---and tends to output higher volatility than other measures. There are also challenges associated with RV and HV; whilst RV has been shown to tend towards an estimate indistinguishable from the latent volatility \citep{andersen-2001}, this accuracy requires data with a high sampling frequency. Additionally, several issues unanimously encompass all volatility estimates, including price irregularities at the tail-ends of return distributions (discussed in detail by \citet{ozbayoglu-2020}) and the complex dependencies and transient relationships between financial variables \citep{timmermann-2004}.


    \subsection{Forecasting Financial Volatility}

    Due to the utility of modelling financial risk, forecasting market volatility has become a popular domain within both financial computing research (such as within the survey of \citet{ozbayoglu-2020}) and the finance industry (shown through the statistics gathered by \citet{chartis-2019}). Furthermore, \citet{ge-2022} demonstrates this is a global trend, discovering that the 35 research papers they surveyed were derived from 11 different countries (including the US, UK, Germany, and China).

    Traditionally, financial institutions and researchers have exploited \emph{generalised autoregressive conditional heteroscedasticity} (GARCH) models to forecast volatility. GARCH models have been shown to be accurate and reliable, with research such as that of \citet{lahmiri-2017} demonstrating the ability of these systems to capture the characteristics of financial time series. These models calculate the volatility of a market from a sequence of logarithmic returns $r_0, r_1, \dots, r_t$, approximating the volatility $v_t$ at time $t$ as the conditional variance $\sigma_{t}^2$ of returns over a specified time period. Namely, GARCH uses the time series $r_0, r_1, \dots, r_t$ to predict the volatility $v_t$, building up a volatility time series of its own from the predicted $c_t$ values: i.e. $v_0, v_1, \dots, v_t$. Early work in this field by \citet{akgiray-1989} showed that GARCH models managed to fit training data (a time series of stock returns) and forecast volatility at future timesteps more accurately than similar models in its class. Similar studies, such as that of \citet{hansen-2005}, have also demonstrated the impressive forecasting accuracy of GARCH models within the foreign exchange market. Additionally, GARCH models have been shown to accurately determine clusters of high and low volatility, which gives a good insight into the short-term risk exhibited in a market as high volatility is typically followed by high volatility, and vice versa \citep{arum-2019}. Largely because of this accuracy, GARCH has become one of the most frequently utilised methods of estimating the volatility of financial time series \citep{cheng-2003}. Even despite modern methods, this model is still a popular choice, often being used as a baseline to judge the performance of newly proposed volatility forecasting models; for example, \citet{rodikov-2022} used GARCH to contextualise the results of their DNN-based system. The calculations inherent to GARCH have also recently been used in several hybrid models to complement the computation of volatility within a larger system. In fact, the survey of volatility forecasting models by \citet{ge-2022} found that of the 35 papers reviewed, 14 focussed upon hybrid models; of these, they found GARCH to be the biggest contributor. \citet{tino-2001} similarly discuss the prevalence of hybrid models, asserting that they combine the predictive performance of complex models with the robustness of simple methods (such as GARCH) for dealing with non-stationary data.


    \subsection{Deep Learning for Volatility Forecasting}

    Whist traditional forecasting methods have been shown to produce accurate estimations of the risk of financial markets, a significant amount of recent work has focussed on applying DL-based modelling to this domain; for example, \citet{chartis-2019} found that $70\%$ of financial institutions use ML for risk analysis and forecasting, most commonly for ``market risk". This has become a popular domain within computational finance research. Such research has included the DNN of \citet{kim-2020} for forecasting the profitability and operational risk of the trading behaviour of retail investors in the stock market, and \citet{groth-2011} who analysed unstructured text data to identify news relating to corporate disclosures that are likely to cause abnormal volatility levels in the stock market. Forecasting the VaR of assets is also a popular application of DL within financial risk, such as the RNN-based approach of \citet{arimond-2020}. In fact, in their analysis of the intra-day risk of the German stock market, \citet{sun-2009} proved that ANNs provide superior forecasting accuracy of VaR compared to other traditional methods. This improved accuracy is one of the core reasons why DL is being increasingly adopted within financial risk, as \citet{chartis-2019} found that $44\%$ of financial firms cite ``greater accuracy of process and analysis" as the motivation behind using ML methods.

    Volatility forecasting is also becoming a popular application of DL; through analysing the publication rate of research papers in 2018, \citet{sezer-2019} found that volatility forecasting was in the top five uses of DL within financial research. Additionally, in their review of ML-based volatility models, \citet{ge-2022} asserted that almost all recent work on forecasting financial volatility relied on ML and DL methods, largely due to their improved accuracy and ability to fit complex time series. DNNs are used to forecast volatility through sequence learning; in this application, time series of logarithmic returns $r_0, r_1, \dots, r_t$ and of previously calculated volatilities $v_0, v_1, \dots, v_t$ are used to predict the following volatility sequence starting at timestep $t+1$ (predicting $v_{t+1}$) and continuing to forecast a specified number of steps into the future (up to $t+n$) through sequence-to-sequence prediction. Namely, the volatility model $M_{\theta}$ is developed that computes the output sequence $y = v_{t+1}, v_{t+2}, \dots, v_{t+n}$ based upon the context of the past sequences $r_0, r_1, \dots, r_t$ and $v_0, v_1, \dots, v_t$, approximating the mapping $y = f( r_0, r_1, \dots, r_t, v_0, v_1, \dots, v_t \vert \theta )$ between past and future time series.

    
    
    This performance improvement has been exemplified through several extended studies; for example, \citet{zhang-2022} found that ANNs can model the strong and stable commonality in intra-day RV to produce better forecasting performance that takes advantage of the shared features between financial variables. These results demonstrated how DNNs are superior at handling complex interactions and dependencies between financial variables, as their high dimensionality allows them to act as better function approximators. Several studies have compared the performance of DL models to traditional methods. Through an evaluation of volatility forecasting models over 23 different stocks, \citet{rahimikia-2020} found that DL gives stronger forecasting power than autoregressive methods such as GARCH. \citet{rodikov-2022} drew a similar conclusion, finding that an LSTM-based model could generate a higher test accuracy than other well-known models. They also found that RNN architectures were effective at estimating financial market variables like volatility as they do not need to know the parameters of the underlying distribution of the variable being forecast. This is different from models such as GARCH, which uses \emph{maximum likelihood estimation} to approximate the parameters of the return and variance functions of the market. Furthermore, \citet{tino-2001} assert that GARCH has been shown not to be able to facilitate notable profits for investors, whereas RNN-based models show promise in facilitating a statistically significant profit for market players using them within their trading strategy. 

    Whilst some researchers opt for alternative architectures (such as the CNN-based model of \citet{chen-2018}), the improved accuracy and potential profit offered by RNNs and LSTMs have established them as a core focus of DL models for volatility forecasting. In their survey of 35 research papers, \citet{ge-2022} found that 9 out of the 21 pure models utilised an RNN, as these networks are a natural fit for time series data. This research typically explores the use of RNNs for forecasting realised volatility within the S\&P 500 market. For instance, \citet{bucci-2020} showed that RNNs outperform all other traditional methods for forecasting RV over S\&P 500 data. Their experimentation demonstrated the ability of an LSTM to capture long-term dependencies within the financial time series, which made this architecture more accurate at forecasting during highly volatile periods. Additionally, \citet{xiong-2016} incorporated Google domestic trend data into their RNN-based model for the S\&P 500, finding that RNNs are more robust against noise in the time series data. Hybrid models that combine RNNs within traditional methods have also been explored in literature, with \citet{ge-2022} finding that 14 of the 35 surveyed papers implemented a composite system. Most commonly, this approach uses a GARCH model to aid the DNN's computation of volatility; for example, \citet{kim-2018} combined an LSTM with GARCH to generate highly accurate predictions of RV over the South Korean stock market.

    Hence, it is clear that the use of DL (in particular RNNs and LSTMs) is an expanding field within financial volatility forecasting. This is primarily due to the increased forecasting accuracy provided by DNNs, as these models have been shown to produce class-leading performance across markets (such as the S\&P 500) and volatility metrics (most notably RV). 


    \section{Green AI}

    Complex DL-based systems are seeing increased adoption across a variety of modelling domains in finance and further afield, primarily due to their superior accuracy. However, many of these implementations ignore the efficiency concerns associated with DL, such as the memory and energy inefficiency of recurrent networks. The ignorant use of these resource-hungry models (dubbed Red AI by \citet{schwartz-2019}) inflict large ESG costs, as their extreme computational load generates thousands of pounds of carbon emissions over training \citep{strubell-2019}, and limits who can research and employ within high-performance ML systems \citep{bender-2021}. These environmental and social impacts have spurred recent attention around Green AI, where DL energy and data-efficient models are developed, motivated by reducing resource requirements, mitigating environmental cost, and benefiting the inclusivity of ML \citep{schwartz-2019}.


    \subsection{Quantifying the Energy Efficiency of Deep Learning}

    As a simple initial approach to mitigating the ESG cost of DL, many proponents of Green AI suggest that all new research papers on cutting-edge DL models should report the training time and resource requirements of their DNNs. \citet{strubell-2019} assert that this should allow future work looking to utilise existing methods to conduct a cost-benefit analysis based upon the resources required to implement such a model. In their analysis of 60 DL research papers, \citet{schwartz-2019} found that $90\%$ of papers in the ACL Anthology, $80\%$ of those in the NeurIPS conference, and $75\%$ in the CVPR conference cited accuracy improvements as the main contribution of their work, with only $10\%$ of ACL papers and $20\%$ of CVPR papers contributing a new efficiency result. \citet{schwartz-2019} argue that this demonstrates the lack of reporting surrounding the energy and data efficiency of DL models, highlighting that describing performance contextually with respect to training budgets improves both the sustainability and inclusivity of this work (allowing future work to be compared despite fewer training resources).

    Despite the consensus on the importance of reporting the resource requirements of these systems, a ubiquitous hardware-independent measure of the computational cost of a DL model has yet to be firmly agreed upon. \citet{schwartz-2019} describe how the expense $Cost(R)$ of processing the result $R$ using a DNN is proportional to the cost of processing a single instance $I$, the size $D$ of the training dataset, and the number of hyperparameters $H$ that require tuning (Equation \ref{eq: cost-r}). They survey several proposed metrics for quantifying this cost, including runtime, parameter count, electricity usage, and carbon emissions; however, they highlight that each of these poses its own challenges, such as the hardware-dependent nature of electricity usage and difficulty in measuring the exact carbon emissions produced by a program. \citet{schwartz-2019} conclude that reporting the total number of \emph{floating-point operations} (FLOPs) to generate the result is the most reliable way to quantify the computational cost, and hence energy efficiency, of a system. This count provides several advantages over alternative metrics: it accurately reports the computational work done by a system, is correlated to runtime (through considering work per time step), and has been successfully used to quantify the energy footprint of ANNs \citep{veniat-2018}.

    \begin{equation}
        \label{eq: cost-r}
        Cost(R) = I \cdot D \cdot H
    \end{equation}

    \citet{schwartz-2019} use the competing models ResNet (he-2015) and ResNeXt (xie-2017) to exemplify how reporting FLOPs can contextualise performance advancements with model efficiency: whilst ResNeXt provided a $0.5\%$ accuracy boost over the \emph{ImageNet} dataset (becoming 2017's leading \emph{ImageNet top-1 accuracy} score), it required $35\%$ more FLOPs. However, \citet{schwartz-2019} note that using FLOP count as an efficiency metric is not without its own limitations; most importantly, FLOP doesn't always exactly correlate with runtime or energy consumption, as it does not take into account auxiliary communication or memory costs. \citet{amodei-2018} echo these benefits of reporting FLOP; however, they highlight that computing the FLOP count of a program is not always trivial. They suggest that in the absence of being able to directly compute the FLOPs of a given DL model, its computational load can be approximated through its GPU utilisation over training; this calculation is demonstrated in Equation \ref{eq: cost-t}, where the computational cost of training $Cost(T)$ in \emph{petaFLOP/s-days} (pfs-days) is given by the product of the number of GPUs used $N_{gpu}$, the processing power per GPU in FLOP/s $P_{GPU}$, the training time in days $T_{days}$, and the estimated average utilisation of these GPUs $U$ (typically taken as $33\%$). To exemplify this approximation, \citet{amodei-2018} calculated that whilst 2012's leading image classifier AlexNet \citep{krizhevsky-2012} had an approximate computational cost of $0.0058$pfs-days, five years later the innovative DNN \emph{Xception} \citep{chollet-2017} had a cost of $5.0$pfs-days (an increase by a factor of $862$).

    \begin{equation}
        \label{eq: cost-t}
        Cost(T) = N_{gpu} \cdot P_{gpu} \cdot T_{days} \cdot U
    \end{equation}

    Further work has attempted to explicitly quantify the carbon emissions associated with ML systems, despite the challenges of this metric identified by \citet{schwartz-2019}, who state that emissions are highly dependent on local electricity infrastructures and hence are often hard to compare between models and regions. For example, \citet{lacoste-2019} propose a \emph{Machine Learning Emissions Calculator} that computes the $CO_2$-equivalents ($CO_2$-eq) of an ML system, measuring the environmental detriment caused by the energy expended training a given model. They assert that despite its complexity to compute, measuring $CO_2$-eq is highly beneficial as it gives a direct insight into the emissions we are trying to minimise.

    This work demonstrates that quantifying the computational, energy, and carbon cost of existing and newly proposed DNNs is an expanding field within DL research and Green AI, solidifying quantification methods as a vital first step to raising awareness about the ESG impacts of DL, and promoting the use of energy-efficient models.


    \subsection{Efficient Neural Network Architectures}

    Once the ESG impact of DNNs has been thoroughly quantified, this information must be taken into account by developers to ensure that new models are developed efficiently. As \citet{schwartz-2019} identify, the cost associated with generating a result from a DNN is proportional to that of processing a single instance $I$. Hence, much research into Green AI has focussed on how the architecture of DNNs can be redesigned to minimise computational complexity and the energy expended to process data instances. 


    \subsubsection{Shrinking Model Sizes}

    Smaller neural networks expend less energy to generate a result, as fewer layers mean fewer computations and memory accesses. For this reason, a significant proportion of Green AI research focuses on developing \emph{compact models}, either through reducing the number of layers within a given ANN or \emph{pruning} its parameter set. In an exploration of the carbon footprint of ML, \citet{lottick-2019} evaluated the energy budget and carbon emissions of increasingly large ANNs. Interestingly, their research showed that whilst increasing the number of layers within an MLP increases its energy consumption, it does not generate a consistent improvement in accuracy, and could even degrade performance. Hence, \citet{lottick-2019} assert that accurate models do not have to be complex, and compact DNNs offer significant promise in generating impressive performance whilst reducing the energy expenditure.

    Early work into ANNs and DNNs focussed on \emph{fully connected networks} (FCNs) that required incredibly large parameter sets and consumed extreme amounts of energy to process data. In an attempt to reduce the cost of such models as networks got deeper, \citet{han-2015} proposed \emph{network pruning} for generating efficient ANN architectures. This method took a pre-trained ANN and allocated an importance score to each channel weight, quantifying its significance to the network's performance. Redundant channels with a score below a given threshold were identified and removed from the ANN, producing a model that maintained accuracy but had a reduced computational and memory cost. \citet{han-2015} demonstrated their pruning technique on AlexNet, producing a $9.1$ times reduction in parameter count from $61$ to $6.7$ million without any accuracy reduction. This impressive result demonstrated the promise of compact models with lower resource requirements, inspiring a spectrum of new research into pruning. For example, \citet{iandola-2016} used a similar pruning technique to \citet{han-2015} to produce an efficient ANN SqueezeNet for deployment in low-memory embedded systems; their model surpassed AlexNet accuracy whilst reducing the parameter set size by a factor of $50$. This dramatic reduction further demonstrated the utility of parameter pruning to applications with limited resources. Additionally, parameter pruning has been applied to RNNs---such as the pruned RNN-based translation model of \citet{see-2016} and compact network of \citet{narang-2017}---producing effective models for sequence learning with lower computational costs.


    \subsubsection{Quantisation and Efficient Variable Representations}
    \label{section: quantisation}

    Due to the publicity surrounding the record-setting performance of complex DNNs, these models are alluring tools for industry players looking to upgrade their workflows—notably \citet{chartis-2019} found that $44\%$ of financial firms cited ``greater accuracy" as the motivation behind adopting ML. However, in many cases, the upper echelons of performance are neither necessary nor beneficial for industrial applications; \citet{kumar-2020} highlight that in resource-constrained applications such as on mobile devices, state-of-the-art DNNs are not feasible as they would quickly engulf the limited power and memory resources. For this reason, Green AI researchers have begun to explore how accuracy-efficiency tradeoffs can be made that produce significant decreases in the energetic cost of DNNs by inflicting slight accuracy drops.

    Much of this research, explored in surveys such as those of \citet{xu-2021} and \citet{cai-2022}, has focussed on decreasing computation and memory costs by using lower precision representations of variables and parameters. Typically this is implemented through \emph{quantisation}, where the value of each variable is mapped to discrete quantisation levels that require fewer bits to store. \citet{xu-2021} divide these quantisation methods into two classes: \emph{deterministic quantisation} and \emph{stochastic quantisation}. The former computes an explicit, pre-determined mapping between original values and their low-bit quantised counterparts, whilst the latter selects the quantised value probabilistically; for example, \emph{random rounding} samples low-bit representations from a discrete distribution where each possible quantisation level has a given probability. Deterministic approaches range from \emph{uniform quantisation}, where floating-point numbers are mapped to their closest low-bit fixed-point representations, to \emph{clustering quantisation}, where parameters are clustered by value and replaced by the mean of their cluster \citep{xu-2021}. \citet{kumar-2020} demonstrate the performance of rounding quantisation by comparing the DNN-based approaches of \citet{courbariaux-2015} and \citet{judd-2016}. Namely, \emph{BinaryConnect} \citep{courbariaux-2015} implements an extreme quantisation approach, using only single-bit representations, but inflicts an accuracy drop of $19\%$; whereas, \emph{Stripes} \citep{judd-2016} reduce variable representations from 32 to 8-bits whilst maintaining an accuracy drop of only $1\%$. Hence, quantisation is an incredibly adaptable technique and can be widely used to decrease the computational cost of models contextually based on their desired accuracy-efficiency tradeoffs.

    These methods implement post-training quantisation, which improves the efficiency of models after they have been developed. Further research has explored quantisation-aware training, which aims to reduce the performance drop inflicted by quantisation by applying it at each training step. For example, \citet{fan-2020b} quantised parameters during training to produce a model that achieved an ImageNet top-1 accuracy score of $80.0\%$ (equivalent to 2017's highest accuracy model ResNeXt) using only $3.3$MB of memory (only $3\%$ of the memory required by ResNeXt). Furthermore, \citet{cai-2022} assert that this training process can be adapted even further to conduct \emph{low-bit training}, where parameters, activation values, and error gradients are all quantised. \emph{DoReFa-Net} \citep{zhou-2016} implemented such low-bit representations, using 1-bit parameters, 2-bit activations, and 6-bit gradients to boost training speed whilst generating an accuracy comparable to AlexNet using 32-bit representations. This demonstrates that quantisation and low-bit representations can be effectively used to produce models with significantly lower memory constraints and energy consumption. Research has also been conducted into quantisation for RNNs and LSTMs: \citet{hubara-2016} explored the effectiveness of quantising weights and activations within recurrent networks, whilst \citet{he-2016} proposed quantising LSTM gates. However, whilst investigating quantisation for RNNs, \citet{ott-2017} found that low-bit training was not ubiquitously effective in this context; thus, they concluded \emph{mixed-precision training} should be used for RNNs, where weight matrices are quantised but activation values retain higher bit representations.


    \subsection{Efficient Model Training}

    Whilst the efficiency of a DNN's architecture is essential to minimising the energy expended by each pass through the network, it is also important to minimise the total number of passes required by training. Hence, a significant amount of research within Green AI focuses on energy-efficient training algorithms that attempt to reduce the number of iterations necessary to train an accurate model.


    \subsubsection{Transfer Learning}

    Possibly the most common method of reducing the length of a DNN training phase is simply utilising a pre-trained model developed for another analogous task. This method, known as \emph{transfer learning}, is discussed by \citet{strubell-2019}, \citet{walsh-2021}, and \citet{schwartz-2019} in their explorations of Green AI. Specifically, transfer learning takes an existing pre-trained \emph{base model}, and tunes its parameters over a new training dataset that covers a new domain. Since the base model has already learned a general ability to complete a similar task, the new target model requires only a short, computationally-inexpensive retraining process, where the model is fine-tuned to accurately capture information from the new domain. Both \citet{strubell-2019} and \citet{walsh-2021} identify that transfer learning significantly reduces the resources required to train a DNN and allows their application within fields with only limited amounts of data. For example, \citet{wang-2020} used transfer learning to apply ResNet to the \emph{Stanford Dogs 120} dataset, a small collection of only 20,580 images, making it only $1.7\%$ of the size of the original ImageNet dataset (of 1.2 million images) used to train ResNet. The use of transfer learning allowed \citeauthor{wang-2020} to develop a model that had an accuracy $11.07\%$ greater than any other DNN developed for this data and reduced the required training iterations by a factor of $10$. This demonstrates that transfer learning is a simple but effective method for reducing the training cost of DNNs, which allows models to be developed for niche domains and minimises the energy required to train accurate ML models. Furthermore, the cost of training the original base model is effectively spread over all target models that utilise it, sharing out its computational cost. For this reason, \citet{schwartz-2019} advocate for more pre-trained models to be released publicly in an attempt to promote transfer learning and minimise the ESG impact of state-of-the-art DL models.


    \subsubsection{Initialisation}

    Instead of directly copying a base model, its parameter values can be used as a starting point from which training a new DNN can begin. This is an example of \emph{initialisation}, which is the process through which the initial values in a network's parameter set are chosen. This is an important part of training, as \citet{xu-2021} found that the rate of convergence of a network's parameters (towards the minimum of the loss function) heavily depends upon their initial values. Furthermore, \citet{hanin-2018} found that primitive techniques for initialising DNNs such as random initiation from a uniform or Gaussian distribution can often lead to slow training, or even result in parameter values never converging.

    Both \citet{hanin-2018} and \citet{xu-2021} evaluate alternative methods by which a network's parameters can be initialised. As described, this could be from an existing model trained over a base task, which is believed to improve generalisability and reduce training time. There are a number of varying ways in which this has been implemented; \emph{feature-based initialisation} assigns borrowed parameter values to a subset of the new DNN's parameters and keeps these fixed during training (improving generalisability), whereas \emph{fine-tuning-based initialisation} trains all new and borrowed parameters, providing greater specialisation. Alternatively, \emph{supervised initialisation} (commonly used for CNN architectures and NLP models) pre-trains the new DNN over similar datasets or for analogous tasks, then reuses these low-level representations as a starting point for training high-level features in the target task over the target dataset. For example, lin-2021 initially pre-trained their DNN-based language model as a general multilingual translator, before specifically applying the network to language pairs to translate between. \emph{Self-supervised initialisation} takes this concept further, allowing pre-training to be conducted without supervised data by learning general representations over unlabelled data (e.g. \emph{ELMo} by \citet{peters-2018}).

    Hence, this established research demonstrates the importance of selecting a suitable initialisation method when developing a new DNN based upon the chosen application and architecture, as the starting point of training can drastically reduce the time and energy expended to generate an accurate model.


    \subsubsection{Progressive Training}
    \label{section: progressive-training}

    \emph{Progressive training} \citep{xu-2021}, also known as \emph{greedy layer-wise pre-training} \citep{xu-2018}, builds upon the aforementioned concept of building up a DNN through a combination of pre-training and tuning. This training procedure constructs a model sequentially layer-by-layer by iteratively adding a new layer and tuning its parameters. This builds up the DNN in a bottom-up approach, whereby the network first trains lower layers to accurately represent low-level features of the input, then trains higher layers based upon the learned parameters of the preceding layers (keeping the parameters of the previously added layers fixed). After the desired number of layers has been added, all parameters are unfixed and their values tuned in a concluding training phase. In theory, this reduces the computational cost of training a DNN, as training shallow, single-layer networks is significantly simpler and less resource intensive than training full, deep networks \citep{xu-2021}. Furthermore, \citet{xu-2021} assert that the parameters of later layers are optimised faster, as values are updated based on information already known about the features of the input (learned while training previous layers). This approach has been successfully used by implementations such as \citet{yang-2020} to radically speed up training. Specifically, \citet{yang-2020} reduced the total training time required by the language model BERT by over $110\%$ (from $85$ to $40$ hours) without degrading accuracy.

    Progressive training is typically implemented through one of two methods: \emph{supervised layer-wise pre-training} or \emph{unsupervised layer-wise pre-training}. Supervised approaches, such as that of \citet{ienco-2019}, train individual layers in a supervised manner, greedily optimising the performance of each. For instance, if building a forecasting model (such as the LSTM-based model of \citet{xu-2018}), each layer is independently trained to predict the future elements of a sequence. Starting by training a shallow network consisting of only an input layer, one hidden layer, and an output layer, the model iteratively gets deeper; this is achieved by first removing (and saving) the output layer, then fixing the values of all previously trained parameters, adding a new hidden layer, appending the saved output layer, and finally training the parameters of this new layer. Once a network of the desired depth has been constructed, the parameters of all layers are unfixed, and a short training round is conducted over the full network, where the parameters found during pre-training are used as a starting point for deducing the optimal global parameter set. 

    Unsupervised approaches, such as the implementations of \citet{xu-2018} and \citet{sagheer-2019}, train each additional layer as an \emph{auto-encoder}. Autoencoders are a type of ANN that learn to output a condensed or encoded representation of its input; they are trained in an unsupervised manner by simply minimising the difference between the input vector and the network's output vector \citep{pinaya-2019}. In unsupervised layer-wise pre-training, network layers are also built up iteratively. However, when it comes to training individual layers, each is trained as an unsupervised autoencoder, and hence learns to output a representation of the input vector. This unsupervised learning is repeated over the entire pre-training process, to result in an autoencoder network of the desired depth \citep{sagheer-2019}. During the subsequent training process of the full network, the model is then repurposed to the desired task (e.g. forecasting sequence elements). Namely, the autoencoder output layer used during pre-training is discarded, and a new output layer for the desired task is appended. With all parameters unfixed, the network then undergoes training, starting from the parameter set found during pre-training, and optimising the network's performance in the chosen domain \citep{sagheer-2019}.

    Research into layer-wise pre-training commonly centres around improving the efficiency of training recurrent networks, as RNNs have been identified as one of the most sensitive architectures to parameter initialisation. Namely, \citet{xu-2018} highlighted that poor training initialisation of RNNs can cause parameters to converge to sub-optimal values, and \citet{ienco-2019} found poorly initialised networks resulted in models with weak generalisability. Fortunately, both supervised and unsupervised layer-wise pre-training have been shown to combat these issues. \citet{ienco-2019} showcase how supervised layer-wise pre-training can be used to produce an RNN-based sequence classifier that matched and exceeded the accuracy of comparable models in a number of applications. Both \citet{xu-2018} and \citet{sagheer-2019} demonstrate that unsupervised layer-wise pre-training is effective at improving training efficiency while preserving accuracy. By comparing LSTM models initialised with layer-wise pre-training to a simple randomised initialisation, \citet{xu-2018} exemplified the performance and efficiency benefits of this technique across a number of domains, including image recognition and sequence-to-sequence learning. \citet{xu-2018} found that unsupervised layer-wise pre-training of LSTMs induced faster convergence toward the optimal parameter set, as their initialised values were located close to local minima (of the loss function). They also found that the resultant model exhibited a smaller total error during testing, suggesting this training method improves the generalisability of LSTMs. Similarly, \citet{sagheer-2019} compared unsupervised pre-training and random parameter initialisation, focussing on deep LSTMs for time series forecasting. They discovered that the layer-wise method produced better and faster convergence, especially when forecasting collections of correlated variables; this suggests unsupervised layer-wise pre-training is a good fit for forecasting sequences like financial time series that exhibit complex, non-linearly dependent variables.

    Hence, progressive training has been widely demonstrated to be effective in decreasing the time and energy required to train DNNs (and in particular RNNs), solidifying it as a useful method within Green AI. Furthermore, the ability of unsupervised pre-training to develop networks that capture the complex dependencies between variables indicates that this method has significant potential to improve the efficiency of the high-performance models used in applications such as financial modelling.


    \subsection{Data Efficiency}

    A common trend in training state-of-the-art models is the utilisation of vast datasets; \citet{bender-2021} summarise much of this work as following a philosophy of ``there's no data like more data". For example, the cutting-edge image classification model of \citet{mahajan-2018} produced record-breaking accuracy in its field, but achieved this through training over a dataset of $3.5$ billion images---three orders of magnitude larger than the training sets used by previous leading models (which typically utilised the \emph{Open Images Dataset} of $9$ million instances). Both \citet{schwartz-2019} and \citet{bender-2021} assert that these inflated datasets are a major problem with Red AI, highlighting that dataset size is a significant contributor to the computational and energetic cost of a model. Therefore, several researchers within the field of Green AI have attempted to reduce the data requirements of training DNNs, aiming to minimise memory requirements for low-resource applications (such as mobile devices), and reduce the time and energy expended training DNNs.


    \subsubsection{Inefficient Use of Data}

    Both \citet{bender-2021} and \citet{walsh-2021} highlight how a significant portion of the energy used to train DL models is due to the inefficient use of data. \citet{bender-2021} assert that much cutting-edge research opts to naively feed expansive collections of data into their model, most of which is not beneficial to learning. They suggest that more time should be put into carefully curating specialised datasets for each model and domain, minimising the time and energy wasted on unhelpful data instances and instead using data more intelligently. \citet{aljarrah-2015} further note that current ML systems are not intelligent enough to efficiently deal with significantly increased data loads. They highlight that commonly used optimisation methods (such as gradient descent) require substantial computational work to locate global optima; thus, when applied to large datasets, these methods accumulate an extreme computational cost.

    Furthermore, there can often be a conflict introduced between the size of architecture used and the amount of training data required, as small ANNs typically require more data to achieve comparable accuracy to deeper networks. Namely, \citet{bender-2021} found that models such as \emph{ALBERT} \citep{lan-2020} that attempt to recreate the performance of high-complexity networks (in this case BERT) using a smaller parameter set typically rely upon large amounts of data to force advances in accuracy during training beyond what would typically be possible with compact networks. This demonstrates that when considering the energy consumption of DNNs, a balance must be drawn between model size and dataset size, as both contribute to a model's computational cost (as shown in Equation \ref{eq: cost-r}).


    \subsubsection{Reducing Data Requirements}

    Several avenues have been proposed within Green AI for reducing the amount of training data needed by DNNs while preserving accurate performance. In their analysis of language models, \citet{bender-2021} highlight \emph{word embedding} as an effective method through which RNNs and LSTMs have been able to reduce their data requirements. Used in NLP tasks, a word embedding layer maps input words to a lower-dimensional representation known as an \emph{embedding vector}. These embedding vectors can represent closely related inputs similarly, allowing the input space to be constricted by grouping similar inputs \citep{bender-2021}. For example, when proposing their word embedding model \emph{ELMo} (a now commonly used NLP tool), \citet{peters-2018} demonstrated that a model utilising their embedding only required $10\%$ of the training data to produce the same accuracy as a chosen baseline model.

    Dataset reduction techniques have also taken afoot in other ML applications. \citet{xu-2021} discuss a variety of these approaches in their Green DL survey; these proposed methods typically rely on pre-trained models analogous to the approaches described for initialising network parameters. Namely, pre-trained models can exploit self-supervised learning to initialise near-optimal parameters without needing labelled data. This reduces data requirements as energy does not need to be expended labelling the data required for pre-training, and the initialised model converges faster downstream during the full training stage \citep{xu-2021}. \emph{Contrastive learning} \citep{chen-2020} is raised by \citet{xu-2021} as a commonly used pre-training method for reducing data requirements in CV. This approach focuses on learning pairwise relationships between data instances, representing similar pairs close together in the data space and pushing divergent pairs far apart, thus allowing the dataset to be constricted. \citet{xu-2021} further highlight the effectiveness of \emph{prompt learning} \citep{liu-2021}, where data instances are labelled with a discrete task-specific template known as a \emph{prompt}, which can improve the efficiency of data sampling as a single prompt can represent up to 100 individual data instances. 


    \subsubsection{Active Learning}
    \label{section: active-learning}

    Beyond pre-training techniques, \citet{xu-2021} note that possibly the most popular and widely explored method for reducing the data required to train DNNs is \emph{active learning}. This method, whose utility to DL has been extensively explored by surveys such as that of \citet{ren-2021}, aims to reduce training costs by selecting data instances from the full data space that are believed to provide the most utility to the model's learning process. 

    Active learning directly adapts the training process of a DNN, moving away from the traditional approach of iteratively training a network over the full dataset $D$ of $N$ instances. Instead, active learning approaches split the training data $D$ into smaller chunks, utilising it as either a \emph{stream} or a \emph{pool}. \emph{Stream-based active learning} individually picks data instances from $D$ and evaluates whether this instance is useful for training (in which case it is fed into the DNN) or not \citep{ren-2021}. More commonly, \emph{pool-based active learning} is implemented; contrary to the stream-based approach, at each training iteration, this method selects a pool of the $n_{sample}$ most useful instances from the full dataset, and trains the network over these \citep{ren-2021}. Specifically, pool-based approaches divide $D$ into the pool set $P$ and validation set $V$; at the start of training, the pool set $P^{(0)}$ is empty, and the validation set $V_0$ contains all elements of the full dataset $D$ (i.e. $d \in D \implies d \in V^{(0)}$). To begin training using pool-based active learning, an initial pool $P^{(0)}$ of $n_{sample}$ data instances (known as the seed) must be sampled from the full dataset; most simplistically, these are drawn at random. These sampled values are then moved from the initial validation set $V^{(0)}$ to the new pool; hence we have the new datasets $P^{(1)}$ (where $\vert P^{(1)} \vert = n_{sample}$) and $V^{(1)}$ (where $\vert V^{(1)} \vert = N - n_{sample}$). After the seed has been selected, the first round of training can be conducted, where the DNN is trained over pool $P^{(1)}$. This partly trained DNN is then used to predict the labels of the remaining $N - n_{sample}$ data points in the validation set; these labels are fed into an \emph{importance function} that gives a score to each instance in $V^{(1)}$ indicating its utility to the learning process of the DNN. The $n_{sample}$ instances with the highest importance score are then selected from the validation set and moved into the pool; hence, we have the new datasets $P^{(2)}$ and $V^{(2)}$ where $\vert P^{(2)} \vert = 2 n_{sample}$ and $\vert V^{(2)} \vert = N - 2 n_{sample}$. This process is repeated over multiple iterations of training, evaluating the importance function and expanding the pool set, until either a dataset size, iteration count, or accuracy threshold has been reached \citep{ren-2021}. 

    \citet{xu-2021} explain that active learning algorithms are typically categorised through the specific importance function they utilise to identify useful data instances. \emph{Uncertainty-based active learning} selects new instances to add to the pool based upon the uncertainty of the predicted labels assigned by the partly trained DNN. Alternatively, \emph{expected-model-change-based active learning} selects data instances from the validation set that are expected to cause the largest change to the parameters or output vector of the DNN. 

    Both \citet{ren-2021} and \citet{xu-2021} assert that this training algorithm drastically increases the data efficiency of DNN training. Through analysing a spectrum of applications, \citet{ren-2021} found that active learning can theoretically achieve an exponential improvement in the amount of time required to label training data (relative to dataset size), as well as being able to successfully deal with high-dimensional data (such as that used in CV) and train efficient but accurate models for sequential data (focussing on machine translation). \citet{xu-2021} similarly assert that active learning significantly reduces the time and energy wasted on redundant data instances that do not benefit the DNN's learning. Furthermore, \citet{ren-2021} note that the resources required to evaluate the importance function does not undermine the efficiency gains provided by the iterative learning process, as this function can be efficiently implemented to only inflict a negligible computational cost. 

    Hence, previous research has demonstrated the utility of active learning in reducing the data constraints of training DNNs, and thus this method is of vital importance to work within Green AI. Furthermore, whilst active learning over time series introduces further complexity due to the temporal nature of variables, approaches such as \citet{peng-2017} and \citet{zimmer-2018} have demonstrated its feasibility in this field, meaning there is significant potential for active learning to help improve the data and energy efficiency of high-performance sequential models such as those used within Fintech.


    % --------------------  EXPERIMENTS ----------------------
    \newpage
    \chapter{Methodology}
    \label{chapter: experiments}

    \section{Introduction to Experiments}

    This study investigates how energy-efficient DL methods proposed by researchers within the field of Green AI can be applied to the deep models commonly used in finance. Specifically, this research will focus on how DNN-based financial market volatility forecasting models can be trained in a manner that preserves accurate performance whilst reducing the energy and data constraints. Through implementing alternative training methods, adapted from Green AI models such as those of \citet{ott-2017} and \citet{xu-2018}, the following experiments attempt to demonstrate how the resource requirements of models typically used within Fintech can be reduced. The proposed LSTM-based modelling explores how both energy and data-efficient training extensions can lower the extreme computational cost of high-performance DNNs, limiting their energy expenditure and mitigating their associated carbon emissions and environmental impact. Hence, this experimentation aims to highlight the importance of considering the ESG impacts of DL, and demonstrate that high-performance DNNs can be utilised in Fintech without sacrificing the SDGs of sustainable finance. Furthermore, this study intends to illustrate how Green AI practices can be applied outside of the existing applications (namely NLP, CV, and mobile computing) that currently constitute the bulk of research within this field \citep{xu-2021}, expanding the scope and impact of sustainable DL.

    To illustrate the benefit of energy-efficient DL to improving the ESG impact of Fintech, two core research hypotheses are tested. Firstly, this study explores the question: \emph{can using Green AI methods reduce the amount of time necessary to train an accurate volatility forecasting model?} Experimentation into the hypothesis that Green AI methods can improve the efficiency of training these models aims to fill the research gap between the goals of sustainable finance (to reduce the carbon emissions of this industry) and the expanding use of DL in Fintech. Namely, by improving the energy efficiency of DNN training, this research demonstrates how the energetic cost (and thus carbon emissions) of DL can be mitigated. This improves the environmental impact of DNNs, and aligns it with the SDGs of sustainable finance, facilitating the use of DL within Fintech and sustainable finance without raising ESG concerns. Secondly, this research examines the further question: \emph{can Green AI methods reduce the data requirements of training an accurate volatility forecasting model?} This extended study explores the hypothesis that Green AI not only shows promise in directly reducing the energy consumed during model training but additionally provides a mechanism through which the use of broad, expansive datasets can be avoided. By minimising data requirements, the cost of DNNs can further be reduced, as less energy is expended labelling training data and learning over redundant instances that do not benefit performance \citep{schwartz-2019}. Furthermore, the use of smaller datasets improves the inclusivity of this field by lowering the bar-to-entry to engaging in DNN-based volatility forecasting, as researchers and market players do not need to invest in expansive computer memory or cloud storage to train accurate models \citep{strubell-2019}. Hence, the exploration of both hypotheses contributes to the field in several ways: expanding the applications of Green AI, reducing the environmental impact of Fintech, and improving the inclusivity of finance.

    To investigate these research hypotheses, the following experimentation is divided into three core studies. Initially, the baseline model at the heart of this research is presented in Section \ref{section: baseline}; this is an LSTM-based financial market volatility forecasting DNN. The underlying methods utilised to build this model, the motivations behind their use, and the relation of this model to typical DNNs implemented within computational finance are all explored. Following this, Section \ref{section: energy-extensions} explores the Green AI methods that have been utilised in an attempt to improve the energy efficiency of the baseline model training process. A comprehensive explanation behind these methods and the utility of their use is given, exploring their potential to reducing the computational cost of DNN training. Finally, Section \ref{section: data-extensions} explores the minimisation of data requirements through Green AI methods; an adapted training process is presented that aims to decrease the size model's training dataset, hence facilitating further computational cost, energy consumption, and carbon emission reductions.


    \section{Baseline Financial Volatility Model}
    \label{section: baseline}

    \subsection{Aims of Study}

    Before an analysis of the benefit of energy and data-efficient training adaptations can be made, a baseline model that is representative of the chosen domain must be implemented. As the aim of this thesis is to demonstrate the utility of Green AI to Fintech and the finance industry in general, the baseline model developed must be a characteristic example of DNNs used in finance. This should ensure that the findings of this research are as universal as possible, demonstrating on a wide scale how the ESG impacts of DL for finance can be mitigated. With this in mind, a common application of DL within finance must be chosen, and a DNN-based model used that has an architecture most typical of existing approaches developed for this domain. This should result in a system characteristic of typical approaches used by researchers and investors implementing DNNs for financial applications. The resource requirements of this model can then be analysed to demonstrate the typical computational, energetic, and carbon cost of DL models for finance. This research should give a specific example of the environmental impact of DL highlighted by \citet{schwartz-2019} and \citet{strubell-2019}, and reinforce the concerning energy consumption and carbon emission figures presented by \citet{hockstad-2018} and \citet{masanet-2020}. It should also reinforce the findings \citet{greenpeace-2021} and \citet{power-2020} surrounding the unsustainability of the finance industry. Furthermore, once the computational cost of the implemented model has been quantified, this will then act as a baseline to which later on efficiency-focused adaptations can be compared, and their success judged.


    \subsection{Dataset}
    \label{section: dataset}

    To complete this baseline study, and the entirety of the research presented by this thesis, the domain of financial market volatility forecasting was chosen, as it is currently one of the most popular applications of DL within computational finance. This is demonstrated through the plethora of surveys and research papers published examining the use of DL for volatility forecasting, such as the extended review of ge-2022 analysing recent advances in the use of DNNs within this domain; in fact, volatility forecasting is within the top five most popular uses of DL within financial research (sezer-2019). Furthermore, chartis-2019 demonstrated that this domain is a popular application of DL within the financial industry, finding that $70\%$ of financial institutions use ML for risk analysis, most commonly for forecasting ``market risk" variables such as market volatility. This extensive use is primarily due to the superior accuracy provided by DL methods: \citet{chartis-2019} found that $44\%$ of financial firms are motivated to use DL by the promise of ``greater accuracy", and a multitude of recent work---such as that of \citet{rahimikia-2020}, bucci-2020, and \citet{rodikov-2022}---has demonstrated that DL outperforms traditional volatility forecasting methods. For this reason, it is clear that DNN-based volatility forecasting is a characteristic example of the use of DL within finance; hence, showing the applicability and benefit of Green AI within this domain acts as an exemplar of how the energy consumption of DL for finance in general can be reduced, and the ESG impacts of Fintech mitigated.

    Despite this choice of domain considerably narrowing the research application from the scope of work encompassing Fintech and computational finance, the field of financial market volatility forecasting is still incredibly large. Therefore, a specific application within volatility forecasting must be chosen. Luckily, reviews such as that of \citet{ge-2022} have extensively surveyed this domain and highlighted the most popular constituent applications. In their analysis of 35 research papers, \citet{ge-2022} found that DNNs were most commonly used for forecasting the volatility of the S\&P 500 market (accounting for 12 of the 35 works), primarily due to the accessibility of data. The \emph{Standard \& Poor's 500 Index} (S\&P 500) is a stock market index capturing the price movements of 500 leading US companies. It is often regarded as one of the best indicators of the performance of the global stock market; for this reason, it is unsurprising that S\&P 500 data has become a popular target for time series modelling, constituting the majority of applications of forecasting models explored in the systematic reviews of \citet{sezer-2019} and \citet{thakkar-2021}. Therefore, in an attempt to make the developed model characteristic of those used within financial modelling, and ensure the results are universal, S\&P 500 data was chosen as the specific modelling domain of this research. 

    To enable sequence learning, the S\&P 500 data is represented as a multivariate time series consisting of $18261$ daily sampled timesteps between the fifth of January 1950 (1950--01--05) and the first of August 2022 (2022--08--01). This period and sampling frequency were specifically chosen as recent DNN models---such as those of as \citet{bucci-2020} and \citet{rodikov-2022}---have demonstrated effective performance in this domain, as daily data strikes an effective compromise between maintaining a reasonable dataset size and having a high enough frequency of data instances to permit detailed analysis. Each timestep maintains the value of seven financial variables of the S\&P 500 market: the daily \emph{open}, \emph{close}, \emph{high}, and \emph{low} prices, trading \emph{volume}, associated \emph{return}, and market \emph{volatility} (Table \ref{table: time-series}). The price values are directly taken from the S\&P 500 market, being used to compute the day's return and then the rolling volatility over the previous $10$ returns. Specifically, the return is calculated as the logarithmic change in closing price from the previous day (Equation \ref{eq: return}). From this, the historical volatility is then calculated as the standard deviation $\sigma_{t}^2$ of returns over the past $10$ days; namely, $\sigma_{t}^2$ is computed from the time series $r_{\tau_1}, \ldots, r_{\tau_2}$ over the time interval $\tau_1 \to \tau_2$ where $\tau_2$ is the current timestep $t$ and $\tau_1$ is 9 timesteps in the past. The HV is specifically calculated through Equation \ref{eq: hv1}, iteratively constructing the volatility time series $v_0, v_1, \ldots, v_t$; this echoes the approach of \citet{lahmiri-2017} who also developed an ANN for volatility forecasting. Historical volatility was specifically chosen as the metric to be forecast as it was identified by \citet{ge-2022} as the most commonly explored way of measuring market volatility, being used in 25 out of the 35 research papers they surveyed. Furthermore, HV has been utilised in many innovative explorations of this field, such as the recent work of \citet{rahimikia-2020} and \citet{rodikov-2022} who both demonstrated the exceptional forecasting accuracy of DNNs for this metric.


    \begin{table}[ht]
        \centering
        \label{table: time-series}
        \begin{tabular}{|l|lllllll|} 
            \hline
            \textbf{Timestep} & \textbf{Open} & \textbf{Close} & \textbf{High} & \textbf{Low} & \textbf{Volume} & \textbf{Return} & \textbf{Volatility}  \\ 
            \hline
            2022-08-01 & 0.855942 & 0.855188 & 0.856735 & 0.853413 & 0.309027 & -0.025801 & 0.140913    \\ 
            \hline
            2022-07-29 & 0.850728 & 0.857619 & 0.855739 & 0.849899 & 0.333186 & 0.128754  & 0.145031    \\ 
            \hline
            2022-07-28 & 0.837990 & 0.845556 & 0.843038 & 0.831855 & 0.338870 & 0.110068  & 0.147466    \\ 
            \hline
            2022-07-27 & 0.822442 & 0.835378 & 0.834864 & 0.823165 & 0.312798 & 0.235645  & 0.129777    \\ 
            \hline
            2022-07-26 & 0.822814 & 0.813996 & 0.816945 & 0.814652 & 0.269089 & -0.105961 & 0.126826    \\
            \hline
        \end{tabular}

        \caption{\centering The final 5 timesteps of the multivariate time series (in reverse chronological order).}
    \end{table}


    The implemented baseline DNN (and all implementations following it) uses the volatility time series $v_0, v_1, \ldots, v_t$ to forecast the daily historical volatility of the S\&P 500 at a single future timestep $t+1$ (generating the prediction value $\hat{v}_{t+1}$). To train and test the model, the full time series is broken down into individual sequences of $10$ timesteps. Namely, each data instance is a small subsequence of the entire time series, comprised of $10$ consecutive timesteps, where each timestep has $7$ variables associated with it (Table \ref{table: time-series}). Hence, each data instance in the full dataset is in essence $7$ small time series; one for the closing prices, returns, historical volatilities, and so on. Each of these variables was normalised through min-max scaling, restricting their values to the range 0--1 (except the returns, which were allowed to fluctuate between $-1$ and $1$ to permit negative values); this scaling approach was similarly used by \citet{rodikov-2022}, who highlighted that normalisation is essential to a stable and efficient training process.


    \begin{figure}[ht]
        \label{fig: subsequences}
        \centering
        \includegraphics[width=0.8\textwidth]{subsequences.png}
        \caption{\centering Subsequences comprising the multivariate time windowed time series for an example instance from the training dataset}
    \end{figure}


    Analogously to many of the approaches explored within the review of \citet{sezer-2019}, the sequence learning task was then set up as a sequence-to-sequence problem, where each multivariate $10$-step time series is input into the DNN, and to generate the output $\hat{y}_t = \hat{v}_{t+1}$ which is a prediction of the true next element $v_{t+1}$ of the volatility time series. Thus, each pass through the model predicts the volatility of the S\&P 500 market at timestep $t+1$ through analysing the preceding multivariate time series over steps $(t-9) \to t$ (i.e. the previous 10 closing prices, returns, volatilities, etc.). This is shown through Figure \ref{fig: subsequences}, which (for a randomly selected data instance) gives an example of subsequences that make up each multivariate $10$-step time series, and Figure \ref{fig: volatility-subsequence} demonstrating a volatility subsequence with the true label $v_{t+1}$ to be predicted by the model.


    \begin{figure}[ht]
        \label{fig: volatility-subsequence}
        \centering
        \includegraphics[width=0.6\textwidth]{volatility-subsequence.png}
        \caption{\centering Volatility subsequence for an example data instance, including the true label to be predicted}
    \end{figure}


    This multivariate, time windowed approach was chosen as it has been effectively used by a host of popular studies on the use of DNNs for time series prediction, and in particular volatility forecasting. For example, \citet{xiong-2016} used $25$-dimensional multivariate time series (incorporating daily open, close, high, and low prices, and returns) to demonstrate the prediction accuracy and robustness of their RNN-based model for the S\&P 500. Furthermore, \citet{xiong-2016}, \citet{bucci-2020}, and \citet{rodikov-2022} all utilised a dataset consisting of time windowed subsequences; \citet{xiong-2016} specifically made predictions using a $10$-step rolling time window, as they found this to be consistent with the majority of comparable models in this field.

    Hence, a dataset has been developed in keeping with popular modelling implementations within this field. This establishes the chosen domain as characteristic of a typical application of DNNs within financial risk modelling, and computational finance in general, thus permitting the results generated by the DNN and training processes to be as universally representative of DL for finance as possible.


    \subsection{Model Approach}
    \label{section: model-architecture}

    To conduct volatility forecasting over the chosen domain, an LSTM-based DNN was implemented. This recurrent architecture was chosen as RNNs and LSTMs have been shown to produce superior accuracy when forecasting time series in computational finance and further afield. Namely, both \citet{lipton-2015} and \citet{yu-2019} identified that the majority of recent advances within sequence modelling have been facilitated by LSTMs, with successful LSTM-based models being implemented across financial applications including forecasting S\&P 500 prices \citep{fjellstrom-2022}. This research has proven the capability of LSTMs to produce state-of-the-art modelling accuracy by effectively modelling both long and short-term dependencies in sequences, and capturing complex dependencies between highly variable features. Furthermore, these architectures have been successfully applied to forecasting financial market volatility, with \citet{ge-2022} finding 9 out of 21 surveyed pure models used RNNs, and \citet{bucci-2020} showing that LSTMs outperform other traditional methods for forecasting RV over S\&P 500 data. Hence, to act as a baseline model that is characteristic of typical DNNs used for forecasting HV, an LSTM architecture was chosen.

    Devising a specific configuration of hyperparameters for the LSTM model was similarly done by surveying the literature base of LSTM-based volatility forecasting, and constructing a hyperparameter set that was representative of typical models in this field. The hyperparameters of the model that were determined in this manner are as follows: \emph{number of layers}, \emph{neurons per layer}, \emph{learning rate}, \emph{epochs}, and \emph{batch size}. First, existing models were used to determine the number of LSTM layers to implement within the deep model. The reviewed DNNs were found to typically exploit between 1 \citep{bucci-2020} and 5 \citep{kim-2018} layers; therefore, to strike a balance between these implementations, the model proposed in this thesis relies on $3$ LSTM layers. Existing implementations typically used 5--25 neurons per layer within each LSTM to evaluate its operations; hence, to demonstrate the computational cost of exploiting cutting-edge DNN-based volatility models the implemented architecture used $24$ neurons per layer (Table \ref{table: architecture}). Since the output layer is simply used to output the predicted volatility at a single timestep $t+1$ (by assembling all outputs of the final LSTM layer), this layer consists of a single neuron fully connected to its preceding hidden layer, from which the network's single output value (the forecast volatility $\hat{v}_{t+1}$) is read.


    \begin{table}[ht]
        \centering
        \label{table: architecture}
        \begin{tabular}{|l|l|l|l|} 
        \hline
        \textbf{Layer} & \textbf{Type} & \textbf{Output Shape}   & \textbf{Parameter Count}  \\ 
        \hline
        Input layer    & Vector        & $(batch\_size, 10, 7)$  & -                         \\ 
        \hline
        Hidden layer 1 & LSTM          & $(batch\_size, 10, 24)$ & 3072                      \\ 
        \hline
        Hidden layer 2 & LSTM          & $(batch\_size, 10, 24)$ & 4704                      \\ 
        \hline
        Hidden layer 2 & LSTM          & $(batch\_size, 10, 24)$ & 4704                      \\ 
        \hline
        Output layer   & FCN           & $(batch\_size, 1)$      & 25                        \\
        \hline
        \end{tabular}
        \caption{\centering Architecture of Baseline LSTM Model}
    \end{table}


    To train the proposed LSTM model, the learning rate, epoch count, and batch size hyperparameters must also be determined. The surveyed models typically exhibited a learning rate of $0.001$ (e.g. \citet{rahimikia-2020} and \citet{zhang-2022}), training processes consisting of between $50$ \citep{rahimikia-2020} and $600$ \citep{xiong-2016} epochs, and most commonly used a batch size of $32$ (e.g. \citet{xiong-2016}). Hence, to implement a model that was both characteristic of these statistics and experimentally determined to produce the best performance, a learning rate of $0.001$, epoch count of $100$, and batch size of $32$ were chosen (Table  \ref{table: hyperparams}).


    \begin{table}[ht]
        \centering
        \label{table: hyperparams}
        \begin{tabular}{|l|l|l|l|l|l|l|l|} 
            \hline
            \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Window}\\\textbf{\footnotesize Size}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Time Series}\\\textbf{\footnotesize Variables}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Prediction}\\\textbf{\footnotesize Length}\end{tabular} & \textbf{\footnotesize Layers} & \textbf{\footnotesize Neurons} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Learning}\\\textbf{\footnotesize Rate}\end{tabular} & \textbf{\footnotesize Epochs} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Batch}\\\textbf{\footnotesize Size}\end{tabular}  \\ 
            \hline
            10                                                                      & 7                                                                                 & 1                                                                             & 3               & 24               & 0.001                                                                     & 100             & 32                                                                     \\
            \hline
        \end{tabular}

        \caption{\centering Hyperparameters of Baseline LSTM Model}
    \end{table}


    \subsection{Analysis Methods}
    \label{section: metrics}

    To demonstrate the performance of the implemented model, the LSTM-based DNN is first trained over a dataset $D_{train}$; its performance is then tested over the separate dataset $D_{test}$. These datasets are constructed by implementing a train-test split of the full dataset $D$ (of multivariate, time windowed sequences of S\&P 500 data), where $D$ is partitioned into the two distinct subsets $D_{train}$ and $D_{test}$ used for training the model and testing its performance on unseen data instances. In this research, an $80 \colon 20$ split is implemented, using $80\%$ of data instances for training and the remaining $20\%$ for testing (determined experimentally to produce the best results); this split is demonstrated for the full (unwindowed) volatility time series in Figure \ref{fig: traintest-split}.


    \begin{figure}[ht]
        \label{fig: traintest-split}
        \centering
        \includegraphics[width=0.8\textwidth]{traintest-split.png}
        \caption{\centering Visualisation of the $80 \colon 20$ train-test split dividing training instances (blue) and testing instances (orange)}
    \end{figure}


    \subsubsection{Accuracy Metrics}
    \label{section: accuracy-metrics}

    After the DNN has been trained over the dataset $D_{train}$, predictions are made using the data instances in $D_{test}$; these predictions are analysed through a collection of statistical metrics that objectively quantify the modelling error $\varepsilon$ and prediction accuracy. Namely, accuracy metrics are used to explicitly measure the divergence between the predicted volatility time series value $\hat{y}_t = \hat{v}_{t+1}$ and the true label $y_t = v_{t+1}$ enumerating the actual recorded HV at that future timestep. These metrics are important as they not only demonstrate the success of the baseline model, but will later be used to contextualise the effect of the implemented energy and data-efficient adaptations, highlighting their impact on performance. 

    One metric typically used by volatility forecasting models (such as that of \citet{zhang-2022}) is the \emph{coefficient of determination} $R^2$, which computes the ratio between the estimated variance of the prediction error $\varepsilon$ and the variance of the variable $y_t$ being predicted, shown in Equation \ref{eq: r-squared} for the predictions $\hat{y}_t$, the true labels $y_t$, and the sample mean of labels $\mu^{(y)}$. The value of $R^2$ for a given set of predictions represents the proportion of the variability of labels that is correctly captured by the model through its predictions. This value ranges between $- \infty$ and $1$, with a score of $1$ meaning the true labels have been perfectly represented by the model's predictions. The coefficient of determination is an effective metric for demonstrating how well a model represents data (in particular time series data), and has been used to showcase the performance of several volatility models such as the ANN implementation of \citet{zhang-2022}. However, it also has several limitations. Most notably, this metric doesn't explicitly quantify whether a model is good or bad, it only shows whether the testing performance is noticeably better than that of a comparison \emph{constant model} that outputs predictions simply as the sample mean of the observed inputs; hence, its results can sometimes be misleading.


    \begin{equation}
        \label{eq: r-squared}
        R^2 = 1 - \frac{\sum_{t=0}^n (y_t - \hat{y}_t)^2}{\sum_{t=0}^n (y_t - \mu^{(y)}))^2}
    \end{equation}


    Because of this limitation, several other statistical metrics are commonly used to quantify the prediction accuracy of DNNs. Two such measures are the \emph{mean absolute error} (MAE) and \emph{root mean squared error} (RMSE). Given the set of predictions $\hat{y}_t$ and labels $y_t$, MAE measures the average magnitude of the prediction error $varepsilon$, taken as the mean absolute difference between $\hat{y}_t$ and $y_t$ over the entire test set (Equation \ref{eq: mae}). RMSE also measures the magnitude of deviations of predicted values from the true labels, computing this as the standard deviation of prediction errors (Equation \ref{eq: rmse}). Low MAE and RMSE values (close to $0$) both indicate that the model is accurate; however, the squared component of RMSE gives increasing weight to large errors and hence is useful for penalising highly anomalous predictions more than slight deviations, whereas MAE gives a uniform representation of $varepsilon$. One limitation of both MAE and RMSE is that they are scale-dependent; namely, when predicting small values the magnitude of the computed $varepsilon$ will always be smaller than that of a model predicting large values, irrespective of the model's accuracy. Hence, these metrics are typically utilised to compare various models over the same domain, such as by \citet{rodikov-2022} who used both MAE and RMSE to compare the performance of LSTMs and GARCH. The MAE and RMSE accuracy metrics will be used similarly in the research of this thesis, comparing the performance of the baseline DNN to the proposed DNNs using energy and data-efficient training processes, in an attempt to quantify how prediction accuracy has been affected by the implemented extensions. 


    \begin{equation}
        \label{eq: mae}
        MAE = \frac{1}{n} \sum_{t=0}^n \lvert y_t - \hat{y}_t \lvert
    \end{equation}
    
    \begin{equation}
        \label{eq: rmse}
        RMSE = \sqrt{\frac{1}{n} \sum_{t=0}^n (y_t - \hat{y}_t)^2}
    \end{equation}


    To further quantify the accuracy of the implemented models, and address the issues inherent to both MAE and RMSE, a scale-independent measure is required. One such metric is the \emph{mean absolute percentage error }(MAPE) of predictions, which computes the absolute difference between $\hat{y}_t$ and $y_t$ over the test set as a percentage average (Equation \ref{eq: mape}). This metric provides an objective and easily interpretable picture of the accuracy of a model and has been used to analyse numerous volatility forecasting DNNs, including by \citet{xiong-2016} and \citet{zhang-2022}. Hence, this research additionally computes the MAPE of each implementation to independently demonstrate the performance of the baseline DNN and efficient adaptations.


    \begin{equation}
        \label{eq: mape}
        MAPE = \frac{1}{n} \sum_{t=0}^n \bigg\lvert \frac{y_t - \hat{y}_t}{y_t} \bigg\lvert * 100\%
    \end{equation}


    \subsubsection{Efficiency Metrics}
    \label{section: efficiency-metrics}

    Beyond generating accurate performance, the efficiency of the implementations is of vital importance to this research. An informative efficiency metric is required to demonstrate the issue with traditional DNN approaches, quantifying the computational cost of the baseline model to give an insight into the environmental impact of such systems. Furthermore, efficiency metrics are imperative to demonstrating the improvements facilitated by Green AI by quantifying how the proposed training adaptations have reduced the resources necessary to train a DNN. 

    Several DL efficiency metrics have been presented by \citet{schwartz-2019}, who assert that the computational cost of a DNN is proportional to the cost of passing a single data instance through the network, the size of the utilised datasets, and the number of hyperparameters being tuned (Equation \ref{eq: cost-r}). Since the architecture used in this research is fixed, and the hyperparameters predetermined, these variables are not covered by the efficiency calculations analysed; however, dataset size is an important aspect of the efficiency of training a DNN, explored later in Section \ref{section: data-extensions}. \citet{schwartz-2019} present several metrics for quantifying the efficiency of a DNN, most notably FLOP count; however, these metrics focus upon the efficiency of the architecture of a DNN---for example, quantifying the number of FLOPs taken to pass a single instance through the network---and thus are not directly applicable to calculating the efficiency of training a DNN.

    \citet{amodei-2018} also explore the efficiency of DNNs, asserting that the cost of training a given network is dependent on the hardware through which this process is implemented, and the training time $T_{days}$ (Equation \ref{eq: cost-t}). As the implemented experimentation all relies upon the same hardware, metrics analysing hardware efficiency are also redundant to our study. However, training time is an important aspect of the cost of DL training, and hence to quantify the energy efficiency of training the total time required to train a DNN with accurate performance is calculated. Training time is a simple but effective measure for highlighting the efficiency of a program, as more time typically means a higher computational cost. Reporting the network training and parameter tuning time of newly developed DNNs has been raised by \citet{strubell-2019} as an effective measure through which a cost-benefit analysis of DL systems can be conducted. Although it is important to note that training time is not a completely representative metric, as it is reliant upon the underlying hardware and software dependencies of the system \citep{schwartz-2019}. However, since this efficiency metric is used for comparison of the relative differences in computational cost between the various training approaches implemented (all of which use the same underlying hardware and software), the calculation of training time is both valid and beneficial in this context.

    Beyond solely evaluating total training time, the efficiency of a training algorithm can additionally be determined by inspecting how fast and smooth the minimum of the loss function is converged upon. As outlined in Section \ref{section: deep-learning}, a DNN is trained by adjusting the network's parameter set in the direction of the negative gradient of the loss function, determined through backpropagation of the DNN's prediction error. Hence, the purpose of DNN training is to find the optimal parameter set that corresponds to the minimum of that loss function, meaning the DNN has an optimally low prediction error. The efficiency of a DL training process can therefore be determined by the speed and smoothness at which this minimum point is approached; this is known as the convergence rate of training, which calculates the size of the step taken towards the loss function's minimum after each epoch. Convergence rate is an important factor when considering the efficiency of a DL training algorithm, as the faster a DNN's parameters converge towards their optimal values, the more effective the early stages of training (as the DNN learns more over each epoch), and thus the less time, fewer calculations and data evaluations, and lower energy consumption is required to develop an accurate model.


    \section{Energy-Efficient Training Extensions}
    \label{section: energy-extensions}

    \subsection{Aims of Study}

    Starting from the proposed model in Section \ref{section: baseline}, energy-efficient Green AI methods are introduced to the training process, aiming to minimise the energy expended to train an accurate DNN for volatility forecasting, and hence reduce the ESG impacts of DL for finance. This research focuses on three avenues through which the training process of a DNN can be adapted to improve energy efficiency, all of which have been explored in literature surveying Green AI \citep{xu-2021}: mixed-precision training, supervised layer-wise pre-training, and unsupervised layer-wise pre-training. Initially, mixed-precision training will be explored, where network computations are quantised to utilise low-bit representations. This exploration aims to demonstrate how using lower precision data and variables can boost training speed whilst still producing an accurate model, thus showing that mixed-precision training is a viable mechanism through which the energetic cost of developing DNNs for computational finance can be reduced. The experimentation then proceeds to explore progressive training methods; this begins by implementing a supervised pre-training process for the proposed forecasting model, in an attempt to demonstrate the utility of this approach in reducing the time and energy expended training RNNs in this domain. Unsupervised pre-training is then explored, where a more complex approach is taken to pre-train the proposed DNN to produce further computational cost reductions.


    \subsection{Model Approach}

    \subsubsection{Mixed-Precision Training}
    \label{section: mixed-precision-method}

    The first method presented by this research for reducing the energy consumption (and hence carbon emissions) of DL is mixed-precision training. As explored in Section \ref{section: quantisation}, a popular focus chosen by developers investigating Green AI and low-resource systems is the use of quantisation. These implementations---such as the quantised LSTM of \citet{he-2016} and quantisation-aware training process of \citet{fan-2020b}---reduce energy and memory costs by lowering the precision at which variables are stored and operations are evaluated. Mixed-precision training---where a combination of quantised and high-precision representations are used to balance efficiency and accuracy---is a core aspect of how quantisation is realised in real-world applications (e.g. the mixed-precision RNN of \citet{ott-2017}). To demonstrate the benefits of quantisation, this research explores the use of mixed-precision training in the context of LSTM-based volatility forecasting. A mixed-precision training process is implemented that aims to reduce the computational and memory cost of training the proposed DNN, reducing the energy consumption and ESG impacts of volatility forecasting and Fintech in general.

    To implement mixed-precision training, part of the Tensorflow ML framework \citep{abadi-2016} known as the \emph{Keras mixed-precision API} was exploited. This API enables ML developers to adapt the precision of variables and operations within an implemented DNN, changing the default $32$-bit precision typically used by DL models implemented in Tensorflow. In this research, the mixed precision API is used to specify a \emph{`mixed\_float16'} precision policy \citep{abadi-2016}, where a combination of $16$-bit and $32$-bit floating-point data types are used during training. Specifically, this instructs Tensorflow to use a $16$-bit floating-point representation for evaluating network computations, whilst maintaining a $32$-bit floating-point representation for storing variables. Using lower precision reduces the amount of memory used by network operations, increasing the speed of calculations and reading from memory by minimising their computational load and allowing increased hardware acceleration. Specifically, $16$-bit representations are utilised as modern GPUs and TPUs have been shown to run operations significantly faster when using $16$-bit precision, as batches of data consume half the amount of memory when being passed through the DNN. High precision is still used for storing variables to maintain numerical stability, ensuring that the model maintains accurate performance. Hence, this implementation of mixed-precision training strikes a compromise between increasing the speed (and reducing the energetic cost) of network operations during DNN training and preserving accurate performance.

    The efficiency benefits provided by mixed-precision training were evaluated by using the same DNN as proposed in Section \ref{section: baseline}. First, the use of a `mixed\_float16' precision policy is specified; an identical network with the same LSTM-based architecture and hyperparameters of the baseline model (Tables \ref{table: architecture} and \ref{table: hyperparams}) is then implemented using variables of the specified precision. This DNN is trained using the same training algorithm as in the baseline case, for the same number of epochs. Once training is complete, both the accuracy of the model's performance, and efficiency of the training approach are evaluated, computing the $R^2$, MAE, RMSE, and MAPE of the optimised DNN, and the total time and memory used over the training process. These metrics are then put into context against those computed for the baseline model, demonstrating the new efficiency gains of the new model, and exemplifying the benefit of mixed-precision training for reducing the energy consumption, carbon emissions, and ESG impacts of DL-based financial volatility forecasting models.


    \subsubsection{Supervised Layer-wise Pre-Training}
    
    Innovative DNN approaches within Green AI research and further afield have been increasingly using pre-training to develop models with better performance and training efficiency. Progressive training is one such method, highlighted by Green AI researchers (such as \citet{xu-2021}) as being capable of reducing the length of DNN training through a computationally inexpensive pre-training phase. As discussed in Section \ref{section: progressive-training}, progressive training builds up a DNN layer-by-layer through an iterative process of adding new layers and training each individually. Research into this approach---such as the exploration of \citet{ienco-2019} into progressive training of RNNs---has shown its effectiveness at reducing the computational cost of training a DNN, as the inexpensive pre-training phase allows a significant reduction in the number of epochs necessary to optimise the parameters of the full network. Both the low-cost pre-training phase (which has low computational cost since only single network layers are trained) and shortened full network training mean that progressive training is an effective way to reduce the time and energy required to train a DNN, and hence provides a promising avenue for improving the ESG impact of Fintech applications such as financial volatility modelling \citep{xu-2021}.

    In this section of research, supervised layer-wise pre-training will be explored to demonstrate the benefits of progressive training. This method---shown by \citet{ienco-2019} to produce impressive results in RNNs---trains network layers individually using supervised learning over a labelled dataset of the desired modelling domain and task. In our case, the time series dataset outlined in Section \ref{section: dataset} is used, with each round of pre-training optimising a new layer's parameters to predict the HV $\hat{v}_{t+1}$ at timestep $t+1$ within a sequence. The supervised pre-training phase begins with a single layer base model; in this research, the base model consists of a single LSTM layer combined with a fully connected single-neuron output layer. Before pre-training can begin, additional hyperparameters must be specified, outlining the number of \emph{pre-training epochs} and \emph{tuning epochs} employed. The utilised implementation conducted each training round over 20 epochs, pre-training each new LSTM layer individually over $20$ epochs. A further $30$ epochs were then used to conclude training, where the parameters of the full network are tuned. Hence, a total of $20 * 3 + 30 = 90$ epochs were trained over to fully optimise the DNN. These specific hyperparameters were chosen as they were experimentally determined to provide the best accuracy-efficiency tradeoff, minimising the total number of epochs necessary but maintaining the model's forecasting performance. 

    The first round of layer-wise pretraining focuses on training the base network. This shallow network is trained for $30$ epochs over the full dataset $D_{train}$, optimising the parameters of the single LSTM layer and the DNN's output layer (which is a fully-connected single neuron, as in the baseline model). After this initial pre-training round, the parameters of the first LSTM layer are fixed, a new LSTM layer is added between the initial hidden layer and the output layer, and this new layer is trained over the next $30$ epochs. This iterative process was repeated until a DNN with $3$ LSTM layers was constructed, meaning this experimentation utilised the same architecture as in the baseline case. After the full network is pre-trained, final tuning begins, where all network parameters are unfixed and optimised over $20$ additional epochs, starting from the initialisation values deduced during pre-training.

    Once a fully trained network was developed, its performance was evaluated on the testing dataset. This involved the calculation of the $R^2$, MAE, RMSE, and MAPE of test predictions, to deduce the accuracy of the model produced through supervised layer-wise pre-training, and quantify the performance difference to the baseline model. The efficiency of this training approach was also evaluated, measuring the total training time (consisting of the pre-training and tuning times) and memory consumption.


    \subsubsection{Unsupervised Layer-wise Pre-Training}

    Although supervised layer-wise pretraining is a simple method through which progressive training can be realised, more advanced approaches typically utilise unsupervised layer-wise pre-training to facilitate greater performance and efficiency improvements. In analyses of the effectiveness of this unsupervised approach to training LSTMs, both \citet{xu-2018} and \citet{sagheer-2019} found it induced faster convergence toward the optimal parameter set and smaller test error than comparable LSTMs using traditional training algorithms. Hence, in an attempt to further improve the efficiency of DNN training in the chosen domain, the previously presented supervised pre-training is adapted to utilise unsupervised layer-wise pre-training.

    Using the same hyperparameters as were experimentally devised in the supervised case, the implemented unsupervised layer-wise pre-training approach began by initialising a similar single-layer LSTM base network. The base architecture was identical to the previously exploited base network, except a different output layer was utilised to enable unsupervised learning. The new output layer consisted of the same dimensions as the input layer such that the network could be trained as an autoencoder. Since the same training dataset of multivariate time series subsequences was utilised, the unsupervised pre-training approach trained each layer to reconstruct this subsequence, meaning the network learned to output a representation of the input time series (tuning its parameters to minimise the difference between each layer's inputs and outputs). Hence, over the first $30$ epochs of pre-training, the single-layer LSTM network learned as an unsupervised autoencoder to reconstruct the multivariate sequence instances of the training dataset. After completion of this initial pre-training round, another LSTM layer was added to the DNN. The new layer was then trained independently also as an autoencoder, minimising the difference between its inputs and outputs. This unsupervised learning is repeated over all $3$ pre-training rounds, iteratively building up an autoencoder network of $3$ LSTM layers. The concluding full network tuning is then used to convert this autoencoder into a volatility forecasting model that predicts the HV $\hat{v}_{t+1}$ at timestep $t+1$ of the input time series. To achieve this, the autoencoder output layer used during pre-training is discarded, and a new output layer is appended that consists of a single fully-connected neuron (for predicting a single volatility value). The parameters of the full network are then optimised over the $20$-epoch tuning process, using supervised learning to develop an accurate forecasting model from the training data.

    To compare the effectiveness of unsupervised layer-wise pre-training to both the supervised approach and baseline training method, the same accuracy ($R^2$, MAE, RMSE, and MAPE) and efficiency (training time and memory consumption) metrics were computed (over training and upon the testing dataset). This allows an analysis of the energy efficiency of this implementation of progressive training, demonstrating the benefits it provides to reducing the energy consumption and carbon emissions of training DNNs for financial volatility forecasting (and Fintech in general).


    \section{Data-Efficient Training Extensions}
    \label{section: data-extensions}

    \subsection{Aims of Study}

    As asserted by \citet{schwartz-2019} in their analysis of Red AI, a core component of the computational cost of state-of-the-art DL is the recent trend of utilising extremely large datasets (Equation \ref{eq: cost-r}). This approach typically relies on generating performance gains through feeding expansive collections of training data into a model, most of which is not beneficial to learning \citep{bender-2021}. Whilst these datasets have been shown to facilitate impressive accuracy gains, current DNNs typically do not deal with data intelligently or efficiently \citep{aljarrah-2015}; this is because a significant amount of time and energy is wasted training over redundant data instances that do not benefit performance. Hence, when training over vast datasets these cutting-edge models accrue a significant energetic cost, further adding to the negative ESG impacts of DL. 

    Therefore, to further improve the energy efficiency of DNNs, this research focuses on how data-efficient training adaptations can be used to minimise the amount of training data necessary for DL, reducing the memory and energy requirements of these systems. A data-efficient model is developed that exploits Green AI methods in an attempt to use training data more intelligently; this is achieved through adapting the training process to only select (and train over) data instances that are beneficial to learning, discarding redundant instances. Thus, the size of the total dataset trained over is reduced, lowering the time and energy expended over training in an attempt to minimise the ESG impacts of DL training in the context of financial volatility forecasting. 

    Furthermore, \citet{bender-2021} showed that compact networks with small parameter sets typically require increased amounts of training data to achieve comparable performance to larger models. Thus, the benefits of intelligent data-efficient training additionally aim to mitigate this requirement, allowing compact networks to be used without inflicting the further computational costs associated with large datasets.


    \subsection{Model Approach}
    \label{section: al-model}

    Possibly the most promising approach to improving the data efficiency of DNNs is active learning \citep{ren-2021}. As described in Section \ref{section: active-learning}, active learning has been shown to drastically improve the data efficiency of training DNNs by reducing the time and energy wasted on redundant data instances \citep{xu-2021}, as well as achieving an exponential improvement in the time required to label training data, and successfully dealing with high-dimensional sequential data \citep{ren-2021}. Hence, in an attempt to improve the data efficiency of DL training in the context of LSTM-based financial volatility forecasting, this experimentation explores the use of active learning.

    The active learning training process aims to reduce computational costs by training over a subset of the full data space comprised of data instances that are believed to provide the most utility to the model's learning process. In this experimentation, pool-based active learning is implemented, which uses two datasets $P \subset D_{train}$ and $V \subseteq D_{train}$ to split useful and unuseful data instances according to an importance function. Beginning with an initial seed $P^{(1)}$ of $n_{sample}$ data instances, an iterative process is exploited where the DNN is trained over pool $P$ and used to predict the outputs of all data instances in the validation set $V$; these predictions are then evaluated through the importance function, and the $n_{sample}$ most important data instances moved into the pool before the next training round is conducted.

    As noted by \citet{xu-2021}, active learning algorithms can be classified through the importance function they use to identify useful data instances; in this research \emph{greedy sampling} (GS) is employed. Originally proposed by \citet{yu-2010}, greedy sampling for active learning selects instances based on their location within the data space, and has been shown to generate some of the best performance of any active learning approach. A specific pair of greedy sampling methods is used, known as \emph{greedy sampling on the inputs} (GSx) and \emph{greedy sampling on the outputs} (GSy). These methods, conceived by \citet{wu-2019}, implement a performant greedy sampling approach that aims to increase the diversity of the input and output spaces of a model, improving training efficiency and model accuracy. In our implementation, GSx is used to specify the initial seed pool $P^{(1)}$, and GSy is used in the following training iterations to select data instances to be moved from the validation set $V^{(i)}$ to the pool set $P^{(i)}$. 

    GSx is a greedy method through which an initial seed pool $P^{(1)}$ can be constructed by selecting $n_{sample}$ data instances from the full dataset $V^{(0)} = D_{train}$ of size $N$ \citep{wu-2019}. First, the GSx algorithm selects the single data instance from the full validation set $V^{(0)}$ that is the shortest distance from the centroid (i.e. mean instance) of the entire dataset; in this case, the Euclidean distance is used, which finds the shortest difference between multidimensional matrices. Once the data instance closest to the dataset's centroid is found, it moved from $V^{(0)}$ to $P^{(1)}$ and then used to choose the remaining $n_{sample} - 1$ data instances in the seed. This is conducted iteratively; to find the next seed instance $x_i$, we first compute the Euclidean distance between each data instance $x_m$ already chosen to be in the seed pool and each instance $x_n$ not yet chosen from $V^{(0)}$ (i.e. from the $N - n_{sample} * i$ remaining instances), constructing a 2D distance matrix (the calculation for which is shown in Equation \ref{eq: GSx-distance}). Then, for each instance $x_n$ remaining in $V^{(0)}$ we select its seed value $v_m$ that minimises the distance $x_n \to x_m$, building a 1D distance vector that enumerates the closest seed value to each remaining instance in $V^{(0)}$ (Equation \ref{eq: GSx-min}). Finally, the data instance $x_n$ furthest from all existing seed values (i.e. the instance corresponding to the maximum of the distance vector) is selected and moved from the validation set $V^{(0)}$ to the seed pool $P^{(1)}$. This process is repeated until we have a full seed $P^{(1)}$ of $n_{sample}$ data instances, and a validation set of $N - n_{sample}$ instances. By adding seeds that are furthest from any others in the pool, GSx ensures that the seed pool used to initially train the DNN is as diverse in the data space as possible, giving the model a complete picture of the breadth of the full training dataset whilst only using a small subset of instances.


    \begin{equation}
        \label{eq: GSx-distance}
        d^{(x)}_{n, m} = \lVert x_n - x_m \lVert
    \end{equation}
  
    \begin{equation}
        \label{eq: GSx-min}
        d^{(x)}_n = \min_m d^{(x)}_{n, m}
    \end{equation}


    GSy has a similar aim to capture diverse instances, but alternatively focuses on the output space of the model $M_{\theta}$ that realises the function $y = f( x \vert \theta )$ \citep{wu-2019}. The GSy algorithm implements an iterative greedy approach where data instances are selected from the validation set $V^{(i)}$ to add to the pool $P^{(i)}$ such that the prediction error of the function $f( x \vert \theta )$ over the total dataset is minimised as much as possible. Namely, when considering an input instance $x_n$, if the model produces an output $y_n = f( x_n \vert \theta )$ that is noticeably distinct from the outputs generated from other data instances, the algorithm selects $x_n$ to add to the training pool as it helps refine the parameters defining the model's predictions. GSy then works by the assertion that the more diverse the pool instances $x_m$ collected, the more accurately the optimal parameters of the model can be determined. However, this algorithm requires a functioning model $f( x \vert \theta )$ to exist (such that outputs can be generated and analysed), so it can only be used to populate a pool that has already been initialised with seed instances. Therefore, \citet{wu-2019} assert that to implement active learning, GSx should be used for determining the seed, followed by GSy to further populate the pool set at later training iterations.

    Given a pool $P^{(1)}$ of $n_{sample}$ data instances, and validation set $V^{(1)}$ of $N - n_{sample}$ instances, the GSy algorithm selects $n_{sample}$ instances to move from $V^{(1)}$ to $P^{(1)}$ (forming the new datasets $P^{(2)}$ and $V^{(2)}$) that it believes will provide the most utility to training. This is done by first evaluating the model $f( x_n \vert \theta )$ over all data instances $x_n \in V^{(1)}$, to create the set of predicted labels $\hat{Y}^{(1)} = \{ \hat{y}_n \colon \hat{y}_n = f( x_n \vert \theta ) \forall x_n \in V^{(1)} \}$. Once all $N - n_{sample}$ predicted labels have been computed over the validation set $V^{(1)}$, a 2D distance matrix is constructed enumerating the distance (as the absolute difference) between each predicted label $\hat{y}_n$ and all true labels $y_m$ of the data instances $x_m$ that already exist within the pool set $P^{(1)}$ (the calculation for which is shown in Equation \ref{eq: GSy-distance}). Analogously to within the GSx algorithm, from the distance matrix a 1D distance vector is constructed over the $N - n_{sample}$ outputs of the validation set $V^{(1)}$, enumerating the shortest distance of each prediction $\hat{y}_n$ to a true label $y_m$ of an instance $x_m$ in the pool set (Equation \ref{eq: GSy-min}). The data instances in the validation set corresponding to the $n_{sample}$ largest distances in the distance vector are then selected and moved from the validation set to the pool, creating the new datasets $P^{(2)}$ and $V^{(2)}$. This process is repeated at each training round, allowing the pool to be iteratively populated with data instances that stimulate diverse outputs from the model being trained. Thus, a DNN is developed that can produce a wide range of appropriate outputs to accurately represent a spectrum of different predictions (allowing the DNN to model a plethora of different possible scenarios), despite only being trained over a limited number of data instances.


    \begin{align}
        \label{eq: GSy-distance}
        d^{(y)}_{n, m} &= \lvert \hat{y}_n - y_m \lvert \\
        &= \lvert f( x_n \vert \theta ) - y_m \lvert
    \end{align}
  
    \begin{equation}
        \label{eq: GSy-min}
        d^{(y)}_n = \min_m d^{(y)}_{n, m}
    \end{equation}


    In their research, \citet{wu-2019} found that the use of GSx and GSy-based active learning both improved the data efficiency of training and produced a model that exhibits impressive generalisability and lower RMSE than comparable approaches using random sampling to select data instances. This demonstrates the effectiveness and robustness of active learning training that uses GSx and GSy; hence, the experimentation into data-efficient training for DNNs presented in this thesis utilises GSx for picking the seed pool and GSy for sampling pool instances during later training rounds. Specifically, GSx is initially used to generate a seed of $n_{sample} = 100$ data instances before the first iteration of active learning training begins, setting this as the pool $P^{(1)}$ and the remaining $N - 100$ data instances from the full training dataset as the validation set $V^{(1)}$. GSy is then used at each training iteration $i$ to select a further $100$ data instances to move from the validation set $V^{(i)}$ into the pool $P^{(i)}$. The full $3$-layer LSTM-based DNN proposed in Section \ref{section: baseline} is utilised over this iterative process, training over the expanding pool set for $100$ training rounds.

    To evaluate how effective and efficient this training process is, and the accuracy of the model it produces, the metrics outlined in Section \ref{section: metrics} are utilised. These inform a comparative analysis between the DNN produced through active learning and the baseline model, and an evaluation of the time and memory efficiency of their training processes. Furthermore, a close inspection of the amount of data utilised over the training process is made, demonstrating how effective active learning is at reducing the data requirements of training DNNs within the domain of LSTM-based financial volatility forecasting. Whilst proposing the GSx and GSy algorithms, \citet{wu-2019} note several limitations with this approach; most notably, they found that when the pool size during the early stages of training is too small (and too few instances are added at each iteration) the model being built exhibits very high variance, meaning its performance is not reliable. Hence, to properly explore the benefits and drawbacks of active learning for improving the data and energy efficiency of DNN training (in the context of volatility forecasting) the convergence of the model's parameters towards the minimum of the loss function is analysed, and the progression of its performance inspected over each training round, documenting the robustness of this training approach.


    % --------------------  EVALUATION ----------------------
    \newpage
    \chapter{Results \& Discussion}
    \label{chapter: results-discussion}

    The results of the experimentation presented in Chapter \ref{chapter: experiments} are explored in the following section. Initially, the data collected from analyses of the implemented methods is presented, summarising what is shown by the research findings, highlighting their significance, and examining their implications for the research hypotheses. Following this, a detailed discussion surrounding the success of each experiment is undertaken, evaluating the accuracy and efficiency of each presented method to deduce how effective Green AI approaches are at reducing the computational cost of DNN training within the chosen application of LSTM-based financial market volatility forecasting. The results of each experiment are presented and discussed with close reference to the accuracy and efficiency measures introduced in Section \ref{section: metrics}, quantifying both the improvement to energy (and data) efficiency facilitated through Green AI and the effect on performance. Initially, the performance of the models using each explored training process will be presented to demonstrate the benefits of each to maintaining and improving the accuracy of a model. Following this, model efficiency will be explored; this will indicate the overall success of this research in its application of Green AI to Fintech. Namely, the energy efficiency of the adapted training processes will be quantified and hence the success of these methods in reducing the computational cost of DNN training discussed, evaluating the utility of this approach to mitigating the ESG impacts of DL for finance. Moreover, the data-efficient training approach will be used to outline how the utilisation of vast, expensive datasets can be reduced, further reducing the computational cost of DL training and the data centres DNNs often rely upon.

    The results of this research were derived by tuning the hyperparameters of each training approach such that the optimised model was as accurate as possible, whilst maintaining an efficiency greater than that of the baseline method. This implemented an accuracy-efficiency compromise that prioritised developing models with low prediction error, subsequently evaluating the efficiency of the training process that built said performant model. To evaluate each training algorithm in a robust and reproducible manner, each was employed five times, optimising a model from scratch over a fixed number of epochs and using identical hyperparameters; the results of the most successful training round were then presented as the outcome of this research. This was to ensure that a representative result of the true ability of each training algorithm was obtained and that the outcome of this research was not biased or degraded by the stochastic nature of DL training algorithms, which can cause results to vary over different executions. To implement and run these algorithms, \emph{Google Colaboratory Pro} was used to achieve higher training performance by exploiting high-end ML hardware, executing each algorithm using a \emph{Tesla T4} GPU. 

    \section{General Predictive Performance}

    o demonstrate the success of both the baseline model and the models trained in an energy and data-efficient manner, an initial analysis tested their ability to forecast volatility over the testing dataset of time series instances not used during training. Figure \ref{fig: predictions} shows the forecasts generated by all models; each plot shows the volatility $\hat{v}_{t+1}$ predicted by the associated model over all timesteps of the testing period, contextualised against the time series of true volatility values $v_{t+1}$. At a given timestep $t$, each model took input of the preceding $10$ timesteps of the multivariate time series (i.e. the time-windowed data instance that ends at $t$) to output its one-step-ahead prediction $\hat{v}_{t+1}$; all forecast values $\hat{v}_{t+1}$ were then collected to construct the forecast time series for each model depicted in Figure \ref{fig: predictions}.


    \begin{figure}[ht!]
        \label{fig: predictions}
        \centering
        \includegraphics[width=0.5\textwidth]{results/all-predictions.png}
        \caption{\centering Predicted volatility time series of each tested model (orange dashed line) against the true volatility time series (blue) over testing dataset.}
    \end{figure}


    From inspection of each plot in Figure \ref{fig: predictions}, it can be seen that all models can generate predictions that accurately follow the trend of the true volatility sequence. Namely, over the entirety of the test set, one can see that each DNN's forecast time series follows the general trend of peaks and troughs exhibited within the volatility time series. However, close inspection of how the different models handle these rapid volatility jumps highlights the differences in forecasting characteristics generated by each alternative training process. If one inspects the significant rise in volatility in late 2008 (corresponding to the 2008 financial crisis), the baseline model is shown to underpredict the true volatility, undercutting the true peak volatility by a fairly significant margin (outputting a $\hat{v}_{t+1} \approx 0.6$ when the true value of $v_{t+1}$ is closer to $0.7$). A similar characteristic can be seen in early 2020 (corresponding to the COVID19 pandemic) and soon thereafter, where the significant fluctuations in volatility are consistently underestimated by the DNN's predictions. This suggests that whilst the baseline DNN appears to model the smaller volatility changes very accurately, large, rapid changes in the true volatility result in less accurate predictions. 

    When the predicted time series of the adapted models are inspected and compared to that of the baseline model, the differences in forecasting ability can be observed. Figure \ref{fig: predictions} shows that the models using supervised and unsupervised layer-wise pre-training most accurately represent both small and large fluctuations in volatility. The plots corresponding to the models exploiting layer-wise pre-training show that the predicted volatility values $\hat{v}_{t+1}$ are significantly closer to the true labels $v_{t+1}$ at times of high volatility (for example, the discussed 2008 and 2020 periods). The forecast values of the model trained through unsupervised layer-wise pre-training appear slightly less accurate than those of its supervised counterpart, as its predictions seem to overestimate the volatility jumps in 2008 and 2020, however less extreme movements seem to be modelled very closely. The model developed through mixed-precision training also appears to generate predictions that lie closer to the true volatility than the baseline model, although with an accuracy slightly below that of the layer-wise pre-trained models. These results suggest that supervised layer-wise pre-training generates a model with the highest accuracy, closely followed by unsupervised layer-wise pre-training, both of which outperform the baseline model. 

    When considering the model constructed through active learning, the forecast time series is also shown to accurately follow the path of the true volatility, with most fluctuations accurately captured. However, these predictions seem to slightly miss the true movements, as the scale of peaks and troughs seems to be consistently underestimated (underpredicting the value of $\hat{v}_{t+1}$ for peaks and overestimating it at troughs), suggesting this model is slightly less performant than other adapted approaches. However, the general trend of the predictions on a wider scale seems (at least) commensurate with those of the baseline model, indicating that the implemented active learning training process does not degrade the forecasting performance possible from a DNN.


    \section{Accuracy Results}

    To demonstrate the success of the implemented training methods more precisely, further analysis evaluated the models through the accuracy metrics presented in Section \ref{section: accuracy-metrics}, the results of which are shown in Table \ref{table: accuracy}. The forecasting performance of each model was quantified over the test dataset held back from training. This began with the baseline model; it was found to exhibit an impressive test accuracy, demonstrating the great ability of the LSTM architecture to model sequences such as the utilised financial volatility time series. Namely, the coefficient of determination of the baseline model was found to be $0.977414$, showing that the sum of squared prediction errors between $\hat{v}_{t+1}$ and $v_{t+1}$ is close to zero (Equation \ref{eq: r-squared}). This means that the true labels have been near-perfectly represented by the model, demonstrating the DNN trained by a traditional training process can generate excellent one-step-ahead forecasts of the market volatility of the S\&P 500. Similarly, the baseline model produced impressive MAE and RMSE scores of $0.008407$ and $0.013830$, demonstrating that the average magnitude of the prediction error is near-zero. Importantly, both the MAE and RMSE are similar, indicating that the model is averse to uniform prediction errors (errors that are uniformly distributed around the true label) highlighted by the MAE and highly deviant predictions (errors that are anomalously far from the label) identified by the RMSE. The model also generated an impressive MAPE of $14.662217\%$, indicating that the DNN has fit the training data well and is producing accurate predictions.


    % accuracy metrics
    \begin{table}[ht]
        \label{table: accuracy}
        \centering
        \begin{tabular}{|l|l|l|l|l|} 
        \hline
        \textbf{Method}                                                               & $\mathbf{R^2}$ & \textbf{MAE} & \textbf{RMSE} & \textbf{MAPE (\%)}  \\ 
        \hline
        Baseline                                                                      & 0.977414       & 0.008407     & 0.013830      & 14.662217           \\ 
        \hline
        Mixed-precision                                                               & 0.977372       & 0.009121     & 0.013843      & 12.126700           \\ 
        \hline
        \begin{tabular}[c]{@{}l@{}}Supervised\\layer-wise pre-training\end{tabular}   & 0.985642       & 0.007814     & 0.011027      & 9.932409            \\ 
        \hline
        \begin{tabular}[c]{@{}l@{}}Unsupervised\\layer-wise pre-training\end{tabular} & 0.98406       & 0.00708     & 0.01162      & 8.80475           \\ 
        \hline
        Active Learning                                                               & 0.971119       & 0.010452     & 0.015639      & 11.08715           \\
        \hline
        \end{tabular}
        \caption{\centering Accuracy data for all methods, evaluated over the testing datset.}
    \end{table}


    After certifying the performance of the baseline model, the accuracy achieved by alternative training methods that prioritised energy and data efficiency could be contextualised. This exploration began with an analysis of mixed-precision training; impressively, the mixed-precision model was found to achieve slightly better performance than its high-precision counterpart. By exploiting lower precision representations, this model was able to achieve higher accuracy than the baseline across almost all metrics, generating a $15.10\%$ lower MAE, $10.50\%$ lower MAPE, $8.53\%$ decrease in RMSE (Table \ref{table: accuracy}). These scores indicate that the DNN exploiting mixed-precision training was able to forecast market volatility more accurately than the baseline across both scale-dependent and scale-independent metrics.

    Following this result, the two models that utilised progressive training were analysed. The first model, which exploited a supervised layer-wise pre-training process, exhibited a further reduction in prediction error, achieving a higher forecasting accuracy than both the baseline and mixed-precision models that used traditional training algorithms. The optimised model produced an outstanding $R^2$ score of $0.985642$, indicating that using supervised layer-wise pre-training facilitated an almost perfect correspondence between $\hat{v}_{t+1}$ and $v_{t+1}$; additionally, this model reduced the MAPE of the baseline model by $32.26\%$ and the mixed-precision model by $18.09\%$. The improvement in accuracy was observed across the board, generating better $R^2$, MAE, RMSE, and MAPE scores than both the baseline and mixed-precision methods. The second progressively trained model tested used unsupervised layer-wise pre-training. This model also exhibited impressive performance, generating predictions of similar accuracy to its supervised counterpart. Namely, the unsupervised pre-trained model also significantly outperformed the baseline implementation, exhibiting a $0.6800\%$ higher $R^2$, $15.78\%$ lower MAE, $15.98\%$ lower RMSE, and an exceptional decrease in MAPE of $39.95\%$. When directly comparing the models using supervised and unsupervised layer-wise pre-training, however, the relative advancements are not so explicit. The test $R^2$ and RMSE scores generated by these models indicate that the supervised approach produced marginally better performance: the $R^2$ of the supervised approach was $0.1607\%$ higher and RMSE $5.103\%$ lower. Alternatively, the results of other accuracy testing metrics suggest the unsupervised approach generated a more performant model, with a $9.393\%$ smaller MAE and $11.35\%$ smaller MAPE. This suggests that both supervised and unsupervised layer-wise pre-training facilitated accuracy gains over the baseline training algorithm, but each focussed on reducing prediction error differently: for example, the lower RMSE of the supervised pre-trained DNN indicates this model less frequently generates predictions that are highly incompatible with the true value (i.e. where $\lvert \hat{v}_{t+1} - v_{t+1} \lvert$ is large). Thus, the accuracy results in Table \ref{table: accuracy} show that both supervised and unsupervised layer-wise pre-training are the most effective training methods for developing high-performance DNNs for volatility forecasting. 

    Once these three energy-efficient approaches had been explored, the experimentation turned to focus on how active learning can be used to improve the data efficiency of DNN training in this domain. The chosen active learning implementation produced impressive performance when evaluating through MAPE, exhibiting a $24.38\%$ lower test MAPE than the baseline model. However, when considering other accuracy metrics, the model was not shown to exhibit such an accuracy benefit over the baseline, having a slightly worse coefficient of determination ($0.6440\%$ lower) and noticeably increased test MAE ($24.33\%$ higher) and RMSE ($13.08\%$ higher). Thus, the performance of the active learning-based model was roughly equivalent to the baseline, outperforming it on some metrics and exhibiting lower accuracy on others. Furthermore, when different parameterisations of the active learning training process were explored (Table \ref{table: al-efficiency}), a few implementations did produce higher accuracy than the baseline (achieved through sacrificing their data efficiency gains); for example, when using 200 training iterations and a sample size of 70, a $26.83\%$ lower RMSE and $34.72\%$ lower MAPE were facilitated (but using $96.36\%$ of the full training dataset). Hence, these results indicate that active learning can be used to develop a model with similar performance to a traditionally trained baseline, supporting the assertion that this method can be tuned to only inflict a marginal accuracy detriment.


    \section{Efficiency Results}

    As outlined in Section \ref{section: efficiency-metrics}, total training time is one of the most applicable metrics for quantifying the energy efficiency of DL training algorithms in this context (as all experiments use the same underlying hardware, software, and DNN architecture). Hence, to examine the success of the presented training adaptations, the time (in seconds) taken to optimise the parameters of each model over their different training processes was quantified by profiling each implementation. 


    \subsection{Training Time Analysis}

    The overarching results of profiling each training algorithm are exhibited in Table \ref{table: efficiency}, which enumerates the total training time taken to develop each model. This analysis found that the baseline training approach took a total of $198.602$ seconds to produce its high-accuracy DNN; this result was then used to contextualise the efficiency of each subsequent training method. The first of these alternate training processes examined was mixed-precision training; this approach lead to a fairly disappointing result, as the algorithm took $18.98\%$ longer to complete than in the baseline, despite using the same number of epochs. This suggests that utilising 16-bit floating-point representations for evaluating operations during training did not directly improve the energy efficiency of training the DNN. However, the other implemented energy efficiency-focused training adaptations did produce more promising results.

    % efficiency metrics
    \begin{table}[ht]
        \centering
        \label{table: efficiency}
        \begin{tabular}{|l|l|} 
        \hline
        \textbf{Method}                                                               & \begin{tabular}[c]{@{}l@{}}\textbf{Total Training }\\\textbf{Time (s)}\end{tabular}  \\ 
        \hline
        Baseline                                                                      & 198.602                                                                              \\ 
        \hline
        Mixed-precision                                                               & 236.306                                                                              \\ 
        \hline
        \begin{tabular}[c]{@{}l@{}}Supervised \\layer-wise pre-training\end{tabular}  & 154.952                                                                             \\ 
        \hline
        \begin{tabular}[c]{@{}l@{}}Unsupervised\\layer-wise~pre-training\end{tabular} & 130.292                                                                             \\ 
        \hline
        Active learning                                                               & 212.453                                                                              \\
        \hline
        \end{tabular}
        \caption{\centering Total training time required for each implemented training process to produce a model with the accuracy outlined in the preceding accuracy table.}
    \end{table}


    Both explored progressive training approaches were shown to generate a significant reduction in the total training time required to produce an accurate model, with unsupervised layer-wise pre-training being the fastest to complete. The unsupervised method facilitated a $34.40\%$ decrease in training time, taking only $130.292$ seconds in total to complete both the pre-training and full network tuning stages. The unsupervised layer-wise pre-training algorithm also produced an impressive speedup over the baseline approach, although not quite as significant as that of its unsupervised counterpart. Namely, the supervised method reduced the total training time required to generate an accurate model to $154.052$ seconds, improving on the baseline by $22.43\%$, but taking $18.24\%$ longer than the approach utilising unsupervised pre-training. These results indicate that the most efficient way to train DNNs is through unsupervised layer-wise pre-training, taking the least amount of time to produce an accurate model, and hence likely being the most energy-efficient approach. Furthermore, it demonstrates that supervised unsupervised layer-wise pre-training are both viable ways to improve the energy efficiency of DNN training, as they both took significantly less time to train an accurate forecasting model when compared to the baseline approach.

    When the training time of the active learning-based approach is analysed, the results do not indicate that this is an effective method through which the overall length of training can be reduced. Namely, the active learning algorithm using hyperparameters detailed in Section \ref{section: al-model} marginally exceeded the total time spent training by the baseline approach by $6.974\%$. This suggests that active learning is not an effective method when aiming to reduce the direct energy efficiency of DL training by minimising total training time whilst preserving accurate performance. Although, a small proportion of the different parameterisations of the active learning process tested did result in a reduction in training time over the baseline algorithm, which was majoritively achieved by drastically limiting the number of training iterations and training dataset size, and hence in general also inflicted an accuracy drop (Table \ref{table: al-efficiency}). Despite this result, it is not concerning that total training time was not reduced, as active learning majoritively focuses on improving the data efficiency of training, not explicitly the energy efficiency. This process instead intends to improve the ESG impacts of DL by reducing resource requirements, data storage and labelling costs, and the energy consumption of large data centres.


    \subsection{Convergence over Training}

    To further quantify the efficiency of each training approach, the convergence of the training error towards the minimum of the loss function was evaluated. Figure \ref{fig: convergence} depicts the convergence rate of all tested training algorithms, showing the progression of the loss function evaluated over the network after each epoch. The plots show the prediction error output by the loss function firstly on input of the training set (used to adjust the DNN's parameters) and secondly on input of a smaller validation set (a collection of data instances held back from the training process) at each iteration of training.


    \begin{figure}[ht!]
        \label{fig: convergence}
        \centering
        \includegraphics[width=0.6\textwidth]{results/all-training-loss.png}
        \caption{\centering Convergence over each tested DNN training algorithm, shown as the progression of the output of the loss function (MAE) used at each epoch to determine the prediction error of the networkn (with a steeper gradient indicating a faster convergence rate).}
    \end{figure}


    When the convergence of the mixed-precision training process is compared to the baseline algorithm, a very similar trend can be seen; both exhibit closely aligned training and validation error curves, implying the models fit well to the training data, and both exhibit a significant decrease in MAE at the start of training with this tapering off in later epochs. The close similarities between both plots suggest that whilst introducing mixed-precision representations has not sped up the training process in terms of convergence per epoch, using these lower bit representations has not degraded the convergence rate of the prediction error towards the minimum of the loss function. Although, upon close inspection of the progression of the validation error over the mixed-precision training process, the curve appears to be slightly rougher than in the baseline case, exhibiting marginally larger fluctuations around the general trend line of convergence over the training process. This suggests that the training process may be slightly less stable when using mixed-precision representations, as the convergence rate is less smooth and uniform than that of the high-precision baseline approach.

    Comparison of the convergence of the two progressive training algorithms demonstrates the different approaches these methods take to developing an accurate model. Firstly, upon inspection of the axes, one can see that the pre-trained DNNs require significantly fewer epochs to optimise the full parameter set than the other training algorithms. The algorithm utilising supervised layer-wise pre-training only required $20$ epochs to facilitate lower training and validation error than the baseline model, with the unsupervised approach only requiring $50$ epochs. Additionally, the initial epochs of both of these algorithms are shown to exhibit a much faster convergence than in the baseline case, reducing both the training and validation error much more rapidly. The convergence plots show that both the supervised and unsupervised layer-wise pre-trained models perform the majority of their minimisation of prediction error within the first 10 epochs, whereas the same reduction is spread over the first 20 epochs in the baseline training process. For example, of the total reduction of validation error from $\sim \! 0.013$ to $\sim \! 0.003$ produced by the supervised approach, $90\%$ occurs within the first 10 epochs (decreasing the MAE to $\sim \! 0.004$). Similarly, $80\%$ of the total reduction in validation MAE ($\sim \! 0.02 \to \; \sim \! 0.005$) in the unsupervised case happens over the first 11 epochs of full network training ($\sim \! 0.02 \to \; \sim \! 0.008$). This demonstrates that the convergence rate of the supervised and unsupervised layer-wise pre-training approaches is greater than that of the baseline algorithm, indicating that progressive training is a significantly more efficient approach as it allows DNNs to learn much faster.

    Analysis of the convergence plot corresponding to the active learning training process similarly highlights that this approach is much more successful in inducing faster learning within a DNN than the baseline algorithm. This is indicated through the incredibly sharp decline in both training and validation error over the early stages of training, generating approximately $83\%$ of the total reduction in validation MAE over the first $8$ epochs. Furthermore, whilst in the baseline case the convergence rate slows considerably after the initial decline in MAE, significantly plateauing after the first $20$ epochs, during active learning the convergence rate stays higher for longer. Namely, a significant reduction in MAE is still observed up to approximately the 32nd epoch, with a plateau only starting to appear after this point. This suggests that the learning process during active learning is more efficient than that of the baseline algorithm, as the network's parameters are optimised more rapidly in the early stages of training.


    \subsection{Efficiency Breakdown of Progressive Training}

    To further analyse the benefit provided by progressive training, a detailed study was conducted into the time taken by the supervised and unsupervised layer-wise pre-training algorithms, breaking down how these approaches improve training efficiency. Table \ref{table: progressive-efficiency} enumerates the results of this study, showcasing the computational cost of the pre-training and full network tuning stages of both progressive training algorithms. These results demonstrate the different approaches taken by the progressive training algorithms, laying out how much time (and how many epochs) was spent on pre-training (broken down per layer) and tuning. 
    
    % progressive learning efficiency
    \begin{table}[ht]
        \centering
        \label{table: progressive-efficiency}
        \begin{tabular}{|l|l|l|l|l|l|l|l|l|} 
        \hline
        \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Pre-Training}\\\textbf{\footnotesize Method}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Pre-}\\\textbf{\footnotesize Training}\\\textbf{\footnotesize Time~(s)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\textbf{\footnotesize Pre-}}\\\textbf{\footnotesize Training}\\\textbf{\footnotesize Epochs}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Layer}\\\textbf{\footnotesize 1~(\%)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Layer}\\\textbf{\footnotesize 2~(\%)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Layer}\\\textbf{\footnotesize 3~(\%)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Tuning}\\\textbf{\footnotesize Time}\\\textbf{\footnotesize (s)}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Tuning}\\\textbf{\footnotesize Epochs}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Total}\\\textbf{\footnotesize Time}\\\textbf{\footnotesize (s)}\end{tabular}  \\ 
        \hline
        None                                                                             & -                                                                                             & -                                                                                                                    & -                                                                         & -                                                                        & -                                                                        & 198.60                                                                                  & 100                                                                      & 198.60                                                                                  \\ 
        \hline
        Supervised                                                                       & 107.89                                                                                        & 20                                                                                                                   & 32.6                                                                      & 29.0                                                                     & 38.4                                                                     & 47.066                                                                                  & 20                                                                       & 154.95                                                                                  \\ 
        \hline
        Unsupervised                                                                     & 43.769                                                                                        & 7                                                                                                                    & 27.4                                                                      & 47.8~                                                                    & 24.7                                                                     & 86.523                                                                                  & 50                                                                       & 130.29                                                                                  \\
        \hline
        \end{tabular}
        \caption{\centering The time taken by pre-training and full network tuning for both supervised and unsupervised layer-wise pre-training (including a breakdown of the time spent pre-training each of the 3 internal LSTM layers), compared to the total time taken by the baseline method.}
    \end{table}


    This study found that supervised layer-wise pre-training was most effective when 20 epochs were used to pre-train each layer, with another 20 epochs used to optimise the full network; in comparison, the unsupervised approach generated the best performance when employing only 7 epochs per pre-training round, then a final tuning round of 50 epochs. Thus, in total the supervised method used 60 low-cost epochs (training only a single layer) and 20 high-cost epochs, whereas the supervised method only employed 21 low-cost epochs but required 50 high-cost epochs. Table \ref{table: progressive-efficiency} illustrates this divergence by showing the proportion of the total training time spent pre-training; the supervised approach allocated $69.63\%$ to this stage, compared to only $33.59\%$ during the unsupervised algorithm. Therefore, whilst unsupervised layer-wise pre-training had a shorter total training time, supervised pre-training facilitated a greater reduction in the required number of full DNN tuning epochs to exceed the performance of the baseline method (exhibiting an $80\%$ decrease in tuning epochs, compared to a $50\%$ decreasing when using the unsupervised approach).

    Table \ref{table: progressive-efficiency} also breaks down how each pre-training round contributed to the overall computational cost of layer-wise pre-training. Of the total pre-training time, the supervised method was found to spend $32.6\%$ of its time training the first LSTM layer of the network, $29.0\%$ on the second layer, and $38.4\%$ on the final layer. Hence, the second hidden layer was faster to pre-train than the initial base model (suggesting the transfer of knowledge between these rounds), but the final LSTM layer was the most computational intensive of all (possible due to the higher-fidelity representations once the network deepens). A different trend was seen over unsupervised pre-training, where the second hidden layer was the most computationally intensive (using $32.6\%$ of the pre-training time), and the final LSTM layer was the fastest ($24.7\%$ of pre-training); this is likely due to the 2nd LSTM layer building up the majority of the representation of the inputs within the autoencoder, meaning the 3rd hidden layer was not required to add as much to the representation learning process (as it could build on the knowledge of its preceding autoencoder layer).

    These results indicate that the two progressive learning implementations reduce the computational cost of training differently. Due to the fewer high-cost epochs required to optimise the full DNN, the tuning round after supervised layer-wise pre-training was less energy intensive than when using the unsupervised method. However, the training time of the unsupervised layer-wise pre-training was in total lower, meaning that whilst more tuning epochs were required, this training approach overall was the most energy-efficient, as the cheap pre-training negated the majority of the computational cost of the subsequent tuning stage.


    \subsection{Data Efficiency of Active Learning}

    Once the energy efficiency of the baseline, mixed-precision, and progressive training algorithms had been thoroughly explored, the data-efficient training process of active learning became the focus of experimentation. Since this approach focusses on simultaneously minimising both the training dataset size and the accuracy drop inflicted by this constriction, active learning is an incredibly adaptable algorithm that can implement a spectrum of accuracy-efficiency compomises. To explore the various balances between accuracy and efficiency achivable, different parameterisations of active learning algorithms were experimented with, manipulating the hyperparameters controlling the number of training iterations and sample size $n_{sample}$. 

    \begin{table}[ht]
        \centering
        \label{table: al-efficiency}
        \begin{tabular}{|l|l|l|l|l|l|l|l|} 
        \hline
        \textbf{\footnotesize Method}                                                            & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Number of}\\\textbf{\footnotesize Iterations}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Sample}\\\textbf{\footnotesize Size}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Final Pool}\\\textbf{\footnotesize Size}\end{tabular} & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Utilisation of}\\\textbf{\footnotesize Full Training}\\\textbf{\footnotesize Dataset (\%)}\end{tabular} & \textbf{\footnotesize RMSE}    & \textbf{\footnotesize MAPE}     & \begin{tabular}[c]{@{}l@{}}\textbf{\footnotesize Training}\\\textbf{\footnotesize Time (s)}\end{tabular}  \\ 
        \hline
        Baseline                                                                   & -                                                                               & -                                                                      & 14600                                                                      & 100                                                                                              & 0.013830         & 14.662217         & 198.602                                                                       \\ 
        \hline
        \multirow{12}{*}{\begin{tabular}[c]{@{}l@{}}Active\\Learning\end{tabular}} & \multirow{3}{*}{\textit{200 }}                                                  & 70                                                                     & 14070                                                                      & 96.36                                                                                            & 0.01012          & 9.57100          & 480.603                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & 45                                                                     & 9045                                                                       & 61.95                                                                                            & 0.01492          & 10.47995          & 436.716                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & \textit{20}                                                            & \textit{4020}                                                              & \textit{27.53}                                                                                   & \textit{0.01863} & \textit{14.03773} & \textit{222.579}                                                              \\ 
        \cline{2-8}
                                                                                & \multirow{3}{*}{150}                                                            & 90                                                                     & 13590                                                                      & 93.08                                                                                            & 0.01404          & 12.25406          & 363.479                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & 55                                                                     & 8305                                                                       & 56.88                                                                                            & 0.01595          & 13.57484          & 326.668                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & 20                                                                     & 3020                                                                       & 20.68                                                                                            & 0.02579          & 25.93023          & 274.484                                                                       \\ 
        \cline{2-8}
                                                                                & \multirow{3}{*}{\textit{100 }}                                                  & 140                                                                    & 14140                                                                      & 96.85                                                                                            & 0.01740          & 15.03445          & 239.561                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & \textit{80}                                                            & \textit{8080}                                                              & \textit{55.34}                                                                                   & \textit{0.015639} & \textit{11.08715} & \textit{212.453}                                                              \\ 
        \cline{3-8}
                                                                                &                                                                                 & 20                                                                     & 2020                                                                       & 13.84                                                                                            & 0.03527          & 29.82608          & 175.556                                                                       \\ 
        \cline{2-8}
                                                                                & \multirow{3}{*}{50}                                                             & 250                                                                    & 12750                                                                      & 87.33                                                                                            & 0.01735          & 13.69113          & 176.087                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & 135                                                                    & 6885                                                                       & 47.16                                                                                            & 0.02580          & 22.39938          & 124.031                                                                       \\ 
        \cline{3-8}
                                                                                &                                                                                 & 20                                                                     & 1020                                                                       & 6.99                                                                                             & 0.03883          & 41.60770          & 89.6995                                                                       \\
        \hline
        \end{tabular}
        \caption{\centering Results of the experimentation into how varying the number of training iterations and sample size effect the data efficiency, resulting accuracy, and training time of an active learning based training algorithm (using GSx and GSy). N.B. notable results are emphasised in italics.}
    \end{table}


    Table \ref{table: al-efficiency} outlines the results of this study, enumerating the different hyperparameter values used and the resulting data efficiency, accuracy, and training time achieved by each active learning procedure. The results demonstrate the general trend that the greater the number of active learning iterations, and the more data added at each iteration, the higher the achieved model accuracy but the lower the data and time efficiency of the training process. Namely, the implementations that use many iterations and a high $n_{sample}$ produce the most accurate models, but utilise the majority of the full training dataset, and require a long training time. For example, the hyperparameters $(N_{iters}, n_{sample}) = (200, 70)$ facilitated a highly performant DNN with a test MAPE of $9.57100$ (a $34.72\%$ decrease from the baseline model), but this required using most of the training data ($96.36\%$ of the full dataset) and a considerably longer training time ($142.0\%$ increase over the baseline).

    However, by tuning these hyperparameters the ability of active learning to facilitate accurate performance whilst considerably reducing training data requirements was shown. Under the hyperparameters $(N_{iters}, n_{sample}) = (200, 20)$, this approach was able to train a DNN that surpassed the performance of the baseline model (reducing the test MAPE by $4.259\%$) using only $27.53\%$ of the amount of training data; this implementation required only $4020$ data instances to train the DNN superior accuracy, compared to the $14600$ instances required in by the baseline training algorithm. Furthermore, whilst this did inflict a longer training time than the baseline, an increase of only $12.07\%$ was observed, suggesting the direct computational cost was not significantly inflated. Another notable parameterisation $(N_{iters}, n_{sample}) = (100, 80)$ was able to produce a model with significantly improved MAPE ($24.38\%$ lower than the baseline) using only $55.34\%$ of the training data, and only inflicting a $6.974\%$ increase in training time. The impressive result solidified this parameterisation as the implementation used to compare the performance of active learning to the other implemented training algorithms in the preceding section. 

    Thus, this study demonstrates how a model developed through active learning can generate predictions with an accuracy commensurate with the baseline model whilst only requiring a small subset of the training dataset. This indicates that active learning is an effective approach for reducing training data requirements whilst maintaining accurate performance.


    \section{Extended Discussion}

    Given the aforementioned results outlining the accuracy and efficiency of the adapted training approaches (and how they compare to the baseline method), an evaluation can be made of the ability of each method in reducing the resource requirements of DL training. This evaluation aims to give a clear picture of the success of the application of Green AI to finance, exploring how the research questions have been answered by analysing the benefits of each method to expanding the applications of Green AI, reducing the environmental impact of Fintech, and improving the inclusivity of finance.


    \subsection{Mixed-Precision Training}

    The implemented mixed-precision training approach attempts to reduce the computational cost of developing DNNs by lowering the precision at which operations within a DNN are evaluated. Hence, this experimentation aimed to explore whether mixed-precision training was a viable method from Green AI for reducing the total time necessary to train an accurate volatility forecasting DNN, evaluating its utility for mitigating the ESG impacts of DL for finance.

    The experimentation found that whilst mixed-precision training did result in a higher accuracy model than the baseline, the training process took $18.98\%$ longer, suggesting this approach did not directly improve energy efficiency. However, a nearly identical convergence rate towards the minimum of the loss function over this training process was demonstrated, suggesting that reducing the memory requirements of DNN training by using smaller 16-bit representations does not negatively impact how well a model can be trained over the same dataset and number of training epochs. Namely, since the convergence rate is largely unchanged from the baseline it is evident that the lower precision variables have not made the training process unstable. 

    Since the mixed-precision implementation was not shown to facilitate a reduction in total training time, there is no evidence found in this research to suggest that mixed-precision representations are an effective method for improving the energy efficiency of training a DNN for financial market volatility forecasting. This is a disappointing conclusion, as it does not echo the results of reduced precision implementations in other fields that have boosted training speed, such as DoReFa-Net \citep{zhou-2016} which showed the benefit of low-bit training to CNN-based image classifiers. However, this is not that surprising, as DoReFa-Net inflicted a significant reduction in precision (using 1-bit parameters, 2-bit activations, and 6-bit gradients) to facilitate their efficiency improvement. This is not the approach taken within this research, as 16-bit representations were used to evaluate network operations, which still require significantly more memory than the approach of \citet{zhou-2016}, and thus cannot be expected to facilitate the same efficiency gains. Furthermore, the presented findings more closely resemble those of \citet{ott-2017}, who found that low-bit training was not ubiquitously effective for RNNs. \citet{ott-2017} asserted that using lower precision representations was not guaranteed to facilitate efficiency improvements in all applications of RNNs; hence, the results presented in this thesis may suggest that LSTM-based volatility forecasting may be one such application.

    The lack of efficiency benefit shown by mixed-precision training could possibly be additionally explained by the model architecture and hardware utilised. Namely, the implemented DNN (and training dataset) is relatively small compared to state-of-the-art implementations developed in cutting-edge DL research and the expensive systems typically used by large institutions. Due to the overhead of setting up DNNs and their training processes, smaller models such as this do not typically benefit from mixed-precision to the same extent as those with more complex architectures, as these overheads constitute a large portion of the training time. Furthermore, hardware accelerators (such as the GPUs relied upon in this study) do not provide as much benefit to smaller models, as the matrix multiplications are not as computationally intensive and so do not benefit as much from the increased parallelisation achievable when using low-precision representations. Therefore, whilst no training time reduction was witnessed in this research, this method may still hold promise in improving the energy efficiency of the higher complexity systems used by financial institutions. 

    Hence, this experimentation did not uncover any benefit of mixed-precision training in improving the energy efficiency of DL for finance and thus did not confirm the research hypothesis. However, one limitation of this research is that the memory consumption over DNN training was not explored; hence, this research does not quantify any reductions to memory usgae that may have been facilitated by mixed-precision representations, which could have revealed an improvement to energy efficiency. Thus, it is left to future studies to explore how the energy efficiency of DL for finance can be improved through minimising memory usage.


    \subsection{Progressive Training}

    On the contrary, the other studies implemented in this research demonstrated significantly more success in their efficiency improvements. As previously presented, both progressive training approaches using supervised and unsupervised layer-wise pre-training were able to significantly reduce total training time whilst producing a DNN with higher prediction accuracy. The supervised method improved the MAPE of the baseline model by $32.26\%$, whereas the unsupervised approach generated a $\sim \! 15\%$ decrease in MAE and RMSE and an impressive $39.95\%$ reduction in MAPE. These results indicate that the true volatility time series has been accurately represented by the predictions of the trained model, enabling accurate predictions. In both cases, the test MAE and RMSE are equally low, suggesting both models make predictions that neither exhibit a trend of consistently missing the true values $v_{t+1}$ (as there's a low MAE) nor output $\hat{v}_{t+1}$ values that are highly divergent from $v_{t+1}$ (shown through the low RMSE). Furthermore, a high convergence rate is exhibited over both progressive training processes, showing that the minimum of the loss function is approached fast and smoothly, demonstrating the stability and robustness of the training algorithms. Not only was an accurate model achieved in both cases, but the total training time was reduced significantly, completing $22.43\%$ faster than the baseline when using the supervised algorithm and $34.40\%$ faster in the unsupervised case. This indicates that supervised and unsupervised layer-wise pre-training are effective ways to reduce the training required to develop an accurate model and hence are successful in improving the energy efficiency of DL for finance as these shorter training times likely correspond to lower energy consumption.

    These results align with other research into progressive training completed further afield, which also highlights its ability to efficiently train accurate models. For instance, \citet{ienco-2019} found supervised layer-wise pre-training improved the generalisability of sequence classifiers, which is similarly shown in the high testing accuracy of the presented models. Additionally, \citet{xu-2018} and \citet{sagheer-2019} found unsupervised layer-wise pre-training could induce faster convergence whilst improving accuracy, both of which are demonstrated in the results of this research. Hence, this study supports the findings of other research into progressive training, extending the known utility of this Green AI method to the field of financial market volatility forecasting.

    Another core success shown within both the supervised and unsupervised pre-training processes was the ability to significantly reduce the number of epochs spent optimising the parameter set of the full network. Namely, the supervised approach reduced the required tuning rounds by $80\%$ to $20$ epochs and the supervised approach lowered it by $50\%$ to $50$ epochs. Since optimising the full 3-layer LSTM network is significantly more computationally intensive than optimising only a single LSTM layer (due to the higher number of parameters, calculations, and updates), this reduction in tuning likely facilitated a significant decrease in the computational cost of training, improving energy efficiency. This reduction was likely facilitated by the layer-wise pre-training stage inducing faster optimisation of the parameters, as the number of epochs in total over both the pre-training and tuning stages was still lower than in the baseline case. Thus, the low-cost pre-training epochs were shown to be effective at teaching the DNN how to make accurate predictions, allowing it to learn faster than possible in the traditional baseline algorithm. Furthermore, this efficiency benefit was likely facilitated by the transfer of useful information learned by each layer over pre-training to the subsequently added layer, allowing parameter updates to incorporate information already learned about the features of the input (as \citet{xu-2021} posited). This is shown through the fact that (in general) later layers in pre-training made up a lower proportion of the total pre-training time. During supervised pre-training, $29.0\%$ of the total time was allocated to training the second LSTM layer, compared to $32.6\%$ spent on the first. Similarly, the unsupervised approach expended $27.4\%$ of pre-training on the first hidden layer and only $24.7\%$ on the last. Thus, these results demonstrate that supervised and unsupervised layer-wise pre-training facilitate a more intelligent and efficient learning process. 

    One limitation of layer-wise pre-training is the extra computational cost of fixing the parameters of the previously pre-trained layers such that they are no longer optimised in subsequent pre-training rounds. When profiling the progressive training algorithms, this cost was included in the pre-training time recorded per layer; thus, the total time calculated is not solely attributed to model training. This means there is a slight divergence between what is included in the total training time statistic between the progressive training algorithms and the baseline. Additionally, more time is spent on auxiliary tasks during each subsequent pre-training round than when training the initial base network, as the first round does not fix any parameters. This could possibly contribute to why some hidden layers added during pre-training do not exhibit an increased training speed from the layer preceding it. However, these must be included in the total times attributed to progressive training to ensure a fair comparison to the baseline method which does not require these auxiliary tasks.

    An interesting difference between the supervised and unsupervised approaches is that the supervised method allocated $69.63\%$ of the total training time to pre-training, compared to only $33.59\%$ during the unsupervised algorithm (which spent more time tuning). This distinction is likely due to the supervised algorithm training the DNN to forecast volatility from the start, whereas the unsupervised approach pre-trained the DNN as an autoencoder. Time series forecasting is known to be a challenging task, but this is not so much the case with representation learning, as training an autoencoder to solely minimise the reconstruction error between inputs and outputs is significantly simpler than forecasting future values. Thus, more epochs were required to produce an accurate DNN over supervised pre-training than in the unsupervised case. However, later stages of unsupervised layer-wise pre-training required the developed autoencoder to be converted into a forecasting model, hence requiring more full network tuning epochs. 

    The implemented supervised and unsupervised layer-wise pre-training approaches were also able to reduce the total number of epochs over both pre-training and tuning; the supervised approach used a total of $20 * 3 + 20 = 80$ epochs, and the unsupervised approach used $7 * 3 + 50 = 71$. \citet{schwartz-2019} showed that a significant part of the cost of training DNNs can be attributed to employing the DNN over all instances in a vast dataset at every training iteration; thus, reducing the number of times the full dataset is evaluated during training (by lowering the number of epochs) is likely to significantly decrease computational costs. Therefore, the $20\%$ and $29\%$ fewer epochs used in total over the full training dataset when employing supervised and unsupervised layer-wise pre-training suggests the computational cost of these training algorithms was lower than that of the baseline case. Furthermore, since many of these epochs were low-cost single-layer optimisations, the computational savings were likely even greater. 

    The observed lower training times, fewer epochs, and computational cost savings indicate that both supervised and unsupervised layer-wise pre-training algorithms improve the energy efficiency of training a DNN in the chosen context of financial volatility forecasting. Whilst the observed reductions in training time have been on the scale of seconds in this research, when the model architecture and training length are scaled up to be comparable to the large, energy-intensive DL systems utilised in the upper echelons of computational finance research and by large financial institutions, these savings will likely also scale, allowing more significant reductions in training time (in the scale of hours and days) and carbon emissions. Thus, these findings suggest that progressive training is an effective Green AI method for improving the energy efficiency of DL for finance, thus proving a possible route through which the carbon emissions of Fintech can be reduced, aligning Fintech more closely with the goals of sustainable finance. This adequately fills the research gap by providing a promising method for mitigating the environmental impact of training DL systems in financial applications, hence facilitating the use of DL in Fintech (and within sustainable finance itself) without undermining any SDGs. Moreover, by reducing the computational cost of training DNNs, this research has additionally improved the inclusivity of DL for finance, as shorter training times and smaller energetic costs lower the resource requirements of developing DNNs, reducing the expense of implementing such a system, and hence allowing more financial players and researchers to reap the benefits of DL.


    \subsection{Active Learning}

    As discussed by \citet{bender-2021} and \citet{walsh-2021}, decreasing the training time of DNNs is not the only route through which the ESG impacts of DL can be mitigated, as a significant portion of the computational cost of high-performance DNNs can be attributed to their inefficient use of data. Thus, this research also explores the use of Green AI methods that improve the data efficiency of DNN training. These methods aim to select data instances more intelligently, reducing the training dataset size necessary to develop accurate DNNs and thus minimising the memory requirements of DL training and the energetic cost of the data centres. This research explored how active learning can facilitate such gains, as this training method has been shown to drastically improve training efficiency in other modelling domains, whilst maintaining accuracy \citep{ren-2021}.

    The completed study demonstrated the spectrum of efficiency-accuracy compromises achievable by active learning. These ranged from the parameterisation $(N_{iters}, n_{sample}) = (200, 20)$ that maintained a comparable test MAPE to the baseline model using only $27.53\%$ of the total training dataset required by the baseline training algorithm, to the $24.38\%$ improvement to MAPE produced using only $55.34\%$ of the training data under the parameterisation $(100, 80)$ (Table \ref{table: al-efficiency}). This shows that active learning is an effective approach for significantly reducing the amount of training data required to develop accurate DNNs in the chosen context, effectively lining up with the research further afield, such as the survey of \citet{ren-2021} showcasing the ability of active learning to deal with high-dimensional data in CV, and the results of \citet{peng-2017} and \citet{zimmer-2018} who both demonstrated the feasibility of this training method for reducing dataset sizes in other time series modelling domains. Hence, the presented research has built upon this preceding work to demonstrate that active learning is effective at improving the data efficiency of DNNs used for financial volatility forecasting, suggesting that this is a useful method for reducing training dataset sizes across the field of DL for finance.

    Since the presented findings demonstrate that active learning managed to reduce the training data used within each epoch, it is likely the total computational cost of this training process was also reduced (although this is not explicitly shown in the training time analysis). One method of approximating the computational cost of each epoch is by estimating the number of parameter optimisation iterations (i.e. applications of gradient descent over the full parameter set) per epoch. If we have a training dataset of $N$ instances and utilise a batch size $b$, there will be a total of $\frac{N}{b}$ full parameter optimisation iterations at each epoch, as the DNN's parameter set is iteratively updated over the full training dataset one batch at a time. In the baseline case, there are $\frac{14600}{32} = 456.25$ parameter optimisation iterations per epoch; note, since the batch size doesn't always perfectly divide the training dataset, the number of optimisation iterations are typically rounded down (to $456$ in this case). If this cost is to be computed for an active learning approach, the equivalent figure for training dataset size must be approximated as the average pool set size over the entirety of training. For the parameterisation $(N_{iters}, n_{sample}) = (100, 80)$ we begin with a seed pool of $80$ instances, and finish training with a pool set of size $8080$ (Table \ref{table: al-efficiency}); thus, the average training pool size over this 100 iteration training process is $N_{pool} = 4080$. Therefore, the active learning algorithm on average employs $\frac{4080}{32} \approx 127$ parameter optimisation iterations per epoch. This means that the smaller average (and total) training dataset employed over the active learning training process has to calculate significantly fewer training prediction errors and parameter updates; hence, the computational cost of updating the parameter set of the DNN at each epoch (and in total) is reduced in comparison to the baseline.

    \citet{ren-2021} assert that this reduction in training data is facilitated through more intelligent data selection. The findings of this research support this hypothesis, suggesting that active learning more intelligently constructs a training dataset even over the complex, multivariate time series data used in this application. This is demonstrated by the associated convergence plot, which shows active learning exhibits a higher convergence rate, maintained over a greater number of epochs; hence, this approach more rapidly optimises the parameter set, approaching the minimum of the loss function and optimal DNN parameterisation at a faster rate over the initial stages of training. The improved convergence is likely due to selected data allowing the DNN to learn more about its modelling task during each epoch, facilitating a faster and more efficient learning process. This indicates that the utilised GSx and GSy algorithms have successfully constructed a training pool that effectively represents the input and output spaces of the model, allowing it to learn more generalisable performance early on. Furthermore, this improved generalisability is also demonstrated in the convergence plot, as active learning is the only process that exhibits a lower validation MAE than training MAE for a significant portion of training, suggesting the DNN is generalising well to produce accurate predictions over the unseen validation dataset.

    This research into active learning aimed to highlight how this Green AI method could be used to decrease the size of the training dataset required to develop an accurate DNN for financial volatility forecasting. This aim has been achieved, demonstrating how active learning can be used to produce equivalent---and even superior performance---to a traditional training approach whilst significantly reducing the training dataset size. By reducing data requirements, another possible avenue for reducing the ESG impacts of DL for finance has been highlighted. Whilst not attempting to directly reduce the energy consumption of training by minimising training time (as is the case with progressive training), active learning does facilitate a reduction in the energetic cost of DL indirectly, mitigating environmental concerns and improving the inclusivity of this field. This is because reducing dataset size lowers the memory consumption of a system, reducing the resource requirements of training (as evaluating a DNN over a smaller dataset is less computationally intensive) as well as decreasing data storage and labelling costs. Furthermore, the datasets employed in state-of-the-art DL systems and industrial applications are often multiple orders of magnitude larger than that used in this research; hence, the energy and carbon savings generated if the successes of this research are applied to higher complexity models will likely be even more impressive. Additionally, this approach has the potential to reduce the energy consumption (and hence carbon emissions) of large data centres by removing the requirement to store expansive datasets, lowering the cost of transferring these datasets between cloud computing servers, and lowering the computational cost of the DNN training processes that use cloud computing. This also lowers the bar-to-entry to training DL systems as developers do not need to gather or store vast collections of data to produce accurate models. Thus, the contribution of this research into data efficiency to the field of computational finance and the financial industry is twofold; firstly, an approach for decreasing the environmental cost of DL for finance has been presented, and secondly, the financial cost of high-performance DL systems has been reduced, improving the inclusivity of Fintech. Furthermore, the successful application of active learning to the new domain of financial market volatility forecasting has expanded the known space of useful applications of Green AI, further demonstrating the utility of these methods, and providing a new avenue Green AI research can take focusing on improving the sustainability of DL in industry.


    % --------------------  CONCLUSIONS ----------------------
    \newpage
    \chapter{Conclusions}
    \label{chapter: conclusion}

    \section{Summary \& Contributions}

    In summary, this research explores the importance of ensuring the energy and data efficiency of DL systems used within the financial sector. Since significant energy consumption can be attributed to DL, and the data centres DNNs rely upon, the expanding use of these tools in Fintech and beyond raises concerns about inflating the already concerning carbon emissions attributable to this domain. To address analogous concerns within the research fields of CV, NLP, and mobile computing, increasing attention has been given to Green AI, which uses energy and data-efficient methods to reduce the ESG impacts of DL. Hence, in an attempt to mitigate the environmental (and social) concerns associated with Fintech's reliance upon DL, and ensure DNNs can be used within the financial sector without compromising the SDGs of sustainable finance, this research has explored the use of Green AI within the field of DL for finance.

    To evaluate the utility of Green AI, this research explored whether Green AI methods are applicable for improving the ESG impacts of industrial applications such as finance. This was achieved by developing a financial market volatility forecasting model that exploited a deep LSTM architecture---as this is a common application of DL for finance---which exploited energy and data-efficient training processes to reduce the computational cost of developing such a model. Mixed-precision training, progressive training, and active learning were implemented and evaluated, determining the success of each method along this aim. The experimentation conducted found that progressive training was the most effective method for improving the energy efficiency of DL training, generating a model with higher performance over a shorter training period. Active learning was also shown to be incredibly beneficial to improving the data efficiency of such a training process, minimising the resource requirements of training such a system.

    Based on these results, the contribution of this research is threefold. Firstly, applying Green AI to this new domain has expanded the known applications of these energy and data-efficient methods, demonstrating the potential of progressive training and active learning to reduce the energy and carbon cost of DL in industrial applications. This proves that the benefits of Green AI are not only possible in state-of-the-art research domains like CV and NLP, or for low-resource systems such as mobile applications, and instead can be exploited to improve the sustainability of DL systems further afield. Hence, a new, impactful avenue of Green AI research has been created, exploring how such methods can be used to mitigate the ESG cost of DNNs employed in industry. Secondly, addressing the energy and data efficiency of a DNN-based volatility model proves that typically used DL systems in finance can conduct highly accurate modelling at a low computational cost. This aligns Fintech closer to the SDGs of sustainable finance, facilitating a potential approach for reducing the carbon emissions of finance by introducing a new approach towards sustainable finance, namely \emph{sustainable DL for sustainable finance}. Thirdly, lowering the resource requirements and computational cost of employing performant DNNs in financial applications improves the social inclusivity of this field, lowing the bar-to-entry to engaging in computational finance research and to using DL systems to inform (and improve) the behaviour of individual financial market players.


    \section{Limitations \& Further Work}

    The research presented within this thesis does exhibit several limitations; most notably, the majority of the efficiency analyses heavily rely on evaluating the total training time of each training approach. Whilst this is an effective metric for enumerating the general computational cost of an algorithm, it is not without its flaws \citep{schwartz-2019}. Hence, to give a more holistic analysis of the utility of the implemented methods, a wider host of efficiency metrics could have been used. Another limitation of this research is its use of a smaller model architecture, dataset, and training length (and inferior computer hardware) than other high-performance DNNs used to generate state-of-the-art performance---which are responsible for the bulk of the ESG concerns related to DL. Therefore, this research has not directly quantified the computational cost savings possible by employing Green AI methods on these larger systems that are more prevalent within the financial industry. Hence, it is unknown how exactly the energy savings demonstrated in this research will scale when Green AI is employed in higher complexity systems. 

    However, these limitations do not restrict the findings of this research, as it is likely the improvements to energy and data efficiency observed are in general feasibly scalable to larger DL systems. Namely, progressive training could be successfully used to improve the energy efficiency of training higher complexity DNNs employed by large financial institutions and mitigate their ESG costs. Additionally, the benefits to data efficiency provided by active learning could be used to reduce the storage, communication, and computing costs of the large servers employed by data centres and financial institutions.

    Based on these limitations, future work in this field could improve the scope and impact of the presented research by further analysing the implemented Green AI algorithms using a wider spectrum of efficiency metrics to give a more detailed and comprehensive overview of their computational cost. For example, the memory consumption of each approach should be inspected, highlighting further achievements of methods such as mixed-precision training that aim to reduce memory costs. Explicitly quantifying the carbon emissions produced when developing financial models and the reduction in emissions generated by each efficient training process would also be beneficial, possibly using the machine learning emissions calculator of \citet{lacoste-2019}. Additionally, since this research only implemented distinct experiments for each Green AI method, evaluating the success of each algorithm in isolation, further benefits could be gained from exploring how these methods could be combined to produce a more cohesive energy and data-efficient DNN training algorithm; for example, a comprehensive system could employ progressive training but use active learning to optimise each hidden layer during pre-training and the full network during tuning.

    Furthermore, this research solely focused on how the efficiency of training DNNs could be improved; however, both \citet{bender-2021} and \citet{xu-2021} assert that a significant proportion of the ESG impacts of DL systems are inflicted after these models have been deployed, not during training. This aspect has also been explored by several Green AI researchers further afield, such as the work of \citet{lacoste-2019} and \citet{cai-2022}, whose methods aim to improve the energy and data efficiency of DNNs throughout the entirety of their lifetime; for example, \emph{low-rank factorisation} \citep{xu-2021} has been shown to effectively reduce the size of DNN architectures, reducing the computational cost of each computation made by the network. Hence, further work could exploit such methods to explore how the ESG impact of DL for finance could be reduced over the entirety of a model's lifetime. This would further the success of this research in providing a new avenue for mitigating the environmental and social concerns of DL for finance.


    % --------------------  BIBLIOGRAPHY ---------------------
    \newpage
    \footnotesize
    \bibliography{bibliography}

\end{document}