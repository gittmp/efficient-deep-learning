% ------------------ DOCUMENT SETUP / PACKAGES ------------------ 
\documentclass[a4paper, 11pt]{report}
\usepackage[a4 paper, top=25mm, bottom=25mm, left=25mm, right=25mm]{geometry}
% \usepackage{times}

\usepackage{float}
\usepackage{color}
\usepackage{amsmath}
\usepackage{emptypage}
\usepackage{array}

\usepackage{graphicx}
\graphicspath{{./resources/}} 

\usepackage{setspace}
\onehalfspacing

% Manages hyperlinks 
\usepackage[colorlinks=true, linkcolor=black, urlcolor=red]{hyperref}

% Bibliography package
\usepackage[round]{natbib}
\bibliographystyle{plainnat}

% Line Break Properties
% \tolerance=1
\emergencystretch=\maxdimen
% \hyphenpenalty=10000
\hbadness=10000


% Title page information
\title{Energy-Efficient Deep Learning for Finance}
\author{Tom Maxwell Potter}
\date{\today}


% ---------------------  DOCUMENT ----------------------
\begin{document}

    % ------------------  TITLE PAGE -------------------
    \begin{titlepage}
        \begin{center}
            % UCL Image
            \vspace*{1cm}
            \makebox[\textwidth]{\includegraphics[width=.5\paperwidth]{resources/UCL_LOGO.png}}
            
            \vfill
            
            % Title
            \makeatletter
            {\Huge\textbf{\@title}}

            \vspace{0.8cm}
            by
            \vspace{0.8cm}

            % Author
            {\Large\textbf{\@author}}

            % Date
            \vspace{1.5cm}
            {\textbf{\\\@date}}

            \vfill

            {A dissertation submitted in part fulfilment\\
            of the requirements for the degree of\\}
            {\setstretch{2.0}
            \textbf{Master of Science}\\
            of\\
            \textbf{University College London\\}}
            \vspace{1cm}
            {Scientific and Data Intensive Computing\\
            Department of Physics and Astronomy}

            \vspace{2cm}
        \end{center}
    \end{titlepage}


    % -----------------  DECLARATION  -------------------
    \pagenumbering{roman}
    \chapter*{Declaration}
    \addcontentsline{toc}{chapter}{Declaration}
    
    I, Tom Maxwell Potter, confirm that the work presented in this thesis is my own. Where information has been derived from other sources, I confirm that this has been indicated in the dissertation.


    % ----------------------  ABSTRACT -----------------------
    \newpage
    \addcontentsline{toc}{chapter}{Abstract}

    \begin{abstract}

        This thesis investigates the use of energy-efficient methods for deep learning-based financial volatility forecasting, aiming to reduce the energetic cost of such models and demonstrate how the sustainability of deep learning for finance can be improved.

        \textbf{Context/background:} The financial sector has long been associated with largely negative environmental, social, and governance (ESG) impacts, including being a major contributor to global carbon emissions. Despite the attempts by some to prioritise \emph{sustainable finance}, the recent expansion of financial technology---incorporating new, expensive methods such as \emph{deep learning} (DL)---has only worsened the energy consumption attributed to this industry, accelerating its carbon emissions.
        
        \textbf{Aims:} In an attempt to address these negative impacts of financial technology, this project aims to develop an energy-efficient DL system for financial modelling. This research will explore \emph{Green AI} methods that attempt to reduce the energy expended training DL models and apply these for the first time to models used in finance. To exemplify the benefits of these methods, a performant financial volatility model will be developed that not only produces accurate results but prioritises generating this performance in an efficient manner, minimising the energy and data resources required during training. This system aims to demonstrate that the principles of Green AI are applicable within the financial sector, furthering the scope of sustainable finance by improving the sustainability of deep learning for finance and, hence, minimising the ESG impacts of the financial sector.
        
        \textbf{Method:} This research will commence with an analysis of the resource requirements of typical systems in the field of deep learning for finance. A particular focus will be given to the domain of financial volatility modelling, as this is a major application of deep learning in finance, and the \emph{long short-term memory} (LSTM) networks typically exploited for such tasks. Energy and data-efficient training methods will be explored, developing a deep model that consumes less energy and requires less data to train, but maintains accurate performance. Specifically, methods such as \emph{active learning}, \emph{progressive training}, and \emph{mixed-precision} will be explored that reduce the resource requirements of training, proving the feasibility of efficient models within this field.
        \\ \\ 
        \textbf{Contributions to science:} 
        \begin{enumerate}
            \item \emph{Expanding the applications of Green AI}. The first application of Green AI to the finance sector, further demonstrating the utility and importance of Green AI in lowering the environmental cost of deep learning. 

            \item \emph{Reducing the environmental impact of financial technology}. The improvement of sustainable finance to include the new research domain of \emph{sustainable deep learning for sustainable finance}. 

            \item \emph{Improving the inclusivity of finance}. Lowering the bar-to-entry to engage in deep learning for finance,  allowing more individuals to leverage financial technology and analytics.
        \end{enumerate}

        \textbf{\\Outline of research:} 
        \begin{itemize}
            \item \underline{Chapter \ref{section: baseline}: \emph{Baseline finanical volatility model}}. An initial deep model will be implemented, using a traditional training process and LSTM architecture, to act as an exemplar of the resource requirements of this domain.

            \item \underline{Chapter \ref{section: energy-extensions}: \emph{Energy-efficient training extensions}}. Several adaptations to the model training process will be made that prioritise reducing the energy consumed by the system.
            
            \item \underline{Chapter \ref{section: data-extensions}: \emph{Data-efficient training extensions}}. Additional adaptations will be made that reduce the necessary amount of training data, further lowering resource requirements.
            
            \item \underline{Chapter \ref{chapter: evaluation}: \emph{Discussion \& evaluation}}. An analysis will be made between the baseline and extended models, comparing the performance and efficiency of each, and discussing their success in reducing resource requirements.
        \end{itemize}
        
        \textbf{\\ \\Keywords:} Green AI, Green Deep Learning, Energy Efficiency, Data Efficiency, Sustainable Finance, Financial Technology, Financial Volatility Modelling, Long Short-Term Memory

    \end{abstract}


    % -----------------  ACKNOWLEDGEMENTS  -------------------
    \newpage
    \chapter*{Acknowledgements}
    \addcontentsline{toc}{chapter}{Acknowledgements}


    % -----------------  TABLE OF CONTENTS -------------------
    \newpage
    \tableofcontents


    % -------------------  LIST OF FIGURES --------------------
    \newpage 
    \listoffigures
    \addcontentsline{toc}{chapter}{List of Figures}


    % -------------------  LIST OF TABLES ---------------------
    \newpage
    \listoftables 
    \addcontentsline{toc}{chapter}{List of Tables}




    % --------------------  INTRODUCTION ----------------------
    \newpage
    \pagenumbering{arabic}
    \chapter{Introduction}
    \label{chapter: intro}

    \section{Topic \& Background}
    \label{section: topic}

    Many industries have recently been under increased pressure to monitor and rectify their environmental impact. This pressure is typically directed towards the perceived high carbon industries that constitute the major pollutant sectors of the economy, such as transport, energy supply, and agriculture. For example, recent estimates suggest that of the $33.5$ billion tons of carbon dioxide emissions generated globally in 2018, $8$ billion tons could be attributed to the transport sector \citep{iea-2022}, and $6$ billion tons to farming and livestock \citep{ahmad-2022}. These concerning figures have rightly sparked increased international discussion surrounding global carbon emissions and sustainability, such as the 2021 \emph{United Nations Climate Change Conference} (COP26).


    \subsection{Sustainable Finance}

    The finance sector has long been closely associated with sustainability concerns such as those discussed at COP26, being a major contributor to global carbon emissions both directly and indirectly. The most visible environmental impact of the financial industry is its direct emissions from business practices such as the distribution of cash through the economy (e.g. cash transport and ATM power consumption), card payment processing centres, and everyday operational costs such as heating office buildings \citep{hanegraaf-2018}. However, indirect emissions---attributable to services such as investing and lending---have been estimated to contribute over 700 times more to the carbon footprint of the financial industry than all direct emissions \citep{power-2020}. This form of carbon emissions, entitled \emph{financed emissions} by \citet{power-2020}, includes practices such as financing fossil fuel companies---who have received \$3.8 trillion in funding from global banks since the \emph{Paris Agreement} was signed in 2016 \citep{rainforest-2021}. In their survey of $700$ global financial institutions, \citet{power-2020} estimated that the production of over $1.04$ billion tons of carbon dioxide was attributable to financed emissions in 2020 (approximately $3.1\%$ of global emissions). However, they note this figure is likely to significantly understate the total global financed emissions, as of the $700$ contacted institutions only $332$ responded, and only $25\%$ of those reported financed emissions (typically on less than $50\%$ of their portfolios). Furthermore, a recent report by \emph{Greenpeace} and the \emph{WWF} concluded that the combined carbon emissions of the largest banks and investors in the UK in 2019 totalled $805$ million tons, which (if consolidated into its own country) would rank 9th in the global list of total emissions per country (Greenpeace, 2021). This figure is $1.8$ times higher than the total emissions of the UK ($455$ million tons), and almost $90\%$ of the global emissions from commercial aviation ($918$ million tons) in the same year \citep{graver-2020}.

    The increasing awareness of the negative \emph{environmental, social, and governance} (ESG) impacts of the finance industry, highlighted by studies such as those of \citet{power-2020} and \citet{greenpeace-2021}, has led researchers to investigate methods that prioritise the \emph{sustainable development goals} (SDGs) within the financial sector. Towards this objective, the field of \emph{sustainable finance} has emerged, which aims to consider ESG impacts and SDGs in financial decisions (such as investment and lending activities) to improve the sustainability of finance. Namely, despite the lack of a rigorous consensus on what constitutes sustainable finance, recent reviews---such as those of \citet{cunha-2021} and \citet{kumar-2022}---typically use the term to refer to research into financial activities, resources, and investments that prioritise long-term sustainability. In particular, a focus is given to those practices that produce a measurable positive improvement to the social and environmental impact of the financial industry, global economy, and wider society. 

    This discussion around sustainable finance largely began with \citeauthor{ferris-1986}'s examination of the benefits of investing pension funds in a socially responsible way. Following this, early research mainly focussed on \emph{socially responsible investing}, where investments are made that not only prioritise profits but further current positive social movements and mitigate societal concerns \citep{cunha-2021}. During the 2000s, research began to exhibit a new focus on environmental sustainability, considering factors such as climate change and renewable energy \citep{laan-2004}. Later research further pushed the scope and impact of environmentally-focused practices with the development of new domains such as \emph{climate finance} \citep{hogarth-2012}, where the mitigation of climate change is prioritised through investment and financing, and \emph{carbon finance} \citep{aglietta-2015}, which focusses on investments that seek to lower or offset carbon emissions. Recent research within sustainable finance has pushed this environmental focus further, aiming to put into practice the sustainability goals set out by the Paris Agreement, ESG factors, and SDGs. Specifically, a new interest has been taken in sustainable investment fields such as \emph{impact investing} \citep{agrawal-2021} and \emph{ESG investing} \citep{alessandrini-2020}, where investments are made that produce measurable improvements to environmental issues (according to criteria such as their ESG impacts). This recent focus has significantly increased the prominence and influence of sustainable finance. In their review of $936$ research papers, \citet{kumar-2022} found that almost $70\%$ of sustainable finance research had been published between 2015--2020, and an exponential trend was exhibited in the increase in papers being published each year; additionally, they found that the top three most cited papers all conducted research in the field of impact investing. Furthermore, \citeauthor{kumar-2022} assert that in 2020, \$400 billion of new sustainability funds were raised on capital markets. Hence, it is clear the scope and impact of sustainable finance is currently on the rise, predominantly driven by a renewed focus on the ESG impacts of financial practices, resources, and investments.


    \subsection{Financial Technology and the Issues with Sustainable Finance}

    Whilst the \$400 billion raised in sustainability funds seems impressive, in the same year the total US equity market value was over \$40 trillion \citep{siblis-2022}, meaning globally only $0.98\%$ of the value of the US equity market alone was raised. Furthermore, recent research has uncovered the prevalence of investment \emph{greenwashing} \citep{popescu-2021}, where institutions misleadingly classify their practices and investments as sustainable without credible data to back up their claims (and often excluding data that would suggest the opposite). In their review, \citet{cunha-2021} raised similar concerns, asserting that research into sustainable finance is currently ``excessively fragmented". These issues indicate that whilst attention is growing around the sustainability of financial practices, this domain is still not widely recognised, and further work and research are still necessary to increase the adoption of sustainable methods and tools within finance. 

    An additional concern is that the tools used to conduct financial practices are becoming increasingly resource-hungry at a pace exceeding the adoption rate of sustainable finance. A clear exemple of this is the increased adoption of technology throughout the finance industry. Recently, a surge of developments in financial technology (\emph{Fintech}) has revolutionalised the methods and practices used across the field of finance, from the large financial institutions and increasing number of Fintech startups, to groups of academic researchers. This Fintech revolution has transformed many aspects of finance, promising to enhance and automate existing financial services, and deliver new, innovative financial products. In their exploration of the evolution of Fintech, \citet{palmie-2020} assert that this adoption emerged in three waves. They suggest that it began with the utilisation of electronic payments and online banking, digitalising the world of finance; the second wave then came with the emergence of blockchain technology and cryptocurrencies, which further disrupted the way currency is stored and transacted. The third and most recent Fintech wave, \citet{palmie-2020} claim, is the current upwards trend in financial institutions' reliance upon \emph{artificial intelligence} (AI). Driven by the promise of increased automation and computing power, the utilisation of AI within the financial sector has been rapidly expanding over recent years, becoming a core component of many of the financial products and services used today: from accurate real-time financial fraud detection \citep{sadgali-2019} to automated analysis of financial statements \citep{amel-2020}.


    \subsection{Deep Learning for Finance}

    Because of their power and potential, AI methods have been used to produce state-of-the-art results over a plethora of scientific problems and research fields: from DeepMind's \emph{AlphaFold} \citep{jumper-2021}---the first programmatic solution to the age-old protein folding problem in Biology---to Google's \emph{PaLM} \citep{chowdhery-2022}---a cutting-edge human language model that delivers breakthrough results in multi-step arithmetic and common-sense reasoning (a major step towards \emph{artificial general intelligence}). This research typically revolves around the use of \emph{machine learning} (ML), where large collections of data are used to train computational models how to perform certain tasks independently \citep{samuel-1959}. Recent innovations in ML have increasingly taken advantage of \emph{deep learning} (DL) methods, which use large, complex models to produce state-of-the-art performance \citep{witten-2017}.

    The recent success in utilising DL---such as that of AlphaFold and PaLM---has driven an increased adoption of AI and DL further afield, such as within the finance industry. In fact, global spending on AI is predicted to double in value by 2024, from \$50 billion in 2020 to an estimated \$110 billion \citep{oecd-2021}. Specifically, the global AI Fintech market was estimated as being worth \$7.91 billion in 2020 and is forecast to grow to \$27 billion by 2026 \citep{mordor-2021}. Furthermore, in a survey of 206 executives from US financial service companies, \citet{gokhale-2019} found that $70\%$ used ML within their financial institutions for practices such as detecting irregular patterns in transitions, and building advanced credit models. In a similar survey, \citet{chartis-2019} found ML to be a core component in current financial technology, revolutionising data processing and modelling practices.

    In their survey of financial professionals, \citet{chartis-2019} discovered that $44\%$ of respondents cited ``greater accuracy of process and analysis" as a key motivation behind their adoption of AI methods. This superior accuracy provided by ML and DL methods---publicised through models like PaLM pushing the boundaries of computational accuracy---provides a compelling alternative to traditional statistical models. Hence, the promise of increased accuracy of performance is a major driving factor behind recent ML adoption within Fintech. For example, recent DL-based financial systems have been shown to predict borrower defaults with greater accuracy than achievable with traditional approaches \citep{albanesi-2019}.


    \subsection{The Issues with Deep Learning}

    Whilst cutting-edge DL models push the boundaries of computational accuracy, few of these systems prioritise the efficient use of energy and data. This has led to DL inflicting a great cost upon the environment, as the energy-intensive algorithms, long training phases, and power-hungry data centres they utilise inflict a high carbon footprint \citep{lacoste-2019}. \citet{schwartz-2019} label these accurate but energy-intensive DL models as \emph{Red AI}, which they define as ``research that seeks to improve accuracy through the use of massive computational power while disregarding the cost". \citeauthor{schwartz-2019} explain how these systems generate their performance gains majoritively through the use of extensive computational resources, such as complex models with vast parameter sets, large collections of data, and power-hungry computer hardware. \citet{bender-2021} illustrate this trend through the progression of recent language models: whilst the 2019's state-of-the-art model \emph{BERT} \citep{devlin-2018} used $340$ million parameters and a $16$GB dataset, the leading models of 2020 (\emph{GPT-3} by \citet{brown-2020}) and 2021 (\emph{Switch-C} by \citet{fedus-2021}) utilised $175$ billion and $1.57$ trillion parameters respectively, and data sets of size $570$GB and $745$GB. In fact, between 2012 and 2018 the computational resources used to train cutting edge models increased by a factor of $300,000$, outpacing \emph{Moore's Law} \citep{amodei-2021}.

    The intense computational load of these DL models does not come for free; the large parameter and data sets mean training, storing, and computing with these models draws a significant amount of energy---referred to by \citet{bietti-2019} as \emph{data waste}. Partly due to this inefficiency, the data centres at which DL models rely upon for storage and cloud computing become a significant hidden contributor to carbon emissions \citep{aljarrah-2015}. Studies such as \citet{masanet-2020} and \citet{malmodin-2018}, 2020 have estimated that processing at these data centres consumes around $200--250$TWh of electricity a year, with the cost of data transmission exceeding this at $260--340$TWh per year \citep{iea-2022}. These estimates suggest that global data centres use more electricity than the majority of countries in the world, ranking above both Australia and Spain \citep{eia-2019}, and account for around $1\%$ of global electricity consumption (rising to $2.4\%$ when including transmission costs). Furthermore, this energy is likely not entirely carbon-neutral; \citet{cook-2017} showed that of their total electricity demand, \emph{Amazon Web Services} only powered $12\%$ through renewable sources, and \emph{Google Cloud} $56\%$ (with the latter figure ranging between $4\%$ and $94\%$ depending on location). These figures are also somewhat unrepresentative of the true carbon emissions of such companies and facilities, as they report net emissions including carbon offsetting measures such as purchasing carbon credits, which \citet{schwartz-2019} argue have negligible impact on mitigating the environmental consequences of this work. This means that these large energy budgets generate considerable carbon emissions, resulting in the use of data centres coming alongside significant environmental detriment. Specifically, research suggests that in 2018, cloud computing at data centres generated the equivalent of $31$ million tons of carbon dioxide \citep{hockstad-2018} in the US alone, equaling the total emissions generated by electric power in the state of California \citep{iea-2022}. Furthermore, the digital technology sector as a whole is estimated to be responsible for $4\%$ of global carbon emissions, with this figure forecast to double by 2025 \citep{bietti-2019}.

    Beyond these general figures, \citet{strubell-2019} showcased the carbon emissions specifically produced by training DL models. They found that training the language model BERT, which utilised 110M parameters and trained for 96 hours over 16 TPUs, produced the equivalent of 1438 lbs of $CO_2$---the same as a trans-American flight. \citeauthor{strubell-2019} also found that whilst the \emph{Evolved Transformer} of \citet{so-2019} improves state-of-the-art accuracy in English-German translation by $0.1$ \emph{BLEU} (a common metric of translated text quality), if implemented on GPU hardware this model could cost \$3.2 million to train (or \$147,000 if using TPUs), and generate $626,155$lbs of $CO_2$ (almost five times the lifetime emissions of an average car).

    It is important to note that the large financial cost associated with these intensive DL models also inflicts a great social cost. Namely, as the systems used at the forefront of DL research get larger and more complex (such as $175$ billion parameters and $745$GB dataset of  Switch-C), the price of storing the model and its training data, as well as the cost of running its training process on the specialist hardware this would require, becomes prohibitively high \citep{schwartz-2019}. This financial barrier restricts who can engage in cutting-edge research to only those with the backing of a large institution. Thus, this lack of accessibility drives a \emph{``rich get richer"} cycle of research funding \citep{strubell-2019} where only research driections within the interest of these institutions receive enough funding. This not only stifles creativity, but leaves the allocation of who benefits from the development of these systems, and who bears the negative side effects, to a handful of large corporations. In particular, \citet{bender-2021} highlight this disconnect between the benefits of energy-intensive DL research and the environmental consequences it inflicts (using the example of language models (LMs)): ``is it fair or just, for example, that the residents of the Maldives [\ldots] pay the environmental price of training and deploying ever larger English LMs".

    Hence, whilst DL been shown to provide state-of-the-art computational accuracy across a range of fields, its environmental impact cannot be ignored. Therefore, the accelerating reliance on ML and DL within Fintech poses issues for its sustainability, as this the models and methods contribute further to the negative ESG impacts of the financial sector. This issue generates a clear conflict between the growing use of DL in Fintech, and the growing need for sustainable finance. Both of these fields provide great utility to the finance sector: Fintech (including DL) provides innovative services to consumers and accurate tools for institutions, and sustainable finance ensures the industry has minimal ESG problems. Moreover, DL has been shown to have utility for work in sustainability further afield, such as improving the efficiency of exploiting renewable energy sources \citep{daniel-2021}, and even within sustainable finance itself, for example using ML to analyse the ESG factors of potential investments \citep{mehra-2022}. For these reasons, a compromise between the use of DL Fintech and the prioritisation of sustainable finance must be reached. 


    \subsection{Green AI}

    This apparent gap between the utility of DL systems---provided by their superior accuracy over traditional methods---and their environmental consequences has recently started to gather attention from ML researchers. In what has become known as \emph{Green AI} \citep{schwartz-2019}, new research has begun to consider how to mitigate the negative ESG impacts of ML by improving the efficiency of DL models. These efficient models reduce the energy required for training and deployment, minimising the carbon emissions they contribute towards. 

    Alongside \emph{mobile computing}, which prioritises energy-efficient methods due to the hardware constraints of mobile devices, the research domains of \emph{Natural Language Processing} (NLP) and \emph{Computer Vision} (CV) are currently the predominat fields focussing on Green AI, as both these areas exploit highly complex models, and require efficient real-time processing when deployed. NLP focuses on the processing and understanding of language (e.g. using language models to make predictions about text sequences), typically using large, complex DL models in an attempt to match and exceed human accuracy in language modelling (e.g. \citet{chowdhery-2022}). CV typically uses complex models with expensive methods---such as the \emph{convolution operation} \citep{dumoulin-2018}---to analyse, classify, and map visual environments. Within these fields, Green AI developers have been working to promote the utilisation of efficient methods that reduce the energy, time, and data requirements of model training by using parameters, data, and operations more intelligently (without producing a significant drop in accuracy). However, the major limitation of Green AI is that these methods have yet to garner significant attention outside of the specific research domains of NLP, CV, and mobile computing. Therefore the utility of these methods to improving the sustainability of ML has largely not been demonstrated further afield, meaning their potential benefits to reduce the energy consumption and carbon emissions of ML have yet to be seen on a wide scale.


    \section{Research Motivations}
    \label{section: motivations}

    Due to the energy-intensive nature of high-performance DL systems, it is clear their usage comes alongside several environmental and social issues, including their significant carbon footprint and financial cost. For this reason, the accelerating adoption of ML and DL methods within Fintech raises concerns about increasing the negative ESG impacts of the financial industry. Namely, the side effects of these complex DL models produced by their large parameter and data sets and long training phases (and hence high energy budgets) are in direct conflict with the general global effort to reduce carbon emissions, and the specific goal of sustainable finance to reduce the negative ESG impacts of the financial sector. However, the benefits of DL to finance (and indeed sustainable finance) have been shown, such as improved fraud detection \citep{sadgali-2019} and ESG data analysis \citep{mehra-2022}, making their continued adoption inevitable. 

    Therefore, to minimise the negative ESG impacts of the financial sector in light of the recent Fintech revolution (and solidify the benefits of sustainable finance), the adoption of Green AI principles and methods is paramount. In the first study of its kind, this thesis investigates how the energy-efficient Green AI methods can be adapted for DL in finance. This research aims to demonstrate how the Fintech revolution (in particular the expanding use of DL) does not have to come with significant environmental and social costs, and how the models and methods used in this field can coincide with the SDGs of sustainable finance. To exemplify the promise of introducing Green AI to Fintech and sustainable finance, energy-efficient methods will be applied to one of the most popular applications of DL in finance: \emph{financial risk modelling}. Specifically, a popular and important area of financial risk analysis known as \emph{volatility forecasting} will be explored, developing a DL-based model that accurately predicts the future volatility (a statistical measure of dispersion) of the \emph{S\&P 500} market, but takes minimal energy and data to train. 

    Volatility forecasting is commonly used to give insight into the risk of a financial market or asset (French et al., 1987); it is also been extensively explored by researchers to showcase the prediction capabilities of DL: for example, \citet{xiong-2015} combine S\&P 500 and Google domestic trends data to model stock volatility through DL, and \citet{zhang-2022} use DL for forecasting intraday volatility. Hence, as financial risk modelling and analysis is often cited as a core application of DL in finance (such as in the reviews of \citet{sezer-2019}, \citet{ozbayoglu-2020}, and \citet{thakkar-2021}, volatility forecasting has been chosen as the specific application within Fintech to demonstrate the promise of applying Green AI methods to the finance industry.

    By showing the utility of Green AI within Fintech, this thesis aims to advance the field of sustainable finance as a whole, creating a new field of \emph{sustainable deep learning for sustainable finance}. This work is the first study to address the conflict of interest between sustainable finance and the growing reliance of Fintech on DL systems with high energy consumption and concerning ESG impacts. Hence, the following research aims to demonstrate how the energy-efficient methods proposed by Green AI research (in the fields of NLP, CV, and mobile computing) can be exploited within the DL systems used in finance, increasing the scope and impact of sustainable finance by allowing it to use helpful DL systems without compromising SDGs, and reducing the emissions of Fintech in general.


    \section{Contributions to Science}
    \label{section: contributions}

    This research contributes to scientific literature and the finance industry in a number of ways:

    \begin{enumerate}
        \item \emph{Expanding the applications of Green AI}. This thesis is the first application of Green AI principles and methodology to the field of finance. This will further demonstrate the utility and promise of Green AI by proving that such systems can provide compelling performance with a lower environmental cost in new domains outside of the existing research focus on NLP, CV, and mobile computing, expanding the scope of this field.
        \item \emph{Reducing the environmental impact of financial technology}. The following work combines the research fields of FinTech, deep learning for finance, sustainable finance, and Green AI, to create the new research domain of \emph{sustainable deep learning for sustainable finance}. This further improves the ESG impact of the financial sector beyond previous work in sustainable finance by mitigating the conflict between the utility of DL models for sustainability modelling and the intrinsic carbon footprint of these energy-intensive systems.
        \item \emph{Improving the inclusivity of finance}. The reduction in the financial, environmental, and social cost of DL for finance also increases the inclusivity of this field. Improving the energy efficiency of these models creates a lower bar-to-entry to using DL in finance, allowing more industry players, developers, and individual traders to utilise the advantages brought by Fintech and DL (for example, accessing improved analytics that allow more informed financial decisions).
    \end{enumerate}


    \section{Research Objectives \& Structure}
    \label{section: structure}

    The initial objective of this research is to analyse the efficiency of a baseline DL model used for financial volatility forecasting. In particular, the resource requirements in terms of training time and training dataset size are inspected, from which an estimate can be produced of the carbon emissions associated with training such a model. A particular focus is given to the use of \emph{long short-term memory} (LSTM) networks, as these are the typically used DL model for sequence forecasting tasks in general, and recent research into volatility forecasting with DL (for example by \citet{xiong-2015}). This network and training process is implemented in \emph{Python} with the popular DL framework \emph{TensorFlow} \citep{abadi-2016}.

    The baseline model is then extended with energy-efficient methods proposed by the field of Green AI. These methods adapt the way in which the LSTM-based model is trained, minimising its resource requirements. An analysis similar to that conducted upon the baseline model is then conducted, quantifying the reduction in time and energy (and hence carbon emissions) made by these energy-efficient extensions. Specifically, methods such as \emph{progressive training} and \emph{mixed-precision} training are explored, and their benefits shown. Beyond these initial adaptations, data-efficient extensions to the training process are additionally be explored, such as the use of \emph{active learning}. These methods, also adapted from Green AI research, further reduce the resource requirements of a DL model, reducing the amount of training data necessary and thus also reducing the memory and energy required by the DL training process.

    Given these energy and data-efficient model extensions, an extensive analysis of the benefit of Green AI methods to this domain is made. This compares both the accuracy of the final model and each energy and data-efficient extension to the original baseline model. The resource requirements (in terms of time, data, and energy) required for training will then be compared, to evaluate any accuracy-efficiency compromises made. A general conclusion is then settled upon as to the utility and viability of Green AI methods to Fintech, sustainable finance, and the finance industry in general.
    \\ \\
    The aforementioned research will be organised as follows: 

    \begin{itemize}
        \item \underline{Chapter \ref{chapter: literature}: \emph{Literature review}}. The relevant literature to this research is first explored to give an overview of the key concepts and methodologies utilised in the following experimentation. This begins with a general exploration of machine learning, which then leads to a more refined discussion of the use of DL in finance, and the specific DL methods and concepts focussed upon within this thesis, giving a clear outline of the domain of financial volatility forecasting. Following this, the review explores the research domain of Green AI, with a focus on the specific methods and practices used in Green AI research which will go on to be a core aspect of this thesis.
        
        \item \underline{Chapter \ref{section: baseline}: \emph{Baseline finanical volatility model}}. An explanation and analysis of the baseline volatility model implemented, exemplifying the typical models, methods, and resource requirements of this domain.

        \item \underline{Chapter \ref{section: energy-extensions}: \emph{Energy-efficient training extensions}}. Once the baseline model has been implemented, several energy-efficient adaptations to its training process shall be explored. This section gives a complete overview of these methods, explaining how they work, the motivations behind their use, and their utility in decreasing the energy consumption of training.
        
        \item \underline{Chapter \ref{section: data-extensions}: \emph{Data-efficient training extensions}}. The utilised methods to reduce the data requirements of training are then be explored, discussing their origins within Green AI, explaining how they work, and evaluating their effectiveness.
        
        \item \underline{Chapter \ref{chapter: evaluation}: \emph{Discussion \& evaluation}}. Once the detail of the utilised model and extensions has been thoroughly explained, the results of the experimentation are discussed, evaluating the accuracy and efficiency of each method to deduce the success of the application of Green AI to finance.
    \end{itemize}


    % --------------------  LITERATURE REVIEW ----------------------
    \newpage
    \chapter{Background \& Literature Review}
    \label{chapter: literature}

    To understand how Fintech can be aligned with the goals of sustainable finance, first, a detailed understanding of ML and DL must be conveyed, and then the avenues used in Green AI to improve the efficiency of these methods can be effectively conveyed. This chapter achieves this by initially introducing the relevant background literature within the field of ML, centring the research of this thesis within the context of existing scientific literature. Specifically, this begins with a general summary of the components and characteristics of ML and DL methods, before focussing on the specific models and structures used within the chosen application of financial volatility modelling. The second part of the chapter outlines the field of Green AI, its motivations, applications, and the relevant tools and practices used in this domain to improve the energy efficiency of DL. These overviews of DL and Green AI act as the basis for the experimentation and analysis that follow within this thesis. Hence, the chapter concludes by drawing together these concepts to evaluate the current state of this research domain and identify the research gaps focussed upon by this thesis.


    \section{Machine Learning}

    The field of machine learning, first discussed by Arthur Samuel in his 1953 exploration of ``mechanical brains" \citep{samuel-1959}, is a subset of artificial intelligence concerned with using data to allow computer programs to independently learn how to complete a given task without explicit information about the rules of such task \citep{samuel-1959}. This learning typically involves a \emph{training} procedure, where the program is supplied with instances of experience from the training dataset describing the problem to be solved. The program learns from dataset $D$ how to perform the desired task $T$ by taking input of a data instance and outputting the result it believes is correct in this scenario. This result is then evaluatied through some performance metric $P$ (typically computed through a \emph{loss function}). For example, in a \emph{prediction} task, the program takes input of a sequence of values relating to some variable and outputs what it predicts to be the next value in the sequence: e.g. \citet{xiong-2015} use a dataset of S\&P 500 values and Google domestic trends to predict future stock market volatility. Alternatively, in a \emph{classification} task, a data point is input into the program to be assigned to a distinct class: e.g. \citet{sadgali-2019} use financial transaction data to classify whether a partiuclar transacition is fradulent or not. Such tasks are typically trained through \emph{supervised learning}, where the correct result (known as the \emph{label}) is provided to the program after it has produced its output, and the correctness of its solution is evaluated through the given metric $P$ (producing an \emph{accuracy} value), and its bahaviour adjusted to maximise performance according to $P$.


    \subsection{Neural Networks}

    Since the conception of machine learning, a core goal has been the development of algorithms that can accurately mimic the processing mechanisms of the human brain. These algorithms, known as \emph{artificial neural networks} (ANN), typically centre around neural models that recreate a version of the complex system of communications between neurons in the brain. The first implementation of such a model was Frank Rosenblatt's \emph{Mark I Perceptron} \citep{rosenblatt-1958}, which attempted to perform binary classification of images. Each pixel in the input image was represented as a single value in a 2-dimensional matrix of input neurons known as the \emph{input layer}. These values were then passed to the single internal neuron through \emph{weighted channels} to compute the weighted sum of all input values,  the result of which was passed through an \emph{activation function} to normalise the result to zero or one—representing an output classification of 'class zero' or 'class one'. 


    \subsection{Deep Learning}

    Whilst Rosenblatt's model produced underwhelming results due to its simplicity, its design became a fundamental building block of the larger, more complex ANNs used today. Namely, modern networks build on this approach by having wider and deeper \emph{model architectures} consisting of more internal neurons arranged in multiple layers. This approach of using \emph{multi-layer perceptrons} was popularised by \citet{rumelhart-1986}; however, recent work has pushed the boundaries of neural network performance by designing deeper and deeper architectures---known as \emph{deep neural networks} (DNNs)---to establish the field of deep learning. These deep networks perform computations in a similar vein to Rosenblatt's perceptron. Information is passed between the input layer $l^{(0)}$ and output layer $l^{(L-1)}$ along weighted channels between neurons, with the \emph{activation value} $a^{(i)}_n$ assigned each neuron $n$ in layer $i$ being determined through the dot product between the weights $W^{(i-1, i)}$ of channels entering $n$ and the values $x^{(i-1)}$ of neurons in the preceding layer $i-1$ connected through those channels \citep{witten-2017}. This product is then summed with a \emph{bias} term $b^{(i)}$, and passed through a chosen activation function $\alpha$ (Equation \ref{eq: activation-value}).

    \begin{equation}
        \label{eq: activation-value}
        a^{(i)}_n = \alpha( W^{(i-1, i)} \cdot x^{(i-1)} + b^{(i)} )
    \end{equation}

    This computation is performed over all layers of the network, starting with the input layer---whose neurons take the values of the input vector---and propagating through to the output layer—the values of which are the final network output. During training, the values of the network's parameters (i.e. the elements of weight matrix $W$ and bias vector $b$) are adjusted as the model learns, manipulating its outputs to more closely align with the true labels. \emph{Backpropagation} can be used to implement this tuning process, where the gradient of loss function with respect to the parameters is determined by propagating the error of the network on the current input backwards through all its layers \citep{zaras-2022}. These parameters can then be adjusted in the direction of the negative gradient through \emph{gradient descent} (towards the minimum of the loss function).


    \subsection{Sequence Modelling Problems}

    In machine learning, a \emph{sequence} is an ordered set of data elements, each of uniform type and dimension: for example, a stream of text can be seen as a sequence of strings (i.e. words), or a video can be modelled as a sequence of pixel matrices (i.e. frames of the video). Mathematically, temporal sequences---typically referred to as \emph{time series}---can be denoted by their data elements and time indices: 

    \begin{equation}
        \label{eq: timeseries-full}
        seq = \langle x_1, t_1 \rangle, \ldots, \langle x_T, t_T \rangle 
        \text{.}
    \end{equation}

    These sequences are commonly abbreviated to simply denote their data elements x, indexed over the time t that they were observed:

    \begin{equation}
        \label{eq: timeseries}
        seq = x_1, x_2, \ldots, x_t, \ldots, x_T
        \text{.}
    \end{equation}

    \emph{Sequence modelling} (also known as \emph{sequence learning}) involves using a machine learning model, known as a sequence model, to analyse and generate sequences. The problem is common in fields such as NLP, due to data being naturally ordered in sequences (e.g. sentences of text): for example, Google's \emph{Universal Sentence Encoder} \citep{cer-2018} is a state-of-the-art DNN for text embedding (the task of encoding sentences as vectors for performing further NLP tasks such as text classification).

    Sequence modelling typically takes three main avenues. Firstly, the \emph{sequence-to-vector} problem takes input of a sequence of data elements $x_1, x_2, \ldots, x_T$ up to timestep $T$ (one element at a time) and outputs a single value, or vector of values (either at each timestep or delayed to after the end of the sequence). For example, in \emph{part-of-speech tagging}---a ML application in NLP extensively reviewed by \citet{chiche-2022}---a model reads elements of a text sequence, and uses the current instance (a particular word in the sequence) and its context (the previously seen words) to classify elements of the sequence grammatically as adjectives, nouns, verbs, etc. In this case, whee each grammatical tag would be allocated a numerical label, and the output would be a vector of probabilities over all tags from which we could select the most probable predicted tag.

    Secondly, the \emph{vector-to-sequence} problem completes the opposite task, taking input of a vector of values and generating a corresponding output sequence. Artificial text generators (e.g. OpenAI's GPT-3 \citep{brown-2020} and Google's PaLM \citep{chowdhery-2022}) are a representative example of this problem, where an output sequence of text is artificially created based upon a vector of input parameters specifying the desired properties of the text.

    The final, and most common, form of sequence modelling is the \emph{sequence-to-sequence} problem, where both the inputs and outputs of the model are a sequence of elements. This task typically involves either \emph{translation}, where the $M$ elements in an original sequence are converted into a new sequence of length $N$, or \emph{prediction}, where the original sequence of elements $x_1, \ldots, x_M$ is used to predict a subsequent sequence $x_{M+1}, \ldots, x_{M+N}$. \emph{Machine translation} is one such application, in which a model reads a sequence of text written in one language, then translates this text into a different language, output as a new sequence: for example, Meta AI's \emph{M2M-100} \citep{fan-2020}, the first language model that can directly translate between any pair of 100 different languages. Prediction tasks are typically used for \emph{time series forecasting}, where the value of data elements is predicted over $N$ future timesteps, in such applications as \emph{stock price prediction} (extensively surveyed by \citet{sezer-2019}). In this case, the output time series $y = x_{T+1}, \ldots, x_{T+N}$ is computed based upon the context of the past time series $x_0, \ldots, x_T$. Specifically, a model $M_{\theta}$ (parameterised by $\theta$) is build that approximates the mapping $y = f( x_0, \ldots, x_T \vert \theta )$ between past and future time series.

    \section{Recurrent Neural Networks}

    \subsection{Neural Networks for Sequence Modelling}

    The task of forecasting future values of a sequence typically poses a challenge for traditional ANN and DNN architectures. A major reason for this is the fixed input structure of a typical ANN, meaning all elements of an input sequences would have to be fed into the network at the same time, with one element per neuron in the input layer. Hence, the input would contain no intrinsic sense of order, severely limiting the amount of analysis possible by the network, as the ordering of sequences such as time series is crucial to understanding and forecasting them \citep{tsantekidis-2022}. The chosen size of the input layer would also pose a challenge for a network with a fixed input structure, as the number of input neurons would have to be made adequately large to accommodate the possible variations in sequence lengths (e.g. the length of a time series of previous values grows as the current timestep being considered moves forward in time). Furthermore, to learn a sequence’s characteristic behaviour requires persistence of information independent from its location within the sequence. Namely, the network must contextually use previous understandings to inform the current instance, wheverver they appeared in the preceding sequence. This is not possible with a traditional ANN as the parameters of channels connected to different input neurons are not shared. Therefore, when the parameters of a specific neuron and its channels are adjusted after seeing an important pattern in part of a sequence, this information will only prove useful if that patten appears again in the exact same location on the input neurons, as if the pattern occurs at a different position it will be input through different neurons with different parameters that have not been previously learned to deal with this subsequence.


    \subsection{Recurrent Networks}

    To overcome these issues with traditional ANNs, \emph{recurrent neural networks} (RNNs) were conceived that utilise recurrent loops to allow varying input lengths and the persistence of learned sequential information \citep{sharma-2022}. These networks take input of a single sequence element $x_t$ at each timestep, using this and the context of previously observed sequence elements to inform the internal state $h_t$ of the network which acts as a form of short-term memory. This state is updated sequentially at each timestep $t$ through the function $g$, parameterised by $\theta^{(g)} = \{ W^{(g)}, U^{(g)}, b^{(g)} \}$, where $W^{(g)}$ and $U^{(g)}$ are weight matrices, and $b^{(g)}$ a bias vector. Namely, the function updates the previous state $h_{t-1}$ with new useful information learned from the input $x_t$ \citep{sharma-2022}:

    \begin{align}
        \label{eq: rnn-state}
        h_t &= g( h_{t-1}, x_t \vert \theta^{(g)} ) \\
        &= W^{(g)} \cdot h_{t-1} + U^{(g)} \cdot x_t + b^{(g)}
        \text{.}
    \end{align}

    The state $h_t$ is then output by the network along a recurrent loop that connects the output of the network at time $t-1$ to the input of the network at the next timestep $t$. This allows information to be passed between successive versions of the same neural network at different points in time; namely, the network itself stays the same at each computational step, it is only the internal state that changes. The concept can be demonstrated by unrolling the network in time, shown in Figure \ref{fig: rnn-diagram}. \citet{tsantekidis-2022} note that to avoid confusion is important to remember that the sequence of networks depicted in illustrations such as Figure BELOW are in fact just the same network only at different points in time, with the connecting arrows between each signifying that the output at time $t-1$ is being passed on to the same network at the subsequent timestep $t$.

    \begin{figure}[ht]
        \label{fig: rnn-diagram}
        \centering
        \includegraphics[width=0.7\textwidth]{rnn.png}
        \caption{\centering Diagram of a RNN at a single timestep $t$ (left) and unrolled in time between timesteps $0$ and $t$ (right)}
    \end{figure}

    To capture more information from the input sequences, RNNs can consist of multiple internal layers (shown in Figure BELOW), allowing them to learn higher-dimensional internal representations \citep{bengio-2009}. In this case, each layer $i$ retains its own internal state $h^{(i)}_t$, which is passed on to both the next layer of the network to compute its state $h^{(i+1)}_t$ (at the current input timestep) and reccurred back into itself at the subsequent timestep to compute the state $h^{(i)}_{t+1}$ \citep{zhang-2021}. 

    \begin{figure}[ht]
        \label{fig: deep-rnn}
        \centering
        \includegraphics[width=0.7\textwidth]{rnn.png}
        \caption{\centering Diagram of a multi-layer RNN (showing layers $1$ to $L$) with each layer unrolled in time between timesteps $1$ and $T$}
    \end{figure}


    \subsection{Training}

    Training of RNNs can be conducted through a similar backpropagation process to that used on traditional DNNs. This is done through computing the final output of the network on a given sequence (i.e. the output state at the final timestep) and evaluating the loss function between this and the true result, finding the total error of the RNN. The error is then propagated backwards through both the internal layers of the network and through time to the network state at each timestep of the input sequence---and hence is referred to as \emph{backpropagation through time} \citep{zhang-2021}. This allows the training procedure to adjust both the network parameters (i.e. weights $W$ and biases $b$) and the parameters $\theta^{(g)}$ of the state function $g$ determining how exactly the internal state $h^{(i)}_t$ is updated at each timestep.


    \subsection{Applications}

    Much recent research into sequence learning has exploited recurrent networks to conduct accurate modelling. In their review of the applications of RNNs within this domain, \citet{lipton-2015} explain how NLP tasks and time-series forecasting are the most common avenues explored by researchers utilising RNNs. These two areas have been extensively surveyed in literature; in their review of DL for NLP, described the rapidly increasing popularity of RNNs in recent years, highlighting language modelling, machine translation, speech recognition, and image captioning as major areas of recent process. They highlight the RNN model of \citet{karpathy-2015} for generating image descriptions---which significantly outperformed existing baselines---as a illustrative example of the benefits provided by this architecture. Additionally, \citet{hewamalage-2021} survey the current and future applications of RNNs for time series forecasting, highlighting the recent success of this model architecture at forecasting competitions, such as an RNN-based model by \citet{smyl-2020} winning The M4 Competition in 2019 with cutting-edge accuracy nearly $10\%$ greater than the utilised baseline \citep{makridakis-2020}.


    \section{Long Short-Term Memory}

    \subsection{The Issue with Recurrent Networks}

    The traditional RNN described above is incredibly effective at learning short-term dependencies within an input sequence, and exploiting these to make accurate predictions of subsequent values. In many cases, however, to understand the full context of a sequence longer-term dependencies need to be taken into account. Unfortunately, research into the effectiveness of recurrent networks by \citet{hochreiter-1991} and \citet{bengio-1994} has shown that RNNs cannot accurately capture or exploit long-term dependencies within sequences, as previous elements that are contextually relevant but were observed many timesteps ago can be forgotten prematurely. This issue is known as the short-term memory problem of RNNs, and is caused by the \emph{vanishing gradient problem} \citep{hochreiter-1991}. This issue is inherent to the backpropagation through time algorithm and the nature of recurrent variables. As demonstrated in Equation \ref{eq: rnn-state}, to calculate the value of a single recurrence involves multiplying by the weight matrix $W^{(g)}$: ignoring the bias and input element terms (as these aren't recurrent), we have $h_t = W^{(g)} \cdot h_{t-1}$. This poses a challenge when a recurrence is made over many steps, which is the case when evaluating the recurrent state function over many timesteps in a long sequence, as the same weight matrix is used over all recurrent state computations (shown in Equation \ref{eq: reccurent-weights} for a length $T$ sequence).

    \begin{align}
        \label{eq: reccurent-weights}
        h_T &= W^{(g)} \cdot W^{(g)} \cdot \ldots \cdot W^{(g)} \cdot h_0 \\
        &= (W^{(g)})^T \cdot h_0
    \end{align}

    Hence, the calculation is dependent on the computation of the weight matrix product $(W^{(g)})^T$; this means that if the parameters in the weight matrix are below one (i.e. we have $W^{(g)} < 1$) then the value of $(W^{(g)})^T$ will tend to zero with increasing sequence length $T$. This causes the gradient of the loss function with respect to the parameters $W^{(g)}$ to also tend to zero during backpropagation, meaning no significant updates are made to the values of $W^{(g)}$ when training on long sequences. Therefore, the gradient is said to \emph{vanish}, causing an inability of the network to learn dependencies over long sequences \citep{bengio-1994}.


    \subsection{Long Short-Term Memory Networks}

    Whilst several solutions to the vanishing gradient problem have been proposed---such as \emph{skip connections} and \emph{leaky recurrent units} \citep{pascanu-2012}---by far the most widespread approach is the use of \emph{Long Short-Term Memory} (LSTM), presented by \citet{hochreiter-1997}. An LSTM network is a RNN that uses multiple internal states and parameter matrices to mitigate the vanishing gradient problem and model both short-term and long-term dependencies within sequences. Similarly to an RNN, this architecture contains a core network consistent of one or more layers---where each layer is a distinct LSTM cell---and information is passed both between these cells (from the input to the output layer of the network) and between cell and and itself at the next timestep (reccuring the output at time $t-1$ to be input at time $t$). However, instead of the single recurrent state $h_t$ of the simple RNN, the LSTM contains two: a short-term memory state $h_t$ (analagous to that of a RNN) and long-term memory state $c_t$. This long-term state $c_t$ is used to retain information from far back in the input sequence, adding and forgetting relevant knowledge as the context of the sequence changes. 

    \begin{figure}[ht]
        \label{fig: lstm}
        \centering
        \includegraphics[width=0.7\textwidth]{lstm.png}
        \caption{\centering Diagram of a the arrangement of logical gates within a LSTM cell}
    \end{figure}

    At each timestep, the preceding long-term state $c_{t-1}$ is input into the LSTM cell (alongside the current sequence element $x_t$) to compute the new state $c_t$. These updates are achieved through several logical \emph{gates} within the network (between each cell and timestep). Firstly, the \emph{forget gate} is used to decide what information to discard from the previous state $c_{t-1}$. The previous short-term state $h_{t-1}$ and input element $x_t$ are multiplied with the forget gate's weight matrices $W^{(F)}$ and $U^{(F)}$ and added to its bias $b^{(F)}$, the result of which is passed through the \emph{sigmoid} activation function to produce the intermediate state value $\tilde{c}^{(F)}_t$, shown in Equation \ref{eq: forget-gate} \citep{zhang-2021}.

    \begin{equation}
        \label{eq: forget-gate}
        \tilde{c}^{(F)}_t = \sigma( W^{(F)} \cdot h_{t-1} + U^{(F)} \cdot x_t + b^{(F)} )
    \end{equation}

    Secondly, when new relevant information is seen in the input sequence, the \emph{input gate} is used to add it to the long-term memory state $c_t$. This process similarly uses the short-term state $h_{t-1}$ and current input $x_t$, but evaluates both a sigmoid and \emph{hyperbolic tangent} activation function in parallel on these inputs to decide both which values of $c_{t-1}$ must be updated (through the sigmoid evaluation) and by how much (the tangent evaluation). The result of these two operations are then multiplied together to give the second intermediate state value $\tilde{c}^{(I)}_t$, shown in Equation \ref{eq: input-gate} \citep{zhang-2021}.

    \begin{align}
        \label{eq: input-gate}
        (\tilde{c}^{(I)}_t)_{\sigma} &= \sigma( W^{(I,0)} \cdot h_{t-1} + U^{(I,0)} \cdot x_t + b^{(I,0)} ) \\
        (\tilde{c}^{(I)}_t)_{\tanh} &= \tanh{( W^{(I,1)} \cdot h_{t-1} + U^{(I,1)} \cdot x_t + b^{(I,1)} )} \\
        \tilde{c}^{(I)}_t &= (\tilde{c}^{(I)}_t)_{\sigma} \cdot (\tilde{c}^{(I)}_t)_{\tanh}
    \end{align}

    Both intermediate state values $\tilde{c}^{(F)}_t$ and $\tilde{c}^{(I)}_t$ are combined to give the new updated long-term state $c_t$ (Equation \ref{eq: long-state}).

    \begin{equation}
        \label{eq: long-state}
        c_t = \tilde{c}^{(F)}_t \cdot c_{t-1} + \tilde{c}^{(I)}_t
    \end{equation}

    The third and final gate used by LSTMs is the \emph{output gate}, which decides what each cell outputs. This is used as both the short-term state $h_t$ and the final network output $y$. The output gate uses the newly computed long-term state $c_t$, input $x_t$, and previous short-term state $h{t-1}$; a biased weighted sum of $h^{t-1}$ and $x_t$ is computed and fed into the sigmoid function, which is then multiplied by the hyperbolic tangent function evaluated on $c_t$ (Equation \ref{eq: output-gate}). Similarly to within the input gate, the sigmoid evaluation determines the values of $h_{t-1}$ to be updated, and the tangent evaluation determines by how much \citep{zhang-2021}.

    \begin{align}
        \label{eq: output-gate}
        (h_t)_{\sigma} &= \sigma( W^{(O)} \cdot h_{t-1} + U^{(O)} \cdot x_t + b^{(O)} ) \\
        (h_t)_{\tanh} &= \tanh{( c_t )} \\
        h_t &= (h_t)_{\sigma} \cdot (h_t)_{\tanh}
    \end{align}

    This system of gates occurs in every LSTM cell, passing information from the input to output layer of the network at every timestep in the form of the short and long-term states. When the network reaches the final timestep, the short-term state is output as the final network output $y$; note, if the sequence modelling problem involves outputting a generated sequence of length $N$ (e.g. forecasting multiple future values of a time series), this is produced one element at a time over $N$ additional timesteps that take no input and output the value $y_i = h_{T+i}$.


    \subsection{Applications}

    Whilst reviewing modern RNN architectures, \citet{lipton-2015} note that most state-of-the-art applications within the field of sequence learning use a LSTM-based model. Similarly,  in their review of the applications of this architecture and its variants \citet{yu-2019} assert that ``almost all" important recent advancements in this domain have been facilitated by LSTMs. This is primarily due to the architecture's ability to accurately model both long and short-term dependencies in sequences; hence, such networks are largely used for the same sequence modelling problems as traditional RNNs. 

    Due to the superior accuracy provided by this architecture, one of the most common use cases of LSTMs for sequence learning is NLP. For example, in their exhaustive study of language modelling, \citet{jozefowicz-2016} found an LSTM-based network to provide the most competitive results in this field. Their work evaluated several ML models such as \emph{convolutional neural networks} and RNNs, finding that a large scale LSTM language model produced the best results, significantly improving state-of-the-art \emph{perplexity} (a commonly used NLP metric of prediction error) from $51.3$ to $30.0$ on the commonly used \emph{One Billion Word Benchmark} dataset \citep{chelba-2013}. Recently, LSTM models have been applied to even more complex tasks within the NLP space due to the ability to capture high-fidelity dependencies. This includes the work of \citet{saleh-2021} who used an LSTM-based DNN to detect hate speech in online content, producing an impressive \emph{F1-score} (the weighted average of a model's \emph{precision} and \emph{recall}) of $93\%$ on a composite dataset from multiple online hate speech repositories.

    Like other RNNs, LSTMs have also been widely used for time series forecasting. Due to the advantages provided by these networks' ability to model long-term dependencies, the benefits of LSTMs have been explored across an extensive spectrum of different time series applications. For example, \citet{shi-2022} recently compared the performance of a number of networks (including both traditional RNNs and LSTMs) for predicting Bejing air quality. They concluded that LSTMs generated more accurate predictions than simpler RNNs, and demonstrated their improved long-term memory by showing LSTMs outperform other networks even when the context window available to the model is small. Another popular area of time series forecasting in which LSTM networks are increasingly being used is in the prediction of financial markets, as it has been shown by studies such as \citet{li-2017} that LSTMs can accurately capture the complex dependencies within highly variable financial variables. The success of LSTMs within this domain has been widely shown across a variety of financial modelling tasks, such as forecasting commodity prices \citep{ly-2021}, stock market indices like the S\&P 500 \citep{fjellstrom-2022}, currency pairs on the Foreign Exchange \citep{qi-2021}, and the fluctuations of financial market risk \citep{du-2019}.







    % --------------------  EXPERIMENTS ----------------------
    \newpage
    \chapter{Experiments}
    \label{chapter: experiments}

    \section{Baseline Financial Volatility Model}
    \label{section: baseline}

    \section{Energy-Efficient Training Extensions}
    \label{section: energy-extensions}

    \section{Data-Efficient Training Extensions}
    \label{section: data-extensions}


    % --------------------  EVALUATION ----------------------
    \newpage
    \chapter{Discussion \& Evaluation}
    \label{chapter: evaluation}

    % --------------------  CONCLUSIONS ----------------------
    \newpage
    \chapter{Conclusions \& Future Work}
    \label{chapter: conclusion}


    % --------------------  BIBLIOGRAPHY ---------------------
    \newpage
    \bibliography{bibliography}

\end{document}